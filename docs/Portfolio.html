<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Here is some of my work in R Studio!</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Brianna Fisher</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="About.html">About</a>
</li>
<li>
  <a href="CV.html">CV / Published Work</a>
</li>
<li>
  <a href="Portfolio.html">Portfolio</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Here is some of my work in R Studio!</h1>

</div>


<div id="portfolio" class="section level1 tabset">
<h1>Portfolio</h1>
<div id="introduction-to-data-science" class="section level2">
<h2>Introduction to Data Science</h2>
<h2>
Introduction to Data Science
</h2>
<div id="problem-set-1" class="section level3">
<h3>Problem Set 1</h3>
<p><strong>Question 1a</strong></p>
<pre class="r"><code>state &lt;- c(1, 2, 3, 4, 5)
pop &lt;- c(12.80, 19.45, 3.56, 0.97, 8.88)
cases &lt;- c(139623, 439238, 52095, 17857, 196337)</code></pre>
<p>The first vector, state, is created using the concatenate function to combine the five states within the data set (state 1, 2, 3, 4, and 5). The second vector, pop, is created using the concatenate function to combine the populations for those five states (12.80, 19.45, 3.56, 0.97, 8.88). The third vector, cases, is created using the concatenate function to combine the numbers of cases for the five states within the data set (139623, 439238, 52095, 17857, 196337). Now, these three vectors store all of the data on COVID-19 for five northeastern states as provided within the data table.</p>
<p><strong>Question 1b</strong></p>
<pre class="r"><code>ne.covid &lt;- cbind(state, pop, cases)
ne.covid</code></pre>
<pre><code>##      state   pop  cases
## [1,]     1 12.80 139623
## [2,]     2 19.45 439238
## [3,]     3  3.56  52095
## [4,]     4  0.97  17857
## [5,]     5  8.88 196337</code></pre>
<p>The object ne.covid was created by combining the three columns (state, population, cases) from the data set into a matrix. The function cbind was implemented and assigned to ne.covid. Once created, running ne.covid displays the same data table but in R.</p>
<p><strong>Question 1c</strong></p>
<pre class="r"><code>mean(ne.covid[,2])</code></pre>
<pre><code>## [1] 9.132</code></pre>
<pre class="r"><code>median(ne.covid[,2])</code></pre>
<pre><code>## [1] 8.88</code></pre>
<pre class="r"><code>max(ne.covid[,2])</code></pre>
<pre><code>## [1] 19.45</code></pre>
<pre class="r"><code>min(ne.covid[,2])</code></pre>
<pre><code>## [1] 0.97</code></pre>
<p>The mean of the second column of ne.covid is 9.132, the median is 8.88, the maximum is 19.45, and the minimum is 0.97.</p>
<p><strong>Question 1d</strong></p>
<pre class="r"><code>mean(ne.covid[,3])</code></pre>
<pre><code>## [1] 169030</code></pre>
<pre class="r"><code>median(ne.covid[,3])</code></pre>
<pre><code>## [1] 139623</code></pre>
<pre class="r"><code>max(ne.covid[,3])</code></pre>
<pre><code>## [1] 439238</code></pre>
<pre class="r"><code>min(ne.covid[,3])</code></pre>
<pre><code>## [1] 17857</code></pre>
<p>The mean of the third column of ne.covid is 169030, the median is 139623, the maximum is 439238, and the minimum is 17857.</p>
<p><strong>Question 1e</strong></p>
<pre class="r"><code>#creating a vector for state population in thousands
pop.in.1000s &lt;- pop*1000

#creating a vector for covid-19 cases per 1000 residenrs
cases.per.1000 &lt;- cases /pop.in.1000s</code></pre>
<p>The vector cases.per.1000 is created by assigning the result of the cases divided by 1000 to the vector. This vector represents the number of COVID-19 cases in each state per 1000 residents.</p>
<p><strong>Question 1f</strong></p>
<pre class="r"><code>state_chr &lt;- c(&quot;PA&quot;, &quot;NY&quot;, &quot;CT&quot;, &quot;DE&quot;, &quot;NJ&quot;)

plot(y=cases.per.1000, x=pop, xlab = &quot;Population in Millions&quot;, ylab = &quot;Cases per 1000 Residents&quot;, main = &quot;COVID-19 in the Northeast&quot;, type = &quot;n&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>plot(y=cases.per.1000, x=pop, xlab = &quot;Population in Millions&quot;, ylab = &quot;Cases per 1000 Residents&quot;, main = &quot;COVID-19 in the Northeast&quot;, text(x = pop, y = cases.per.1000, labels = state_chr))</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-6-2.png" width="672" /> The scatterplot “COVID-19 in the Northeast” was created by using the plot function. Within the plot, y is designated as cases.per.1000, x is designated as population in millisons, the x-axis is labeled population with the xlab function, and the y-axis is labeled Cases per 1000 Residents with the ylab function. The title is created with the main function, and is labeled COVID-19 in the Northeast. The states are assigned to their corresponding dot on the scatterplot with the text function.</p>
<p><strong>Question 1g</strong></p>
<pre class="r"><code>#adding cases.per.1000 and state_chr as columns to the ne.covid matrix
ne.covid &lt;- cbind(ne.covid, cases.per.1000, state_chr)

#subsetting to identify the state most affected by COVID 19
ne.covid[ne.covid[, &quot;cases.per.1000&quot;] == max(cases.per.1000), c(&quot;state_chr&quot;, &quot;cases.per.1000&quot;)]</code></pre>
<pre><code>##          state_chr     cases.per.1000 
##               &quot;NY&quot; &quot;22.5829305912596&quot;</code></pre>
<pre class="r"><code>#subsetting to identify the state least affected by COVID 19
ne.covid[ne.covid[, &quot;cases.per.1000&quot;] == min(cases.per.1000), c(&quot;state_chr&quot;, &quot;cases.per.1000&quot;)]</code></pre>
<pre><code>##      state_chr cases.per.1000 
##           &quot;PA&quot; &quot;10.908046875&quot;</code></pre>
<p>New York has been the most affected by COVID-19, with around 22.58 cases per 1000 residents. Pennsylvania has been the least affceted by COVID-19, with about 10.91 cases per 1000 residents.</p>
<p><strong>Question 1h</strong> If I could get one more piece of information about COVID-19 to describe how affected each of these states are, it would be the death rates. The death rates would signify the level of impact COVID-19 had on the states, as some people within the cases category may be asymptomatic or have mild symptoms and are okay now.</p>
<p><strong>Question 2a</strong></p>
<pre class="r"><code>load(&quot;/Users/briannafisher/Dropbox/Github/BriannaFisher/data/2019MLBTeamsData.Rdata&quot;)
bb$avgattpergame &lt;- as.numeric(bb$home.attendance/bb$games.played)
median(bb$avgattpergame)</code></pre>
<pre><code>## [1] 14055.35</code></pre>
<pre class="r"><code>max(bb$avgattpergame)</code></pre>
<pre><code>## [1] 24532.77</code></pre>
<pre class="r"><code>min(bb$avgattpergame)</code></pre>
<pre><code>## [1] 5008.037</code></pre>
<p>Because there are an even number of teams, no singular team had the median average attendance of 14055.35 but the two in the middle were the Washington Nationals (with a median average attendance of 13949.27) and the Minnesota Twins (with a median average attendance of 14161.43). The team with the maximum average attendance per game was the Los Angeles Dodgers, with a maximum of 24532.77. The team with the minimum average attendance per game was the Miami Marlins, with a minimum of 5008.037.</p>
<p><strong>Question 2b</strong></p>
<pre class="r"><code>bb$teambattavg &lt;- as.numeric(bb$hits/bb$at.bats)
median(bb$teambattavg)</code></pre>
<pre><code>## [1] 0.2492307</code></pre>
<pre class="r"><code>max(bb$teambattavg)</code></pre>
<pre><code>## [1] 0.2740068</code></pre>
<pre class="r"><code>min(bb$teambattavg)</code></pre>
<pre><code>## [1] 0.2364828</code></pre>
<p>Because there are an even number of teams, no singular team had the median team batting average of 0.2492307 but the two in the middle were the Oakland Athletics (with a median team batting average of 0.2488761) and the Cleveland Indians (with a median team batting average of 0.2495853). The team with the maximum batting average is the Houston Astros with a maximum of 0.2740068. The team with the minimum batting average is the Toronto Blue Jays with a minimum of 0.2364828.</p>
<p><strong>Question 2c</strong></p>
<pre class="r"><code>plot(y=bb$avgattpergame, x=bb$teambattavg, xlab = &quot;Team Batting Average&quot;, 
     ylab = &quot;Average Attendance Per Game&quot;, main = &quot;MLB 2019 Data&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-10-1.png" width="672" /> The graph looks very spread out, with the dots in seemingly randomly places. It is fairly nonlinear with no clear direction or association. The graph tells you that there is no relationship between team skill and attendance.</p>
<p><strong>Question 2d</strong></p>
<pre class="r"><code>bb$runs.pitch &lt;- as.numeric(bb$opponent.runs/bb$outs.pitched)
plot(y=bb$avgattpergame, x=bb$runs.pitch, xlab = &quot;Opponent Runs per Outs Pitched&quot;, 
     ylab = &quot;Average Attendance Per Game&quot;, main = &quot;MLB 2019 Data 2&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-11-1.png" width="672" /> The graph looks like the lower the opponent runs per outs pitched, the higher the average attendance per game. The same is for the opposite, the lower the average attendance per game, the higher the opponent runs per outs pitched. There are some outliers, though, that do not fit the pattern. This tells you that while there is not a linear relationship between team skill and attendance, there is an association between the two.</p>
<p><strong>Question 2e</strong></p>
<pre class="r"><code>bb$teamwinper &lt;- as.numeric(bb$wins/bb$games.played)
plot(y=bb$teamwinper, x=bb$teambattavg, xlab = &quot;Team Batting Average&quot;, 
     ylab = &quot;Team Winning Percentage&quot;, main = &quot;MLB 2019 Data 3&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-12-1.png" width="672" /> The graph looks more linear than the other two, with team winning percentage increasing as team batting average increases. This tells you that there is a positive relationship between team batting average and team winning percentage.</p>
<p><strong>Question 2 Bonus</strong></p>
<pre class="r"><code>prediction &lt;- lm(formula = teamwinper ~ teambattavg, data = bb)
plot(bb$teamwinper ~ bb$teambattavg, xlab = &quot;Team Batting Average&quot;, 
     ylab = &quot;Team Winning Percentage&quot;, main = &quot;MLB 2019 Data 3&quot;)
abline(prediction)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-13-1.png" width="672" /> To make a better prediction of a team’s winning percentage, the least squares regression line can be calculated and plotted in order to show where the predicted values would lie. This line is used to predict the value of y, or team winning percentage, for any value of x, or team batting average.</p>
<p><strong>Question 3a</strong></p>
<pre class="r"><code>recentgrads &lt;- read.csv(&quot;/Users/briannafisher/Dropbox/Github/BriannaFisher/data/recentgrads.csv&quot;, header = TRUE, na=&quot;NA&quot;)
ncol(recentgrads)</code></pre>
<pre><code>## [1] 15</code></pre>
<pre class="r"><code>nrow(recentgrads)</code></pre>
<pre><code>## [1] 173</code></pre>
<p>There are 15 columns and 173 rows in the “recentgrads” data set. The unit of analysis of the data is major, as the data focuses on the jobs college graduates received depending on their major.</p>
<p><strong>Question 3b</strong></p>
<pre class="r"><code>table(recentgrads$Major_category)</code></pre>
<pre><code>## 
##     Agriculture &amp; Natural Resources                                Arts 
##                                  10                                   8 
##              Biology &amp; Life Science                            Business 
##                                  14                                  13 
##         Communications &amp; Journalism             Computers &amp; Mathematics 
##                                   4                                  11 
##                           Education                         Engineering 
##                                  16                                  29 
##                              Health           Humanities &amp; Liberal Arts 
##                                  12                                  15 
## Industrial Arts &amp; Consumer Services                   Interdisciplinary 
##                                   7                                   1 
##                 Law &amp; Public Policy                   Physical Sciences 
##                                   5                                  10 
##            Psychology &amp; Social Work                      Social Science 
##                                   9                                   9</code></pre>
<p>There are 16 different major categories that the data are divided into. The engineering category has the most majors, with 29 different majors.</p>
<p><strong>Question 3c</strong></p>
<pre class="r"><code>sum(recentgrads$Women, na.rm = T)</code></pre>
<pre><code>## [1] 3895228</code></pre>
<pre class="r"><code>percentwomen &lt;- sum(recentgrads$Women, na.rm = T)/sum(recentgrads$Total, na.rm = T)
percentwomen * 100</code></pre>
<pre><code>## [1] 57.52255</code></pre>
<p>There are 3,895,228 women included in the dataset. 57.52% of the people in the dataset are women.</p>
<p><strong>Question 3d</strong></p>
<pre class="r"><code>recentgrads$womengrads &lt;- as.numeric(recentgrads$Women/recentgrads$Total)
order(recentgrads$womengrads, na.last = T)</code></pre>
<pre><code>##   [1]  74  67  27   2   4   9   1 107 112  12   6   3  51  53  82  15  29  10
##  [19]  11  66  21  26  76  20  46  39  32  18  43  54  25  44  72  23  85 113
##  [37]  14  16 111  64  34  28  24 144  36   5  31  17  19  37 106 108  73  80
##  [55] 159  33 136  65 142  63  70  38  48  58  13   7 148  42 115  59  86  77
##  [73]  95  90  79 103 140  75 147 118  83  68  41  47  84 126   8  69 123  30
##  [91]  98  55 122 133  81  93 161  60  91  62 134 124 143  78 102 168  61  57
## [109] 169 109 131  71 128 121 117 167 150  94  88  97 145 137 141  50 166  96
## [127] 158 138  92 163  49 125 100 160 127 120 162 116 130 154  87  40 153  45
## [145]  99 132 105 110 135 146 119 172 156 171 155 114 170 149  56 104 129 173
## [163]  89  35 152 157 101 151 139  52 164 165  22</code></pre>
<pre class="r"><code>recentgrads[74,]</code></pre>
<pre><code>##    Major_code                 Major Total Men Women
## 74       3801 MILITARY TECHNOLOGIES   124 124     0
##                         Major_category Employed Full_time Part_time
## 74 Industrial Arts &amp; Consumer Services        0       111         0
##    Full_time_year_round Unemployed Unemployment_rate Median P25th P75th
## 74                  111          0                 0  40000 40000 40000
##    womengrads
## 74          0</code></pre>
<pre class="r"><code>order(recentgrads$womengrads, na.last = F)</code></pre>
<pre><code>##   [1]  22  74  67  27   2   4   9   1 107 112  12   6   3  51  53  82  15  29
##  [19]  10  11  66  21  26  76  20  46  39  32  18  43  54  25  44  72  23  85
##  [37] 113  14  16 111  64  34  28  24 144  36   5  31  17  19  37 106 108  73
##  [55]  80 159  33 136  65 142  63  70  38  48  58  13   7 148  42 115  59  86
##  [73]  77  95  90  79 103 140  75 147 118  83  68  41  47  84 126   8  69 123
##  [91]  30  98  55 122 133  81  93 161  60  91  62 134 124 143  78 102 168  61
## [109]  57 169 109 131  71 128 121 117 167 150  94  88  97 145 137 141  50 166
## [127]  96 158 138  92 163  49 125 100 160 127 120 162 116 130 154  87  40 153
## [145]  45  99 132 105 110 135 146 119 172 156 171 155 114 170 149  56 104 129
## [163] 173  89  35 152 157 101 151 139  52 164 165</code></pre>
<pre class="r"><code>recentgrads[165,]</code></pre>
<pre><code>##     Major_code                     Major Total  Men Women Major_category
## 165       2307 EARLY CHILDHOOD EDUCATION 37589 1167 36422      Education
##     Employed Full_time Part_time Full_time_year_round Unemployed
## 165    32551     27569      7001                20748       1360
##     Unemployment_rate Median P25th P75th womengrads
## 165        0.04010498  28000 21000 35000  0.9689537</code></pre>
<p>The major that had the highest percentage of women graduates is education, with a percentage of 96.90%. The major that had the lowest percentage of women graduates is military technologies, with a percentage if 0%.</p>
<p><strong>Question 4</strong> Random sampling is such a vital component of survey research because it ensures that there is no bias within the sample. If people were chosen to take a survey, the surveyor could have picked specific people who they know will prove their hypothesis and therefore discredit the validity and accuracy of the survey. Random sampling also ensures that the results are representative of the entire population, which is why a sample of 1500 people is enough to learn about the whole US population. If the sample is completely random, then we can be confident that a sample of only 1500 people will include people who represent all different backgrounds and interests within the country.</p>
</div>
<div id="problem-set-2" class="section level3">
<h3>Problem Set 2</h3>
<p><strong>Question 1a</strong></p>
<pre class="r"><code>load(&quot;/Users/briannafisher/Dropbox/Github/BriannaFisher/data/exitpoll2016.RData&quot;)
require(tidyverse)
exit.untouched &lt;- exit

#The unit of observation is people, and the dataset had more than one row for every observation.
#Because there are two rows for each observation, using the spread function will condense each repeated row into one. Since the favorable candidate and favorable rating variables are the ones repeated, they are used to create the new columns
exit &lt;- spread(exit,
                 key = favorable.cand,
                 value = favorable.rating)
head(exit)</code></pre>
<pre><code>##       id PRSPA16 PHIL3 partyid married       sex.age.race educ.hs educ.somecoll
## 1 138951       2     3       2       1   male 30-44 White       1             0
## 2 138952       1     2       1       1     male 65+ White       0             0
## 3 138953       2     3       1       2   male 45-65 White       1             0
## 4 138954       1     1       1       1   female 65+ White       0             0
## 5 138955       1     1       2       2 female 18-29 White       0             1
## 6 138956       2     3       2       1     male 65+ White       1             0
##   educ.bach educ.postgrad clinton trump
## 1         0             0       2     1
## 2         0             1       1     2
## 3         0             0       2     1
## 4         0             1       1     2
## 5         0             0       1     2
## 6         0             0       2     1</code></pre>
<pre class="r"><code>#Using the recode function, the values of 1 and 2 can be replaced for favorable and unfavorable
attributes(exit.untouched$favorable.rating)</code></pre>
<pre><code>## $label
## [1] &quot;Is your opinion of [favorable.cand]:&quot;
## 
## $labels
##   favorable unfavorable        omit 
##           1           2           9</code></pre>
<pre class="r"><code>exit$clinton &lt;- recode(exit$clinton, &quot;1&quot; = &quot;Favorable&quot;, &quot;2&quot; = &quot;Unfavorable&quot;)
exit$trump &lt;- recode(exit$trump, &quot;1&quot; = &quot;Favorable&quot;, &quot;2&quot; = &quot;Unfavorable&quot;)</code></pre>
<p><strong>Question 1b</strong></p>
<pre class="r"><code>#the gather function brings responses that are spread over multiple columns into one. The recode function
#changes the data from numeric values (0/1) that don&#39;t make sense
exit &lt;- gather(exit,
                 key = &quot;educ&quot;, 
                 value = &quot;val&quot;,
               starts_with(&quot;educ.&quot;))

exit$educ &lt;- recode(exit$educ, &quot;educ.hs&quot; = &quot;hs&quot;, &quot;educ.somecoll&quot; = &quot;some college&quot;, 
                    &quot;educ.bach&quot; = &quot;bachelors&quot;, &quot;educ.postgrad&quot; = &quot;postgrad&quot;)

#Since all unknown values are coded as 99 (using attributes checks this), I can recode those values to NA
attributes(exit.untouched$educ.hs)</code></pre>
<pre><code>## $label
## [1] &quot;What was the last grade of school you completed? [high school or less]&quot;
## 
## $labels
##     yes      no unknown 
##       1       0      99</code></pre>
<pre class="r"><code>attributes(exit.untouched$educ.somecoll)</code></pre>
<pre><code>## $label
## [1] &quot;What was the last grade of school you completed? [some college/assoc. degree]&quot;
## 
## $labels
##     yes      no unknown 
##       1       0      99</code></pre>
<pre class="r"><code>attributes(exit.untouched$educ.bach)</code></pre>
<pre><code>## $label
## [1] &quot;What was the last grade of school you completed? [college graduate]&quot;
## 
## $labels
##     yes      no unknown 
##       1       0      99</code></pre>
<pre class="r"><code>attributes(exit.untouched$educ.postgrad)</code></pre>
<pre><code>## $label
## [1] &quot;What was the last grade of school you completed? [postgraduate study]&quot;
## 
## $labels
##     yes      no unknown 
##       1       0      99</code></pre>
<pre class="r"><code>exit[exit$educ == &quot;99&quot;] &lt;- NA

exit$val &lt;- NULL

#Could have also changed the names by reshaping the education data
exit$educ[exit$educ == &quot;educ.hs&quot;] &lt;- &quot;hs&quot;
exit$educ[exit$educ == &quot;educ.somecoll&quot;] &lt;- &quot;some college&quot;
exit$educ[exit$educ == &quot;educ.bach&quot;] &lt;- &quot;bachelors&quot;
exit$educ[exit$educ == &quot;educ.postgrad&quot;] &lt;- &quot;postgrad&quot;</code></pre>
<p><strong>Question 1c</strong></p>
<pre class="r"><code>#The separate function splits the sex.age.race column into three separate columns
exit &lt;- separate(exit,
         col = &quot;sex.age.race&quot;,
         into = c(&quot;sex&quot;,&quot;age&quot;,&quot;race&quot;), sep = &quot; &quot;)

#Converting the columns to factors and then replacing the missing/unknown values with NA cleans the data
exit$age = as.factor(exit$age)
summary(exit$age)</code></pre>
<pre><code>##  -999 18-29 30-44 45-65   65+ 
##    52  1948  2924  4600  2304</code></pre>
<pre class="r"><code>exit$age[which(exit$age == &quot;-999&quot;)] = NA
summary(exit$age)</code></pre>
<pre><code>##  -999 18-29 30-44 45-65   65+  NA&#39;s 
##     0  1948  2924  4600  2304    52</code></pre>
<pre class="r"><code>exit$sex = as.factor(exit$sex)
summary(exit$sex)</code></pre>
<pre><code>##  female    male unknown 
##    6484    5328      16</code></pre>
<pre class="r"><code>exit$sex[which(exit$sex == &quot;unknown&quot;)] = NA
summary(exit$sex)</code></pre>
<pre><code>##  female    male unknown    NA&#39;s 
##    6484    5328       0      16</code></pre>
<pre class="r"><code>#The race values are already coded as NA for missing, but could have done this 
exit$race = as.factor(exit$race)
summary(exit$race)</code></pre>
<pre><code>##           Asian           Black Hispanic/Latino              NA           Other 
##             124            1268             768              92             180 
##           White 
##            9396</code></pre>
<pre class="r"><code>exit$race[which(exit$race == &quot;NA&quot;)] = NA
summary(exit$race)</code></pre>
<pre><code>##           Asian           Black Hispanic/Latino              NA           Other 
##             124            1268             768               0             180 
##           White            NA&#39;s 
##            9396              92</code></pre>
<p><strong>Question 1d</strong></p>
<pre class="r"><code>#the new varible third.party is created by recoding everything as NA and then recoding each value to the 
#new value. Using NULL removes the variable from the data set
attributes(exit.untouched$PRSPA16)</code></pre>
<pre><code>## $label
## [1] &quot;In today&#39;s election for president, did you just vote for:&quot;
## 
## $format.stata
## [1] &quot;%8.0g&quot;
## 
## $labels
##    Did not vote Hillary Clinton    Donald Trump    Gary Johnson      Jill Stein 
##               0               1               2               3               4 
##           Other 
##               9</code></pre>
<pre class="r"><code>exit$third.party &lt;- &quot;NA&quot;
exit$third.party[exit$PRSPA16 == 1] &lt;- &quot;0&quot;
exit$third.party[exit$PRSPA16 == 2] &lt;- &quot;0&quot;
exit$third.party[exit$PRSPA16 == 3] &lt;- &quot;1&quot;
exit$third.party[exit$PRSPA16 == 4] &lt;- &quot;1&quot;
exit$third.party[exit$PRSPA16 == 9] &lt;- &quot;1&quot;

exit$PRSPA16 &lt;- NULL</code></pre>
<p><strong>Question 1e</strong></p>
<pre class="r"><code>#the as.numeric function converts the married variable into a dummy variable, coding 1 for married and 
#0 for not married
attributes(exit.untouched$married) #tells us that 1 is yes and 2 is no</code></pre>
<pre><code>## $label
## [1] &quot;Are you currently married?&quot;
## 
## $format.stata
## [1] &quot;%8.0g&quot;
## 
## $labels
## Yes  No 
##   1   2</code></pre>
<pre class="r"><code>exit$married &lt;- as.numeric(exit$married==1)</code></pre>
<p><strong>Question 1f</strong></p>
<pre class="r"><code>#the factor function recodes the PHIL3 and partyid variables into meaningful labels 
attributes(exit.untouched$PHIL3)</code></pre>
<pre><code>## $label
## [1] &quot;On most political matters, do you consider yourself:&quot;
## 
## $format.stata
## [1] &quot;%8.0g&quot;
## 
## $labels
##      Liberal     Moderate Conservative 
##            1            2            3</code></pre>
<pre class="r"><code>exit$PHIL3 &lt;- factor(exit$PHIL3, labels = c(&quot;Liberal&quot;, &quot;Moderate&quot;, &quot;Conservative&quot;))

attributes(exit.untouched$partyid)</code></pre>
<pre><code>## $label
## [1] &quot;No matter how you voted today, do you usually think of yourself as a:&quot;
## 
## $format.stata
## [1] &quot;%8.0g&quot;
## 
## $labels
##       Democrat     Republican    Independent Something else 
##              1              2              3              4</code></pre>
<pre class="r"><code>exit$partyid &lt;- factor(exit$partyid, labels = c(&quot;Democrat&quot;, &quot;Republican&quot;, &quot;Independent&quot;,
                                                &quot;Something Else&quot;))</code></pre>
<p><strong>Question 1g</strong></p>
<pre class="r"><code>#the rename function will change the name of a column to something more meaningful
exit &lt;- rename(exit, ideology = PHIL3)</code></pre>
<p><strong>Question 1h</strong></p>
<pre class="r"><code>#I split the age variable into the different groups and then also made the partyid ordinal so that they can be compared. From there I made them into a table, and then did the frequencies of the results. Lastly, I added together all of the values in the table to make sure that they are equal to 1. 
exit$Age.group &lt;- &quot;NA&quot;
exit$Age.group[exit$age == &quot;18-29&quot;] &lt;- 1
exit$Age.group[exit$age == &quot;30-44&quot;] &lt;- 2
exit$Age.group[exit$age == &quot;45-65&quot;] &lt;- 3
exit$Age.group[exit$age == &quot;65+&quot;] &lt;- 4

exit$ID &lt;- &quot;NA&quot;
exit$ID[exit$partyid == &quot;Republican&quot;] &lt;- 5
exit$ID[exit$partyid == &quot;Democrat&quot;] &lt;- 6
exit$ID[exit$partyid == &quot;Independent&quot;] &lt;- 7
exit$ID[exit$partyid == &quot;Something Else&quot;] &lt;- 8

mytable &lt;- table(exit$Age.group, exit$ID)
prop.table(mytable)</code></pre>
<pre><code>##     
##                 5            6            7            8           NA
##   1  0.0568143389 0.0696652012 0.0229962800 0.0064254312 0.0087926953
##   2  0.0825160636 0.0980723706 0.0480216436 0.0121745012 0.0064254312
##   3  0.1531958066 0.1531958066 0.0520798106 0.0125126818 0.0179235712
##   4  0.0723706459 0.0798106189 0.0280689888 0.0030436253 0.0114981400
##   NA 0.0006763612 0.0020290835 0.0013527224 0.0000000000 0.0003381806</code></pre>
<pre class="r"><code>sum(prop.table(mytable))</code></pre>
<pre><code>## [1] 1</code></pre>
<p>Comparing the frequencies of the age group and ID variables, there is a 6.97% chance of people in the age group of 18-29 being democratic and a 5.68% chance of people being 18-29 and republican. The values are lower for independents and ‘something else,’ with a 0.64% chance of someone 18-29 being independent and a 0.88% chance of someone being 18-29 and something else. This is fairly expected, as young people are more likely to be democratic than any other party. Another interesting observation is that there is the same 15.32% chance of someone being 45-65 years old and either a republican or democrat. The 45-65-year-old age group also has the highest chance of being independent (5.21%) and something else (1.25%) than any other group. This may be explained by the fact that middle aged and older people are more likely to watch the news and therefore align with a particular political party, or because this age group has particularly high voter turnout and therefore need to be registered to a party to participate.</p>
<p><strong>Question 2a</strong></p>
<pre class="r"><code>library(rio)
UNICEF_untouched &lt;- rio::import(&quot;/Users/briannafisher/Dropbox/Github/BriannaFisher/data/unicefdata.xlsx&quot;)

UNICEF &lt;- UNICEF_untouched

#Problems with the data:
#1. Columns are named with numbers
#2. Column names are in the actual columns
#3. Have dashes and x&#39;s within the data</code></pre>
<p><strong>Question 2b</strong></p>
<pre class="r"><code>#The names function renames the columns with the names in the first row, and then the first two rows can be deleted. Using rownames will renumber the rows so that it does not start at 3.
names(UNICEF) &lt;- lapply(UNICEF[1, ], as.character)
UNICEF = UNICEF[-1,] 
UNICEF = UNICEF[-1,] 
row.names(UNICEF) &lt;- 1:nrow(UNICEF)</code></pre>
<p><strong>Question 2c</strong></p>
<pre class="r"><code>#rowmeans tells us the mean value of each row in the data set. The rows that have &quot;1&quot; as their mean 
#are blank and should be removed. The -c function lets you drop rows or 
#columns from the data set
rowMeans(is.na(UNICEF))</code></pre>
<pre><code>##          1          2          3          4          5          6          7 
## 0.14285714 0.14285714 0.07142857 0.14285714 0.14285714 0.14285714 0.14285714 
##          8          9         10         11         12         13         14 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 
##         15         16         17         18         19         20         21 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.07142857 0.07142857 0.14285714 
##         22         23         24         25         26         27         28 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.07142857 0.07142857 
##         29         30         31         32         33         34         35 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 
##         36         37         38         39         40         41         42 
## 0.14285714 0.07142857 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 
##         43         44         45         46         47         48         49 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.00000000 0.14285714 0.14285714 
##         50         51         52         53         54         55         56 
## 0.14285714 0.07142857 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 
##         57         58         59         60         61         62         63 
## 0.14285714 0.14285714 0.07142857 0.14285714 0.14285714 0.14285714 0.14285714 
##         64         65         66         67         68         69         70 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 
##         71         72         73         74         75         76         77 
## 0.14285714 0.14285714 0.14285714 0.07142857 0.14285714 0.14285714 0.14285714 
##         78         79         80         81         82         83         84 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 
##         85         86         87         88         89         90         91 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.07142857 0.14285714 
##         92         93         94         95         96         97         98 
## 0.14285714 0.14285714 0.07142857 0.14285714 0.07142857 0.14285714 0.07142857 
##         99        100        101        102        103        104        105 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.07142857 0.14285714 
##        106        107        108        109        110        111        112 
## 0.00000000 0.14285714 0.14285714 0.14285714 0.07142857 0.14285714 0.14285714 
##        113        114        115        116        117        118        119 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 
##        120        121        122        123        124        125        126 
## 0.07142857 0.14285714 0.14285714 0.14285714 0.14285714 0.07142857 0.14285714 
##        127        128        129        130        131        132        133 
## 0.07142857 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 
##        134        135        136        137        138        139        140 
## 0.14285714 0.14285714 0.14285714 0.07142857 0.14285714 0.14285714 0.14285714 
##        141        142        143        144        145        146        147 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 
##        148        149        150        151        152        153        154 
## 0.14285714 0.14285714 0.14285714 0.07142857 0.14285714 0.14285714 0.14285714 
##        155        156        157        158        159        160        161 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 
##        162        163        164        165        166        167        168 
## 0.14285714 0.07142857 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 
##        169        170        171        172        173        174        175 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.07142857 0.14285714 
##        176        177        178        179        180        181        182 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 
##        183        184        185        186        187        188        189 
## 0.14285714 0.14285714 0.14285714 0.07142857 0.14285714 0.14285714 0.14285714 
##        190        191        192        193        194        195        196 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.07142857 
##        197        198        199        200        201        202        203 
## 0.14285714 1.00000000 0.92857143 0.14285714 0.14285714 0.14285714 0.14285714 
##        204        205        206        207        208        209        210 
## 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 1.00000000 
##        211        212        213        214        215        216        217 
## 0.92857143 0.92857143 0.92857143 0.92857143 0.92857143 0.92857143 0.92857143 
##        218 
## 0.92857143</code></pre>
<pre class="r"><code>which(rowMeans(is.na(UNICEF)) == 1)</code></pre>
<pre><code>## 198 210 
## 198 210</code></pre>
<pre class="r"><code>UNICEF &lt;- UNICEF[,-c(12,14)]
UNICEF &lt;- UNICEF[-c(198),]
row.names(UNICEF) &lt;- 1:nrow(UNICEF)
UNICEF &lt;- UNICEF[-c(209),]
row.names(UNICEF) &lt;- 1:nrow(UNICEF)

#Now, the dataset has 12 columns and 216 rows. (When I take out the summary indicators on 2. h.,
#the dataset has 12 columns and 215 rows).</code></pre>
<p><strong>Question 2d</strong></p>
<pre class="r"><code>#The rename function changes the names of the columns so that they are more manageable
attributes(UNICEF)</code></pre>
<pre><code>## $names
##  [1] &quot;Countries and areas&quot;                        
##  [2] &quot;Under-5 mortality rate (U5MR) (1990)&quot;       
##  [3] &quot;Under-5 mortality rate (U5MR) 2015&quot;         
##  [4] &quot;U5MR (male)&quot;                                
##  [5] &quot;U5MR (female)&quot;                              
##  [6] &quot;Neonatal  mortality  rate&quot;                  
##  [7] &quot;Total population (thousands)&quot;               
##  [8] &quot;Annual no. of births (thousands)&quot;           
##  [9] &quot;GNI per capita (US$)&quot;                       
## [10] &quot;Life expectancy at birth (years)&quot;           
## [11] &quot;Total adult literacy rate          (%)&quot;     
## [12] &quot;Primary school net enrolment ratio      (%)&quot;
## 
## $row.names
##   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
##  [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36
##  [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54
##  [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
##  [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
##  [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108
## [109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126
## [127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144
## [145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162
## [163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180
## [181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198
## [199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216
## 
## $class
## [1] &quot;data.frame&quot;</code></pre>
<pre class="r"><code>UNICEF &lt;- rename(UNICEF, c(&quot;Countries&quot; = &quot;Countries and areas&quot;, 
                 &quot;U5MR.1990&quot; = &quot;Under-5 mortality rate (U5MR) (1990)&quot;,
                 &quot;U5MR.2015&quot; = &quot;Under-5 mortality rate (U5MR) 2015&quot;,
                 &quot;U5MR.Male&quot; = &quot;U5MR (male)&quot;, &quot;U5MR.Female&quot; = &quot;U5MR (female)&quot;, 
                 &quot;Total.Pop&quot; = &quot;Total population (thousands)&quot;,
                 &quot;Annual.Births&quot; = &quot;Annual no. of births (thousands)&quot;,
                 &quot;GNI.Per.Capita&quot; = &quot;GNI per capita (US$)&quot;, 
                 &quot;Neonatal.Mortality.Rate&quot; = &quot;Neonatal  mortality  rate&quot;,
                 &quot;Life.Expect.Birth&quot; = &quot;Life expectancy at birth (years)&quot;,
                 &quot;Total.Adult.Lit.Rate&quot; = &quot;Total adult literacy rate          (%)&quot;,
                 &quot;Primary.School.Net.Enrollment.Ratio&quot; = &quot;Primary school net enrolment ratio      (%)&quot;
                 ))</code></pre>
<p><strong>Question 2e</strong></p>
<pre class="r"><code>which(UNICEF$Countries == &quot;Notes:&quot;)</code></pre>
<pre><code>## [1] 209</code></pre>
<pre class="r"><code>UNICEF &lt;- UNICEF[-c(209:216),]</code></pre>
<p><strong>Question 2f</strong></p>
<pre class="r"><code>#This recodes all values of - in the dataset to NA
UNICEF[UNICEF == &quot;-&quot;] &lt;- NA

#This converts the columns to be numeric and replaces the letters with NAs
UNICEF[,2:12] &lt;- as.numeric(as.character(unlist(UNICEF[,2:12])))</code></pre>
<p><strong>Question 2g</strong></p>
<pre class="r"><code>#The round function will round the values to the largest whole number after multiplying the variable by 1000
UNICEF$Total.Pop &lt;- round(UNICEF$Total.Pop*1000)
UNICEF$Annual.Births &lt;- round(UNICEF$Annual.Births*1000)</code></pre>
<p><strong>Question 2h</strong></p>
<pre class="r"><code>#Subsetting the data creates a new dataset with only the wanted obeservations.
summary.indicators &lt;- subset(UNICEF[199:208,])
UNICEF &lt;- UNICEF[-c(198:208),]

#Now, the UNICEF dataset has 197 rows and the summary indicators dataset has 10 rows.</code></pre>
<p><strong>Question 2i</strong></p>
<pre class="r"><code>#to find the change in mortality rate from 1990 to 2015, the two can be subtracted (any countries with apositive value had an increase in mortality rate)
change.mortality &lt;- (UNICEF$U5MR.2015)-(UNICEF$U5MR.1990)
print(change.mortality)</code></pre>
<pre><code>##   [1]  -90  -27  -21   -6  -69  -18  -15  -36   -5   -6  -63  -12  -17 -106   -5
##  [16]  -12   -6  -23  -80 -101  -86  -13  -10  -45   -2  -12 -113  -90  -38  -88
##  [31]  -50   -3  -47  -76  -11  -43  -19  -51  -49  -16   -7  -60   -9   -7   -8
##  [46]  -12  -18  -89   -5  -54    4  -29  -35  -62  -42  -96 -104  -17 -146   -8
##  [61]   -5   -5  -42 -101  -36   -5  -65   -8  -11  -52 -144 -136  -21  -77   NA
##  [76]  -38  -13   -4  -78  -58  -42  -22   -5   -8   -6  -15   -3  -19  -39  -53
##  [91]  -40   -9  -44  -95  -12  -25    2 -185  -29   NA  -12   -7 -111 -178  -10
## [106]  -85 -139   -5  -14  -33   -9  -34  -21   -4  -86  -12  -52 -161  -60  -29
## [121]  -22 -105   -4   -5  -45 -232 -104    9   -6  -27  -58  -20  -14  -32  -26
## [136]  -63  -30  -12  -11  -13   -4  -17  -27  -16 -110  -17   -9   -7  -13   -8
## [151]  -64  -29  -93  -21   -3 -144   -5  -11   -7  -12  -43  -19 -160   -7  -11
## [166]  -23  -58  -27  -14   -4   -4  -24  -63  -25  -31 -123  -68   -5  -11  -43
## [181]  -61  -40  -30 -132  -11  -10   -5 -116   -4  -13  -33   -8  -15  -29  -84
## [196] -127   -5</code></pre>
<p>Three countries had a mortality rate that increased over this 25-year period. The countries are Dominica, Lesotho, and Niue.</p>
<p><strong>Question 2j</strong></p>
<pre class="r"><code>#the cor functions finds the correlation between the GNI per captia and neonatal mortality rate
cor(UNICEF$GNI.Per.Capita, 
    UNICEF$Neonatal.Mortality.Rate, &quot;complete.obs&quot;)</code></pre>
<pre><code>## [1] -0.560275</code></pre>
<pre class="r"><code>#the plot function creates a scatterplot of the GNI per capita and neonatal mortality rate 
plot(y=UNICEF$Neonatal.Mortality.Rate, x=UNICEF$GNI.Per.Capita,
     xlab = &quot;Gross National Income Per Capita in U.S. Dollars&quot;,
     ylab = &quot;Neonatal Mortality Rate&quot;, 
     main = &quot;Gross National Income Per Capita in U.S. Dollars and Neonatal Mortality&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-35-1.png" width="672" /> When looking at the relationship between Gross National Income per capita and Neonatal Mortality Rate, the correlation can be calculated as -0.56. This indicates that there is a negative association between Gross National Income per capita and Neonatal Mortality Rate. This can also be seen in the plot of Gross National Income per capita and Neonatal Mortality Rate, with Neonatal Morality Rate decreasing as Gross National Income per capita increases.</p>
</div>
<div id="problem-set-3" class="section level3">
<h3>Problem Set 3</h3>
<p><strong>Question 1a</strong></p>
<pre class="r"><code>library(rio)
library(tidyr)
library(dplyr)

#the rio package reads in the data from excel
mont_untouched &lt;- rio::import(&quot;/Users/briannafisher/Dropbox/Github/BriannaFisher/data/2018-General-Montgomery.xls&quot;)
mont &lt;- mont_untouched
dekalb &lt;- read.csv(&quot;/Users/briannafisher/Dropbox/Github/BriannaFisher/data/dekalb-cleaned.csv&quot;)

#first, dim is used to see how many columns are coded as precincts. Then the gather function is used to 
#put all of the precincts in one column
dim(mont)</code></pre>
<pre><code>## [1] 195  54</code></pre>
<pre class="r"><code>mont &lt;- gather(mont, key = &quot;precinct&quot;, value = &quot;votes&quot;, 4:54)

#Using !grepl, I can remove the columns with the over and under votes and the number of registered voters and 
#ballots cast in each precinct
mont &lt;- mont[!grepl(&quot;Over Votes&quot;, mont$Candidate),]
mont &lt;- mont[!grepl(&quot;Under Votes&quot;, mont$Candidate),]
mont &lt;- mont[!grepl(&quot;REGISTERED VOTERS&quot;, mont$`Contest Title`),]
mont &lt;- mont[!grepl(&quot;BALLOTS CAST&quot;, mont$`Contest Title`),]
row.names(mont) &lt;- 1:nrow(mont)

#na.omit is used to remove the rows with missing vote data
mont &lt;- na.omit(mont)

#the rename function recodes the column names
attributes(mont)</code></pre>
<pre><code>## $names
## [1] &quot;Contest Title&quot; &quot;Party&quot;         &quot;Candidate&quot;     &quot;precinct&quot;     
## [5] &quot;votes&quot;        
## 
## $row.names
##    [1]    1    2    3    4    5   11   12   13   14   15   16   17   18   19
##   [15]   20   21   22   23   24   25   26   27   28   29   30   31   32   33
##   [29]   34   35   36   37   38   39   40   41   42   43   44   45   46   47
##   [43]   48   49   50   51   52   53   54   55   56   57   58   59   60   61
##   [57]   65   66   67   77   78   85   86   87   88   89   90   91   92   93
##   [71]   94   95   96   97   98   99  100  101  102  103  104  105  106  107
##   [85]  108  109  115  116  117  118  119  120  121  122  123  124  125  126
##   [99]  127  128  129  130  131  132  133  134  135  136  137  138  139  140
##  [113]  141  142  143  144  145  146  147  148  149  150  151  152  153  154
##  [127]  155  156  157  158  159  160  161  162  163  164  165  169  170  171
##  [141]  181  182  189  190  191  192  193  194  195  196  197  198  199  200
##  [155]  201  202  203  204  205  206  207  208  209  210  211  212  213  219
##  [169]  220  221  222  223  224  225  226  227  228  229  230  231  232  233
##  [183]  234  235  236  237  238  239  240  241  242  243  244  245  246  247
##  [197]  248  249  250  251  252  253  254  255  256  257  258  259  260  261
##  [211]  262  263  264  265  266  267  268  269  270  271  272  278  279  280
##  [225]  283  284  293  294  295  296  297  298  299  300  301  302  303  304
##  [239]  305  306  307  308  309  310  311  312  313  314  315  316  317  323
##  [253]  324  325  326  327  328  329  330  331  332  333  334  335  336  337
##  [267]  338  339  340  341  342  343  344  345  346  347  348  349  350  351
##  [281]  352  353  354  355  356  357  358  359  360  361  362  363  364  365
##  [295]  366  367  368  369  370  371  372  373  377  378  379  389  390  397
##  [309]  398  399  400  401  402  403  404  405  406  407  408  409  410  411
##  [323]  412  413  414  415  416  417  418  419  420  421  427  428  429  430
##  [337]  431  432  433  434  435  436  437  438  439  440  441  442  443  444
##  [351]  445  446  447  448  449  450  451  452  453  454  455  456  457  458
##  [365]  459  460  461  462  463  464  465  466  467  468  469  470  471  472
##  [379]  473  474  475  476  477  481  482  483  491  492  493  494  501  502
##  [393]  503  504  505  506  507  508  509  510  511  512  513  514  515  516
##  [407]  517  518  519  520  521  522  529  530  531  532  533  534  535  536
##  [421]  537  538  539  540  541  542  543  544  545  546  547  548  549  550
##  [435]  551  552  553  554  555  556  557  558  559  560  561  562  563  564
##  [449]  565  566  567  568  569  570  571  572  573  574  575  576  577  578
##  [463]  579  580  581  585  586  587  588  589  595  596  605  606  607  608
##  [477]  609  610  611  612  613  614  615  616  617  618  619  620  621  622
##  [491]  623  624  625  626  633  634  635  636  637  638  639  640  641  642
##  [505]  643  644  645  646  647  648  649  650  651  652  653  654  655  656
##  [519]  657  658  659  660  661  662  663  664  665  666  667  668  669  670
##  [533]  671  672  673  674  675  676  677  678  679  680  681  682  683  684
##  [547]  685  689  690  691  699  700  709  710  711  712  713  714  715  716
##  [561]  717  718  719  720  721  722  723  724  725  726  727  728  729  730
##  [575]  731  732  733  739  740  741  742  743  744  745  746  747  748  749
##  [589]  750  751  752  753  754  755  756  757  758  759  760  761  762  763
##  [603]  764  765  766  767  768  769  770  771  772  773  774  775  776  777
##  [617]  778  779  780  781  782  783  784  785  786  787  788  789  793  794
##  [631]  795  805  806  813  814  815  816  817  818  819  820  821  822  823
##  [645]  824  825  826  827  828  829  830  831  832  833  834  835  836  837
##  [659]  841  842  843  844  845  846  847  848  849  850  851  852  853  854
##  [673]  855  856  857  858  859  860  861  862  863  864  865  866  867  868
##  [687]  869  870  871  872  873  874  875  876  877  878  879  880  881  882
##  [701]  883  884  885  886  887  888  889  890  891  892  893  897  898  899
##  [715]  907  908  909  910  917  918  919  920  921  922  923  924  925  926
##  [729]  927  928  929  930  931  932  933  934  935  936  937  938  939  940
##  [743]  941  945  946  947  948  949  950  951  952  953  954  955  956  957
##  [757]  958  959  960  961  962  963  964  965  966  967  968  969  970  971
##  [771]  972  973  974  975  976  977  978  979  980  981  982  983  984  985
##  [785]  986  987  988  989  990  991  992  993  994  995  996  997 1001 1002
##  [799] 1003 1015 1016 1017 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030
##  [813] 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044
##  [827] 1045 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061
##  [841] 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075
##  [855] 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089
##  [869] 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1105 1106
##  [883] 1107 1108 1109 1115 1116 1119 1120 1121 1125 1126 1127 1128 1129 1130
##  [897] 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144
##  [911] 1145 1146 1147 1148 1149 1153 1154 1155 1156 1157 1158 1159 1160 1161
##  [925] 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175
##  [939] 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189
##  [953] 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203
##  [967] 1204 1205 1209 1210 1211 1212 1213 1223 1224 1225 1229 1230 1231 1232
##  [981] 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246
##  [995] 1247 1248 1249 1250 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266
## [1009] 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280
## [1023] 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294
## [1037] 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308
## [1051] 1309 1313 1314 1315 1316 1317 1327 1328 1329 1333 1334 1335 1336 1337
## [1065] 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351
## [1079] 1352 1353 1354 1355 1356 1357 1363 1364 1365 1366 1367 1368 1369 1370
## [1093] 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384
## [1107] 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398
## [1121] 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412
## [1135] 1413 1417 1418 1419 1431 1432 1433 1437 1438 1439 1440 1441 1442 1443
## [1149] 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457
## [1163] 1458 1459 1460 1461 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476
## [1177] 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490
## [1191] 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504
## [1205] 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1521
## [1219] 1522 1523 1524 1525 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550
## [1233] 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1569 1570
## [1247] 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584
## [1261] 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598
## [1275] 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612
## [1289] 1613 1614 1615 1616 1617 1618 1619 1620 1621 1625 1626 1627 1628 1629
## [1303] 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658
## [1317] 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1675 1676 1677
## [1331] 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691
## [1345] 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705
## [1359] 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719
## [1373] 1720 1721 1722 1723 1724 1725 1726 1727 1728 1732 1733 1749 1750 1751
## [1387] 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765
## [1401] 1766 1767 1768 1769 1770 1777 1778 1779 1780 1781 1782 1783 1784 1785
## [1415] 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799
## [1429] 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813
## [1443] 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827
## [1457] 1828 1829 1833 1834 1835 1847 1848 1849 1853 1854 1855 1856 1857 1858
## [1471] 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872
## [1485] 1873 1874 1875 1876 1877 1883 1884 1885 1886 1887 1888 1889 1890 1891
## [1499] 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905
## [1513] 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919
## [1527] 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933
## [1541] 1934 1935 1936 1937 1938 1939 1942 1943 1944 1957 1958 1959 1960 1961
## [1555] 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975
## [1569] 1976 1977 1978 1979 1980 1981 1985 1986 1987 1988 1989 1990 1991 1992
## [1583] 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006
## [1597] 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020
## [1611] 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034
## [1625] 2035 2036 2037 2038 2039 2040 2041 2042 2043 2046 2047 2048 2053 2054
## [1639] 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074
## [1653] 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2091 2092 2093
## [1667] 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107
## [1681] 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121
## [1695] 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135
## [1709] 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2150 2151
## [1723] 2152 2157 2158 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175
## [1737] 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189
## [1751] 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208
## [1765] 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222
## [1779] 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236
## [1793] 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2254 2255
## [1807] 2256 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281
## [1821] 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295
## [1835] 2296 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311
## [1849] 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325
## [1863] 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339
## [1877] 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2358
## [1891] 2359 2360 2361 2362 2363 2364 2367 2368 2369 2373 2374 2375 2376 2377
## [1905] 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391
## [1919] 2392 2393 2394 2395 2396 2397 2398 2399 2400 2403 2404 2405 2406 2407
## [1933] 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421
## [1947] 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435
## [1961] 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449
## [1975] 2450 2451 2452 2453 2454 2455 2456 2462 2463 2464 2471 2472 2473 2477
## [1989] 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491
## [2003] 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2505 2506 2507 2508
## [2017] 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522
## [2031] 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536
## [2045] 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550
## [2059] 2551 2552 2553 2554 2555 2556 2557 2561 2562 2563 2571 2572 2573 2574
## [2073] 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594
## [2087] 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2609 2610 2611
## [2101] 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625
## [2115] 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639
## [2129] 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653
## [2143] 2654 2655 2656 2657 2658 2659 2660 2661 2665 2666 2667 2677 2678 2679
## [2157] 2680 2681 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696
## [2171] 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2713
## [2185] 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727
## [2199] 2728 2729 2730 2731 2732 2733 2734 2735 2736 2737 2738 2739 2740 2741
## [2213] 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754 2755
## [2227] 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2769 2770 2771 2781
## [2241] 2782 2789 2790 2791 2792 2793 2794 2795 2796 2797 2798 2799 2800 2801
## [2255] 2802 2803 2804 2805 2806 2807 2808 2809 2810 2811 2812 2813 2817 2818
## [2269] 2819 2820 2821 2822 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832
## [2283] 2833 2834 2835 2836 2837 2838 2839 2840 2841 2842 2843 2844 2845 2846
## [2297] 2847 2848 2849 2850 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860
## [2311] 2861 2862 2863 2864 2865 2866 2867 2868 2869 2873 2874 2875 2885 2886
## [2325] 2893 2894 2895 2896 2897 2898 2899 2900 2901 2902 2903 2904 2905 2906
## [2339] 2907 2908 2909 2910 2911 2912 2913 2914 2915 2916 2917 2923 2924 2925
## [2353] 2926 2927 2928 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939
## [2367] 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953
## [2381] 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967
## [2395] 2968 2969 2970 2971 2972 2973 2977 2978 2979 2982 2983 2984 2989 2990
## [2409] 2991 2992 2993 2997 2998 2999 3000 3001 3002 3003 3004 3005 3006 3007
## [2423] 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021
## [2437] 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040
## [2451] 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053 3054
## [2465] 3055 3056 3057 3058 3059 3060 3061 3062 3063 3064 3065 3066 3067 3068
## [2479] 3069 3070 3071 3072 3073 3074 3075 3076 3077 3081 3082 3083 3093 3094
## [2493] 3095 3096 3097 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111
## [2507] 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125
## [2521] 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144
## [2535] 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154 3155 3156 3157 3158
## [2549] 3159 3160 3161 3162 3163 3164 3165 3166 3167 3168 3169 3170 3171 3172
## [2563] 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186
## [2577] 3187 3190 3191 3192 3197 3198 3199 3200 3201 3205 3206 3207 3208 3209
## [2591] 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3223
## [2605] 3224 3225 3226 3227 3228 3229 3235 3236 3237 3238 3239 3240 3241 3242
## [2619] 3243 3244 3245 3246 3247 3248 3249 3250 3251 3252 3253 3254 3255 3256
## [2633] 3257 3258 3259 3260 3261 3262 3263 3264 3265 3266 3267 3268 3269 3270
## [2647] 3271 3272 3273 3274 3275 3276 3277 3278 3279 3280 3281 3282 3283 3284
## [2661] 3285 3289 3290 3291 3303 3304 3305 3309 3310 3311 3312 3313 3314 3315
## [2675] 3316 3317 3318 3319 3320 3321 3322 3323 3324 3325 3326 3327 3328 3329
## [2689] 3330 3331 3332 3333 3339 3340 3341 3342 3343 3344 3345 3346 3347 3348
## [2703] 3349 3350 3351 3352 3353 3354 3355 3356 3357 3358 3359 3360 3361 3362
## [2717] 3363 3364 3365 3366 3367 3368 3369 3370 3371 3372 3373 3374 3375 3376
## [2731] 3377 3378 3379 3380 3381 3382 3383 3384 3385 3386 3387 3388 3389 3393
## [2745] 3394 3395 3407 3408 3409 3413 3414 3415 3416 3417 3418 3419 3420 3421
## [2759] 3422 3423 3424 3425 3426 3427 3428 3429 3430 3431 3432 3433 3434 3435
## [2773] 3436 3437 3443 3444 3445 3446 3447 3448 3449 3450 3451 3452 3453 3454
## [2787] 3455 3456 3457 3458 3459 3460 3461 3462 3463 3464 3465 3466 3467 3468
## [2801] 3469 3470 3471 3472 3473 3474 3475 3476 3477 3478 3479 3480 3481 3482
## [2815] 3483 3484 3485 3486 3487 3488 3489 3490 3491 3492 3493 3497 3498 3499
## [2829] 3511 3512 3513 3517 3518 3519 3520 3521 3522 3523 3524 3525 3526 3527
## [2843] 3528 3529 3530 3531 3532 3533 3534 3535 3536 3537 3538 3539 3540 3541
## [2857] 3547 3548 3549 3550 3551 3552 3553 3554 3555 3556 3557 3558 3559 3560
## [2871] 3561 3562 3563 3564 3565 3566 3567 3568 3569 3570 3571 3572 3573 3574
## [2885] 3575 3576 3577 3578 3579 3580 3581 3582 3583 3584 3585 3586 3587 3588
## [2899] 3589 3590 3591 3592 3593 3594 3595 3596 3597 3601 3602 3603 3615 3616
## [2913] 3617 3621 3622 3623 3624 3625 3626 3627 3628 3629 3630 3631 3632 3633
## [2927] 3634 3635 3636 3637 3638 3639 3640 3641 3642 3643 3644 3645 3651 3652
## [2941] 3653 3654 3655 3656 3657 3658 3659 3660 3661 3662 3663 3664 3665 3666
## [2955] 3667 3668 3669 3670 3671 3672 3673 3674 3675 3676 3677 3678 3679 3680
## [2969] 3681 3682 3683 3684 3685 3686 3687 3688 3689 3690 3691 3692 3693 3694
## [2983] 3695 3696 3697 3698 3699 3700 3701 3705 3706 3707 3719 3720 3721 3725
## [2997] 3726 3727 3728 3729 3730 3731 3732 3733 3734 3735 3736 3737 3738 3739
## [3011] 3740 3741 3742 3743 3744 3745 3746 3747 3748 3749 3755 3756 3757 3758
## [3025] 3759 3760 3761 3762 3763 3764 3765 3766 3767 3768 3769 3770 3771 3772
## [3039] 3773 3774 3775 3776 3777 3778 3779 3780 3781 3782 3783 3784 3785 3786
## [3053] 3787 3788 3789 3790 3791 3792 3793 3794 3795 3796 3797 3798 3799 3800
## [3067] 3801 3802 3803 3804 3805 3806 3807 3808 3814 3815 3816 3823 3824 3825
## [3081] 3829 3830 3831 3832 3833 3834 3835 3836 3837 3838 3839 3840 3841 3842
## [3095] 3843 3844 3845 3846 3847 3848 3849 3850 3851 3852 3853 3854 3855 3856
## [3109] 3859 3860 3861 3862 3863 3864 3865 3866 3867 3868 3869 3870 3871 3872
## [3123] 3873 3874 3875 3876 3877 3878 3879 3880 3881 3882 3883 3884 3885 3886
## [3137] 3887 3888 3889 3890 3891 3892 3893 3894 3895 3896 3897 3898 3899 3900
## [3151] 3901 3902 3903 3904 3905 3906 3907 3908 3909 3910 3911 3912 3918 3919
## [3165] 3920 3921 3922 3923 3924 3933 3934 3935 3936 3937 3938 3939 3940 3941
## [3179] 3942 3943 3944 3945 3946 3947 3948 3949 3950 3951 3952 3953 3954 3955
## [3193] 3956 3957 3963 3964 3965 3966 3967 3968 3969 3970 3971 3972 3973 3974
## [3207] 3975 3976 3977 3978 3979 3980 3981 3982 3983 3984 3985 3986 3987 3988
## [3221] 3989 3990 3991 3992 3993 3994 3995 3996 3997 3998 3999 4000 4001 4002
## [3235] 4003 4004 4005 4006 4007 4008 4009 4010 4011 4012 4013 4017 4018 4019
## [3249] 4020 4021 4037 4038 4039 4040 4041 4042 4043 4044 4045 4046 4047 4048
## [3263] 4049 4050 4051 4052 4053 4054 4055 4056 4057 4058 4059 4060 4061 4067
## [3277] 4068 4069 4070 4071 4072 4073 4074 4075 4076 4077 4078 4079 4080 4081
## [3291] 4082 4083 4084 4085 4086 4087 4088 4089 4090 4091 4092 4093 4094 4095
## [3305] 4096 4097 4098 4099 4100 4101 4102 4103 4104 4105 4106 4107 4108 4109
## [3319] 4110 4111 4112 4113 4114 4115 4116 4117 4118 4119 4120 4138 4139 4140
## [3333] 4141 4142 4143 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154
## [3347] 4155 4156 4157 4158 4159 4160 4161 4162 4163 4164 4165 4171 4172 4173
## [3361] 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183 4184 4185 4186 4187
## [3375] 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197 4198 4199 4200 4201
## [3389] 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211 4212 4213 4214 4215
## [3403] 4216 4217 4218 4219 4220 4221 4222 4223 4224 4228 4229 4242 4243 4244
## [3417] 4245 4246 4247 4248 4249 4250 4251 4252 4253 4254 4255 4256 4257 4258
## [3431] 4259 4260 4261 4262 4263 4264 4265 4266 4267 4268 4269 4275 4276 4277
## [3445] 4278 4279 4280 4281 4282 4283 4284 4285 4286 4287 4288 4289 4290 4291
## [3459] 4292 4293 4294 4295 4296 4297 4298 4299 4300 4301 4302 4303 4304 4305
## [3473] 4306 4307 4308 4309 4310 4311 4312 4313 4314 4315 4316 4317 4318 4319
## [3487] 4320 4321 4322 4323 4324 4325 4329 4330 4331 4332 4333 4339 4340 4349
## [3501] 4350 4351 4352 4353 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363
## [3515] 4364 4365 4366 4367 4368 4369 4370 4371 4372 4373 4379 4380 4381 4382
## [3529] 4383 4384 4385 4386 4387 4388 4389 4390 4391 4392 4393 4394 4395 4396
## [3543] 4397 4398 4399 4400 4401 4402 4403 4404 4405 4406 4407 4408 4409 4410
## [3557] 4411 4412 4413 4414 4415 4416 4417 4418 4419 4420 4421 4422 4423 4424
## [3571] 4425 4426 4427 4428 4429 4430 4431 4432 4436 4437 4453 4454 4455 4456
## [3585] 4457 4458 4459 4460 4461 4462 4463 4464 4465 4466 4467 4468 4469 4470
## [3599] 4471 4472 4473 4474 4475 4476 4477 4483 4484 4485 4486 4487 4488 4489
## [3613] 4490 4491 4492 4493 4494 4495 4496 4497 4498 4499 4500 4501 4502 4503
## [3627] 4504 4505 4506 4507 4508 4509 4510 4511 4512 4513 4514 4515 4516 4517
## [3641] 4518 4519 4520 4521 4522 4523 4524 4525 4526 4527 4528 4529 4530 4531
## [3655] 4532 4533 4534 4535 4536 4554 4555 4556 4557 4558 4559 4560 4561 4562
## [3669] 4563 4564 4565 4566 4567 4568 4569 4570 4571 4572 4573 4574 4575 4576
## [3683] 4577 4578 4579 4580 4581 4587 4588 4589 4590 4591 4592 4593 4594 4595
## [3697] 4596 4597 4598 4599 4600 4601 4602 4603 4604 4605 4606 4607 4608 4609
## [3711] 4610 4611 4612 4613 4614 4615 4616 4617 4618 4619 4620 4621 4622 4623
## [3725] 4624 4625 4626 4627 4628 4629 4630 4631 4632 4633 4634 4635 4636 4637
## [3739] 4638 4639 4640 4658 4659 4660 4661 4662 4663 4664 4665 4666 4667 4668
## [3753] 4669 4670 4671 4672 4673 4674 4675 4676 4677 4678 4679 4680 4681 4682
## [3767] 4686 4687 4688 4691 4692 4693 4694 4695 4696 4697 4698 4699 4700 4701
## [3781] 4702 4703 4704 4705 4706 4707 4708 4709 4710 4711 4712 4713 4714 4715
## [3795] 4716 4717 4718 4719 4720 4721 4722 4723 4724 4725 4726 4727 4728 4729
## [3809] 4730 4731 4732 4733 4734 4735 4736 4737 4738 4739 4740 4741 4742 4743
## [3823] 4744 4753 4754 4765 4766 4767 4768 4769 4770 4771 4772 4773 4774 4775
## [3837] 4776 4777 4778 4779 4780 4781 4782 4783 4784 4785 4786 4790 4791 4792
## [3851] 4795 4796 4797 4798 4799 4800 4801 4802 4803 4804 4805 4806 4807 4808
## [3865] 4809 4810 4811 4812 4813 4814 4815 4816 4817 4818 4819 4820 4821 4822
## [3879] 4823 4824 4825 4826 4827 4828 4829 4830 4831 4832 4833 4834 4835 4836
## [3893] 4837 4838 4839 4840 4841 4842 4843 4844 4845 4846 4847 4848 4857 4858
## [3907] 4869 4870 4871 4872 4873 4874 4875 4876 4877 4878 4879 4880 4881 4882
## [3921] 4883 4884 4885 4886 4887 4888 4889 4890 4894 4895 4896 4899 4900 4901
## [3935] 4902 4903 4904 4905 4906 4907 4908 4909 4910 4911 4912 4913 4914 4915
## [3949] 4916 4917 4918 4919 4920 4921 4922 4923 4924 4925 4926 4927 4928 4929
## [3963] 4930 4931 4932 4933 4934 4935 4936 4937 4938 4939 4940 4941 4942 4943
## [3977] 4944 4945 4946 4947 4948 4949 4950 4951 4952 4961 4962 4973 4974 4975
## [3991] 4976 4977 4978 4979 4980 4981 4982 4983 4984 4985 4986 4987 4988 4989
## [4005] 4990 4991 4992 4993 4994 4998 4999 5000 5003 5004 5005 5006 5007 5008
## [4019] 5009 5010 5011 5012 5013 5014 5015 5016 5017 5018 5019 5020 5021 5022
## [4033] 5023 5024 5025 5026 5027 5028 5029 5030 5031 5032 5033 5034 5035 5036
## [4047] 5037 5038 5039 5040 5041 5042 5043 5044 5045 5046 5047 5048 5049 5050
## [4061] 5051 5052 5053 5054 5055 5056 5062 5063 5064 5065 5066 5077 5078 5079
## [4075] 5080 5081 5082 5083 5084 5085 5086 5087 5088 5089 5090 5091 5092 5093
## [4089] 5094 5095 5096 5097 5098 5099 5100 5101 5102 5103 5104 5105 5106 5107
## [4103] 5108 5109 5110 5111 5112 5113 5114 5115 5116 5117 5118 5119 5120 5121
## [4117] 5122 5123 5124 5125 5126 5127 5128 5129 5130 5131 5132 5133 5134 5135
## [4131] 5136 5137 5138 5139 5140 5141 5142 5143 5144 5145 5146 5147 5148 5149
## [4145] 5150 5151 5152 5153 5154 5155 5156 5157 5158 5159 5160 5161 5162 5163
## [4159] 5164 5165 5166 5167 5168 5169 5170 5171 5172 5173 5174 5175 5176 5177
## [4173] 5178 5179 5180 5181 5182 5183 5184 5185 5186 5187 5188 5189 5190 5191
## [4187] 5192 5193 5194 5195 5196 5197 5198 5199 5200 5201 5202 5203 5204 5205
## [4201] 5206 5207 5208 5209 5210 5211 5212 5213 5214 5215 5216 5217 5218 5219
## [4215] 5220 5221 5222 5223 5224 5225 5226 5227 5228 5229 5230 5231 5232 5233
## [4229] 5234 5235 5236 5237 5238 5239 5240 5241 5242 5243 5244 5245 5246 5247
## [4243] 5248 5249 5250 5251 5252 5253 5254 5255 5256 5257 5258 5259 5260 5261
## [4257] 5262 5263 5264 5265 5266 5267 5268 5269 5270 5271 5272 5273 5274 5275
## [4271] 5276 5277 5278 5279 5280 5281 5282 5283 5284 5285 5286 5287 5288 5289
## [4285] 5290 5291 5292 5293 5294 5295 5296 5297 5298 5299 5300 5301 5302 5303
## [4299] 5304
## 
## $class
## [1] &quot;data.frame&quot;
## 
## $na.action
##    6    7    8    9   10   62   63   64   68   69   70   71   72   73   74   75 
##    6    7    8    9   10   62   63   64   68   69   70   71   72   73   74   75 
##   76   79   80   81   82   83   84  110  111  112  113  114  166  167  168  172 
##   76   79   80   81   82   83   84  110  111  112  113  114  166  167  168  172 
##  173  174  175  176  177  178  179  180  183  184  185  186  187  188  214  215 
##  173  174  175  176  177  178  179  180  183  184  185  186  187  188  214  215 
##  216  217  218  273  274  275  276  277  281  282  285  286  287  288  289  290 
##  216  217  218  273  274  275  276  277  281  282  285  286  287  288  289  290 
##  291  292  318  319  320  321  322  374  375  376  380  381  382  383  384  385 
##  291  292  318  319  320  321  322  374  375  376  380  381  382  383  384  385 
##  386  387  388  391  392  393  394  395  396  422  423  424  425  426  478  479 
##  386  387  388  391  392  393  394  395  396  422  423  424  425  426  478  479 
##  480  484  485  486  487  488  489  490  495  496  497  498  499  500  523  524 
##  480  484  485  486  487  488  489  490  495  496  497  498  499  500  523  524 
##  525  526  527  528  582  583  584  590  591  592  593  594  597  598  599  600 
##  525  526  527  528  582  583  584  590  591  592  593  594  597  598  599  600 
##  601  602  603  604  627  628  629  630  631  632  686  687  688  692  693  694 
##  601  602  603  604  627  628  629  630  631  632  686  687  688  692  693  694 
##  695  696  697  698  701  702  703  704  705  706  707  708  734  735  736  737 
##  695  696  697  698  701  702  703  704  705  706  707  708  734  735  736  737 
##  738  790  791  792  796  797  798  799  800  801  802  803  804  807  808  809 
##  738  790  791  792  796  797  798  799  800  801  802  803  804  807  808  809 
##  810  811  812  838  839  840  894  895  896  900  901  902  903  904  905  906 
##  810  811  812  838  839  840  894  895  896  900  901  902  903  904  905  906 
##  911  912  913  914  915  916  942  943  944  998  999 1000 1004 1005 1006 1007 
##  911  912  913  914  915  916  942  943  944  998  999 1000 1004 1005 1006 1007 
## 1008 1009 1010 1011 1012 1013 1014 1018 1019 1020 1046 1047 1048 1102 1103 1104 
## 1008 1009 1010 1011 1012 1013 1014 1018 1019 1020 1046 1047 1048 1102 1103 1104 
## 1110 1111 1112 1113 1114 1117 1118 1122 1123 1124 1150 1151 1152 1206 1207 1208 
## 1110 1111 1112 1113 1114 1117 1118 1122 1123 1124 1150 1151 1152 1206 1207 1208 
## 1214 1215 1216 1217 1218 1219 1220 1221 1222 1226 1227 1228 1251 1252 1253 1254 
## 1214 1215 1216 1217 1218 1219 1220 1221 1222 1226 1227 1228 1251 1252 1253 1254 
## 1255 1256 1310 1311 1312 1318 1319 1320 1321 1322 1323 1324 1325 1326 1330 1331 
## 1255 1256 1310 1311 1312 1318 1319 1320 1321 1322 1323 1324 1325 1326 1330 1331 
## 1332 1358 1359 1360 1361 1362 1414 1415 1416 1420 1421 1422 1423 1424 1425 1426 
## 1332 1358 1359 1360 1361 1362 1414 1415 1416 1420 1421 1422 1423 1424 1425 1426 
## 1427 1428 1429 1430 1434 1435 1436 1462 1463 1464 1465 1466 1518 1519 1520 1526 
## 1427 1428 1429 1430 1434 1435 1436 1462 1463 1464 1465 1466 1518 1519 1520 1526 
## 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1563 1564 
## 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1563 1564 
## 1565 1566 1567 1568 1622 1623 1624 1630 1631 1632 1633 1634 1635 1636 1637 1638 
## 1565 1566 1567 1568 1622 1623 1624 1630 1631 1632 1633 1634 1635 1636 1637 1638 
## 1639 1640 1641 1642 1643 1644 1670 1671 1672 1673 1674 1729 1730 1731 1734 1735 
## 1639 1640 1641 1642 1643 1644 1670 1671 1672 1673 1674 1729 1730 1731 1734 1735 
## 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1771 1772 1773 
## 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1771 1772 1773 
## 1774 1775 1776 1830 1831 1832 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 
## 1774 1775 1776 1830 1831 1832 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 
## 1846 1850 1851 1852 1878 1879 1880 1881 1882 1940 1941 1945 1946 1947 1948 1949 
## 1846 1850 1851 1852 1878 1879 1880 1881 1882 1940 1941 1945 1946 1947 1948 1949 
## 1950 1951 1952 1953 1954 1955 1956 1982 1983 1984 2044 2045 2049 2050 2051 2052 
## 1950 1951 1952 1953 1954 1955 1956 1982 1983 1984 2044 2045 2049 2050 2051 2052 
## 2055 2056 2057 2058 2059 2060 2086 2087 2088 2089 2090 2148 2149 2153 2154 2155 
## 2055 2056 2057 2058 2059 2060 2086 2087 2088 2089 2090 2148 2149 2153 2154 2155 
## 2156 2159 2160 2161 2162 2163 2164 2190 2191 2192 2193 2194 2249 2250 2251 2252 
## 2156 2159 2160 2161 2162 2163 2164 2190 2191 2192 2193 2194 2249 2250 2251 2252 
## 2253 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2297 2298 2353 
## 2253 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2297 2298 2353 
## 2354 2355 2356 2357 2365 2366 2370 2371 2372 2401 2402 2457 2458 2459 2460 2461 
## 2354 2355 2356 2357 2365 2366 2370 2371 2372 2401 2402 2457 2458 2459 2460 2461 
## 2465 2466 2467 2468 2469 2470 2474 2475 2476 2502 2503 2504 2558 2559 2560 2564 
## 2465 2466 2467 2468 2469 2470 2474 2475 2476 2502 2503 2504 2558 2559 2560 2564 
## 2565 2566 2567 2568 2569 2570 2575 2576 2577 2578 2579 2580 2606 2607 2608 2662 
## 2565 2566 2567 2568 2569 2570 2575 2576 2577 2578 2579 2580 2606 2607 2608 2662 
## 2663 2664 2668 2669 2670 2671 2672 2673 2674 2675 2676 2682 2683 2684 2710 2711 
## 2663 2664 2668 2669 2670 2671 2672 2673 2674 2675 2676 2682 2683 2684 2710 2711 
## 2712 2766 2767 2768 2772 2773 2774 2775 2776 2777 2778 2779 2780 2783 2784 2785 
## 2712 2766 2767 2768 2772 2773 2774 2775 2776 2777 2778 2779 2780 2783 2784 2785 
## 2786 2787 2788 2814 2815 2816 2870 2871 2872 2876 2877 2878 2879 2880 2881 2882 
## 2786 2787 2788 2814 2815 2816 2870 2871 2872 2876 2877 2878 2879 2880 2881 2882 
## 2883 2884 2887 2888 2889 2890 2891 2892 2918 2919 2920 2921 2922 2974 2975 2976 
## 2883 2884 2887 2888 2889 2890 2891 2892 2918 2919 2920 2921 2922 2974 2975 2976 
## 2980 2981 2985 2986 2987 2988 2994 2995 2996 3022 3023 3024 3025 3026 3078 3079 
## 2980 2981 2985 2986 2987 2988 2994 2995 2996 3022 3023 3024 3025 3026 3078 3079 
## 3080 3084 3085 3086 3087 3088 3089 3090 3091 3092 3098 3099 3100 3126 3127 3128 
## 3080 3084 3085 3086 3087 3088 3089 3090 3091 3092 3098 3099 3100 3126 3127 3128 
## 3129 3130 3188 3189 3193 3194 3195 3196 3202 3203 3204 3230 3231 3232 3233 3234 
## 3129 3130 3188 3189 3193 3194 3195 3196 3202 3203 3204 3230 3231 3232 3233 3234 
## 3286 3287 3288 3292 3293 3294 3295 3296 3297 3298 3299 3300 3301 3302 3306 3307 
## 3286 3287 3288 3292 3293 3294 3295 3296 3297 3298 3299 3300 3301 3302 3306 3307 
## 3308 3334 3335 3336 3337 3338 3390 3391 3392 3396 3397 3398 3399 3400 3401 3402 
## 3308 3334 3335 3336 3337 3338 3390 3391 3392 3396 3397 3398 3399 3400 3401 3402 
## 3403 3404 3405 3406 3410 3411 3412 3438 3439 3440 3441 3442 3494 3495 3496 3500 
## 3403 3404 3405 3406 3410 3411 3412 3438 3439 3440 3441 3442 3494 3495 3496 3500 
## 3501 3502 3503 3504 3505 3506 3507 3508 3509 3510 3514 3515 3516 3542 3543 3544 
## 3501 3502 3503 3504 3505 3506 3507 3508 3509 3510 3514 3515 3516 3542 3543 3544 
## 3545 3546 3598 3599 3600 3604 3605 3606 3607 3608 3609 3610 3611 3612 3613 3614 
## 3545 3546 3598 3599 3600 3604 3605 3606 3607 3608 3609 3610 3611 3612 3613 3614 
## 3618 3619 3620 3646 3647 3648 3649 3650 3702 3703 3704 3708 3709 3710 3711 3712 
## 3618 3619 3620 3646 3647 3648 3649 3650 3702 3703 3704 3708 3709 3710 3711 3712 
## 3713 3714 3715 3716 3717 3718 3722 3723 3724 3750 3751 3752 3753 3754 3809 3810 
## 3713 3714 3715 3716 3717 3718 3722 3723 3724 3750 3751 3752 3753 3754 3809 3810 
## 3811 3812 3813 3817 3818 3819 3820 3821 3822 3826 3827 3828 3857 3858 3913 3914 
## 3811 3812 3813 3817 3818 3819 3820 3821 3822 3826 3827 3828 3857 3858 3913 3914 
## 3915 3916 3917 3925 3926 3927 3928 3929 3930 3931 3932 3958 3959 3960 3961 3962 
## 3915 3916 3917 3925 3926 3927 3928 3929 3930 3931 3932 3958 3959 3960 3961 3962 
## 4014 4015 4016 4022 4023 4024 4025 4026 4027 4028 4029 4030 4031 4032 4033 4034 
## 4014 4015 4016 4022 4023 4024 4025 4026 4027 4028 4029 4030 4031 4032 4033 4034 
## 4035 4036 4062 4063 4064 4065 4066 4121 4122 4123 4124 4125 4126 4127 4128 4129 
## 4035 4036 4062 4063 4064 4065 4066 4121 4122 4123 4124 4125 4126 4127 4128 4129 
## 4130 4131 4132 4133 4134 4135 4136 4137 4166 4167 4168 4169 4170 4225 4226 4227 
## 4130 4131 4132 4133 4134 4135 4136 4137 4166 4167 4168 4169 4170 4225 4226 4227 
## 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239 4240 4241 4270 4271 4272 4273 
## 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239 4240 4241 4270 4271 4272 4273 
## 4274 4326 4327 4328 4334 4335 4336 4337 4338 4341 4342 4343 4344 4345 4346 4347 
## 4274 4326 4327 4328 4334 4335 4336 4337 4338 4341 4342 4343 4344 4345 4346 4347 
## 4348 4374 4375 4376 4377 4378 4433 4434 4435 4438 4439 4440 4441 4442 4443 4444 
## 4348 4374 4375 4376 4377 4378 4433 4434 4435 4438 4439 4440 4441 4442 4443 4444 
## 4445 4446 4447 4448 4449 4450 4451 4452 4478 4479 4480 4481 4482 4537 4538 4539 
## 4445 4446 4447 4448 4449 4450 4451 4452 4478 4479 4480 4481 4482 4537 4538 4539 
## 4540 4541 4542 4543 4544 4545 4546 4547 4548 4549 4550 4551 4552 4553 4582 4583 
## 4540 4541 4542 4543 4544 4545 4546 4547 4548 4549 4550 4551 4552 4553 4582 4583 
## 4584 4585 4586 4641 4642 4643 4644 4645 4646 4647 4648 4649 4650 4651 4652 4653 
## 4584 4585 4586 4641 4642 4643 4644 4645 4646 4647 4648 4649 4650 4651 4652 4653 
## 4654 4655 4656 4657 4683 4684 4685 4689 4690 4745 4746 4747 4748 4749 4750 4751 
## 4654 4655 4656 4657 4683 4684 4685 4689 4690 4745 4746 4747 4748 4749 4750 4751 
## 4752 4755 4756 4757 4758 4759 4760 4761 4762 4763 4764 4787 4788 4789 4793 4794 
## 4752 4755 4756 4757 4758 4759 4760 4761 4762 4763 4764 4787 4788 4789 4793 4794 
## 4849 4850 4851 4852 4853 4854 4855 4856 4859 4860 4861 4862 4863 4864 4865 4866 
## 4849 4850 4851 4852 4853 4854 4855 4856 4859 4860 4861 4862 4863 4864 4865 4866 
## 4867 4868 4891 4892 4893 4897 4898 4953 4954 4955 4956 4957 4958 4959 4960 4963 
## 4867 4868 4891 4892 4893 4897 4898 4953 4954 4955 4956 4957 4958 4959 4960 4963 
## 4964 4965 4966 4967 4968 4969 4970 4971 4972 4995 4996 4997 5001 5002 5057 5058 
## 4964 4965 4966 4967 4968 4969 4970 4971 4972 4995 4996 4997 5001 5002 5057 5058 
## 5059 5060 5061 5067 5068 5069 5070 5071 5072 5073 5074 5075 5076 
## 5059 5060 5061 5067 5068 5069 5070 5071 5072 5073 5074 5075 5076 
## attr(,&quot;class&quot;)
## [1] &quot;omit&quot;</code></pre>
<pre class="r"><code>mont &lt;- rename(mont, &quot;office&quot; = &quot;Contest Title&quot;, &quot;party&quot; = &quot;Party&quot;,
               &quot;candidate&quot; = &quot;Candidate&quot;)

#the recode function is used to clean the values within the party variable
unique(mont$party)</code></pre>
<pre><code>## [1] &quot;DEM&quot; &quot;REP&quot; &quot;NON&quot; &quot;IND&quot;</code></pre>
<pre class="r"><code>mont$party &lt;- recode(mont$party, &quot;DEM&quot; = &quot;Democrat&quot;, &quot;REP&quot; = &quot;Republican&quot;, &quot;NON&quot; = &quot;No party&quot;,
               &quot;IND&quot; = &quot;Independent&quot;)

#you can create new variables for the state and county
mont$state &lt;- &quot;Alabama&quot;
mont$county &lt;- &quot;Montgomery&quot;

#the &quot;c&quot; function reorders the columns
mont &lt;- mont[, c(6, 7, 4, 1, 3, 2, 5)]</code></pre>
<p><strong>Question 1b</strong></p>
<pre class="r"><code>#1. b. You can use a for loop to find the total number of votes in each office 

#first create vectors with what you want to loop over
office &lt;- unique(mont$office)
office</code></pre>
<pre><code>##  [1] &quot;STRAIGHT PARTY&quot;                                          
##  [2] &quot;UNITED STATES REPRESENTATIVE, 2ND CONGRESSIONAL DISTRICT&quot;
##  [3] &quot;GOVERNOR&quot;                                                
##  [4] &quot;LIEUTENANT GOVERNOR&quot;                                     
##  [5] &quot;ATTORNEY GENERAL&quot;                                        
##  [6] &quot;CHIEF JUSTICE OF THE SUPREME COURT&quot;                      
##  [7] &quot;ASSOCIATE JUSTICE OF THE SUPREME COURT, PLACE 1&quot;         
##  [8] &quot;ASSOCIATE JUSTICE OF THE SUPREME COURT, PLACE 2&quot;         
##  [9] &quot;ASSOCIATE JUSTICE OF THE SUPREME COURT, PLACE 3&quot;         
## [10] &quot;ASSOCIATE JUSTICE OF THE SUPREME COURT, PLACE 4&quot;         
## [11] &quot;STATE TREASURER&quot;                                         
## [12] &quot;COMMISSIONER OF AGRICULTURE AND INDUSTRIES&quot;              
## [13] &quot;SECRETARY OF STATE&quot;                                      
## [14] &quot;STATE AUDITOR&quot;                                           
## [15] &quot;COURT OF CIVIL APPEALS JUDGE, PLACE 1&quot;                   
## [16] &quot;COURT OF CIVIL APPEALS JUDGE, PLACE 2&quot;                   
## [17] &quot;COURT OF CIVIL APPEALS JUDGE, PLACE 3&quot;                   
## [18] &quot;COURT OF CRIMINAL APPEALS JUDGE, PLACE 1&quot;                
## [19] &quot;COURT OF CRIMINAL APPEALS JUDGE, PLACE 2&quot;                
## [20] &quot;COURT OF CRIMINAL APPEALS JUDGE, PLACE 3&quot;                
## [21] &quot;PUBLIC SERVICE COMMISSION, PLACE 1&quot;                      
## [22] &quot;PUBLIC SERVICE COMMISSION, PLACE 2&quot;                      
## [23] &quot;CIRCUIT COURT JUDGE, 15TH JUDICIAL CIRCUIT, PLACE 6&quot;     
## [24] &quot;STATE SENATOR, DISTRICT 26&quot;                              
## [25] &quot;STATE REPRESENTATIVE, DISTRICT 77&quot;                       
## [26] &quot;PROPOSED STATEWIDE AMENDMENT NUMBER ONE (1)&quot;             
## [27] &quot;PROPOSED STATEWIDE AMENDMENT NUMBER TWO (2)&quot;             
## [28] &quot;PROPOSED STATEWIDE AMENDMENT NUMBER THREE (3)&quot;           
## [29] &quot;PROPOSED STATEWIDE AMENDMENT NUMBER FOUR (4)&quot;            
## [30] &quot;CIRCUIT CLERK, MONTGOMERY COUNTY&quot;                        
## [31] &quot;DISTRICT COURT JUDGE, MONTGOMERY COUNTY, PLACE NO. 2&quot;    
## [32] &quot;DISTRICT COURT JUDGE, MONTGOMERY COUNTY, PLACE NO. 3&quot;    
## [33] &quot;MONTGOMERY COUNTY JUDGE OF PROBATE&quot;                      
## [34] &quot;MONTGOMERY COUNTY SHERIFF&quot;                               
## [35] &quot;PROPOSED LOCAL AMENDMENT NUMBER ONE (1)&quot;                 
## [36] &quot;STATE SENATOR, DISTRICT 25&quot;                              
## [37] &quot;STATE REPRESENTATIVE, DISTRICT 74&quot;                       
## [38] &quot;STATE REPRESENTATIVE, DISTRICT 76&quot;                       
## [39] &quot;UNITED STATES REPRESENTATIVE, 7TH CONGRESSIONAL DISTRICT&quot;
## [40] &quot;STATE REPRESENTATIVE, DISTRICT 69&quot;                       
## [41] &quot;STATE REPRESENTATIVE, DISTRICT 78&quot;                       
## [42] &quot;UNITED STATES REPRESENTATIVE, 3RD CONGRESSIONAL DISTRICT&quot;
## [43] &quot;STATE REPRESENTATIVE, DISTRICT 75&quot;                       
## [44] &quot;STATE REPRESENTATIVE, DISTRICT 90&quot;</code></pre>
<pre class="r"><code>precinct &lt;- unique(mont$precinct)
precinct</code></pre>
<pre><code>##  [1] &quot;101 D&quot; &quot;102 V&quot; &quot;103 M&quot; &quot;104 W&quot; &quot;105 A&quot; &quot;106 P&quot; &quot;107 T&quot; &quot;201 S&quot; &quot;202 B&quot;
## [10] &quot;203 H&quot; &quot;204 F&quot; &quot;205 S&quot; &quot;206 M&quot; &quot;207 H&quot; &quot;208 C&quot; &quot;209 F&quot; &quot;210 P&quot; &quot;211 R&quot;
## [19] &quot;301 D&quot; &quot;302 F&quot; &quot;303 E&quot; &quot;304 L&quot; &quot;305 F&quot; &quot;306 E&quot; &quot;401 S&quot; &quot;402 M&quot; &quot;403 C&quot;
## [28] &quot;404 A&quot; &quot;405 H&quot; &quot;406 N&quot; &quot;407 K&quot; &quot;408 H&quot; &quot;409 C&quot; &quot;410 S&quot; &quot;411 U&quot; &quot;412 U&quot;
## [37] &quot;413 W&quot; &quot;501 T&quot; &quot;502 S&quot; &quot;503 L&quot; &quot;504 R&quot; &quot;505 F&quot; &quot;506 D&quot; &quot;507 D&quot; &quot;508 P&quot;
## [46] &quot;509 W&quot; &quot;510 G&quot; &quot;511 A&quot; &quot;512 S&quot; &quot;ABSEN&quot; &quot;PROVI&quot;</code></pre>
<pre class="r"><code>#create a matrix to store the results of the loop
office.precinct &lt;- matrix(NA, nrow=length(precinct), ncol=length(office))

#loop over the votes for the precincts and offices
for(j in 1: length(office)){
  
  for(i in 1:length(precinct)){
    office.precinct[i,j] &lt;-  sum(mont$votes[mont$precinct==precinct[i] &amp; mont$office==office[j]], 
                                 na.rm=T)
  }
  
}

#you can use the max function to find the maximum number of votes in one precinct/office
#combination
max(office.precinct)</code></pre>
<pre><code>## [1] 4675</code></pre>
<pre class="r"><code>#which will tell you the exact row / column of the office / precinct
which(office.precinct == max(office.precinct), arr.ind = T)</code></pre>
<pre><code>##      row col
## [1,]  23   3</code></pre>
<pre class="r"><code>#I can subset from my vectors to find the names of the combination
office[3]</code></pre>
<pre><code>## [1] &quot;GOVERNOR&quot;</code></pre>
<pre class="r"><code>precinct[23]</code></pre>
<pre><code>## [1] &quot;305 F&quot;</code></pre>
<p>The office-precinct combination that had the highest number of votes is the 305 F precinct and Governor office with 4675 votes.</p>
<p><strong>Question 1c</strong></p>
<pre class="r"><code>#You can first find the mean since this is the statistic we want
mean(mont$votes[mont$party==&quot;Democrat&quot; &amp; mont$office==&quot;GOVERNOR&quot;],na.rm=T)</code></pre>
<pre><code>## [1] 955.3333</code></pre>
<pre class="r"><code>mean(mont$votes[mont$party==&quot;Republican&quot; &amp; mont$office==&quot;GOVERNOR&quot;],na.rm=T)</code></pre>
<pre><code>## [1] 558.6471</code></pre>
<pre class="r"><code>#you can then use a t test to find out if either means are statistically significant
diff.test &lt;- t.test(mont$votes[mont$party==&quot;Democrat&quot; &amp; mont$office==&quot;GOVERNOR&quot;],
                    mont$votes[mont$party==&quot;Republican&quot; &amp; mont$office==&quot;GOVERNOR&quot;])

diff.test</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  mont$votes[mont$party == &quot;Democrat&quot; &amp; mont$office == &quot;GOVERNOR&quot;] and mont$votes[mont$party == &quot;Republican&quot; &amp; mont$office == &quot;GOVERNOR&quot;]
## t = 2.9114, df = 99.392, p-value = 0.004442
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  126.3468 667.0257
## sample estimates:
## mean of x mean of y 
##  955.3333  558.6471</code></pre>
<p>The average number of votes for the Democratic candidate for Governor was 955. The average number of votes for the Republican candidate for Governor was 559. These two numbers are statistically distinguishable from one another. If the average number of votes for the Democratic and Republican candidates for Governor is the same, we would expect to see a difference in the means 0.44% of the time. Since the p-value (0.0044) is so low, we can reject the null hypothesis. There is significant statistical evidence to support that the two means are statistically distinguishable from each other.</p>
<p><strong>Question 2a</strong></p>
<pre class="r"><code># You can use the sample function to simulate fliping a coin
dodgers.win.prob &lt;- c(1,1,1,1,1,1,0,0,0,0)

sample(dodgers.win.prob, size = 7, replace = TRUE) #output: 0 0 1 1 1 0 1</code></pre>
<pre><code>## [1] 1 0 0 1 1 0 1</code></pre>
<pre class="r"><code>dodgers &lt;- sum(sample(dodgers.win.prob, size = 7, replace = TRUE))</code></pre>
<p>In my simulation, the Dodgers won a majority of the games (actually four exactly), therefore winning the world series.</p>
<p><strong>Question 2b</strong></p>
<pre class="r"><code>#first create an empty vector to put the results
dodgers.wins &lt;- rep(NA, 10000)

#Then you can use a for loop to simulate the same 7-game world series 10,000 times
for(i in 1:10000){
  dodgers.wins[i]&lt;- sum(sample(dodgers.win.prob, size = 7, replace = TRUE))
  
}

#you can create a probability table of the results
prob.dodgers.wins &lt;- prop.table(table(dodgers.wins))
print(prob.dodgers.wins)</code></pre>
<pre><code>## dodgers.wins
##      0      1      2      3      4      5      6      7 
## 0.0008 0.0180 0.0790 0.1942 0.2915 0.2573 0.1303 0.0289</code></pre>
<pre class="r"><code>#to find the proportion of times the Dodgers won 4 or more games, you can sum the results
sum(prob.dodgers.wins[5:8])</code></pre>
<pre><code>## [1] 0.708</code></pre>
<p>The Dodgers win four or more games 71.33% of the time.</p>
<p><strong>Question 2c</strong></p>
<pre class="r"><code>#You can use sequence to make a vector with every second game
odd.games &lt;- seq(7, 151, by = 2)
dodgers.win.95 &lt;- rep(NA, length(odd.games))

#you can use a double for loop to find the wins for the length of the series
for(j in 1:length(odd.games)){
  for(i in 1:10000){
    dodgers.wins[i]&lt;- sum(sample(dodgers.win.prob, size = odd.games[j], replace = TRUE))
    
  }
  dodgers.win.95[j]&lt;- prop.table(table(dodgers.wins &gt;= (odd.games[j]/2)))[&quot;TRUE&quot;]
}

print(dodgers.win.95)</code></pre>
<pre><code>##  [1] 0.7137 0.7363 0.7475 0.7744 0.7820 0.7992 0.8122 0.8295 0.8383 0.8500
## [11] 0.8623 0.8698 0.8747 0.8742 0.8837 0.8989 0.9003 0.9007 0.9099 0.9157
## [21] 0.9169 0.9193 0.9289 0.9324 0.9296 0.9357 0.9414 0.9446 0.9478 0.9496
## [31] 0.9492 0.9530 0.9552 0.9607 0.9619 0.9587 0.9619 0.9656 0.9669 0.9706
## [41] 0.9725 0.9742 0.9731 0.9760 0.9788 0.9776 0.9784 0.9815 0.9806 0.9816
## [51] 0.9826 0.9819 0.9854 0.9859 0.9864 0.9852 0.9867 0.9868 0.9869 0.9882
## [61] 0.9890 0.9902 0.9870 0.9907 0.9915 0.9894 0.9909 0.9923 0.9909 0.9915
## [71] 0.9929 0.9933 0.9920</code></pre>
<pre class="r"><code>#you can use the plot function to make a graph of the proportion of times the better team
# wins and the series length
plot(x = odd.games, y = dodgers.win.95, xlab = &quot;Series Length&quot;, ylab = 
&quot;Proportion of Times the Better Team Wins&quot;, main = &quot;Simulated World Series Wins&quot;)
abline(h=0.95)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-41-1.png" width="672" /> The series would have to be around 67 games in order for the better team to win 95% of the 10,000 simulated world series games.</p>
<p><strong>Question 2d</strong> Given the current series length, we cannot reject the null hypothesis that the better team should win 50% of the time. Because the standard is that better team should win 95% of the time, our value of 71.33% is statistically insignificant when they only play seven games.</p>
</div>
<div id="problem-set-5" class="section level3">
<h3>Problem Set 5</h3>
<p><strong>Question 1a</strong></p>
<pre class="r"><code>fem &lt;- read.csv(&quot;/Users/briannafisher/Dropbox/Github/BriannaFisher/data/washington_replication_data.csv&quot;)

#You can use the dim full get the number of members in Congress since each row corresponds to a member
dim(fem)</code></pre>
<pre><code>## [1] 435  16</code></pre>
<pre class="r"><code>#you can use the sum function to add the number of females and divide it by the total number of members to
#get the proportion of females in Congress
sum(fem$female)/435</code></pre>
<pre><code>## [1] 0.1103448</code></pre>
<pre class="r"><code>#doing this gets the same answer
mean(fem$female, na.rm = T)</code></pre>
<pre><code>## [1] 0.1103448</code></pre>
<pre class="r"><code>#you can use the sum function to add the number of republicans (when party = 2) and divide it by the 
#total number of members to get the proportion of females in Congress
sum(fem$party==2)/435</code></pre>
<pre><code>## [1] 0.5241379</code></pre>
<pre class="r"><code>#doing this gets the same answer
mean(fem$party == 2, na.rm = T)</code></pre>
<pre><code>## [1] 0.5241379</code></pre>
<pre class="r"><code>#you can create logical variables for democrats and women in order to find the proportions
fem$women &lt;- fem$female == 1
fem$democrat &lt;- fem$party == 1
mean(fem$women[fem$democrat], na.rm = T)</code></pre>
<pre><code>## [1] 0.1650485</code></pre>
<pre class="r"><code>#you can do the same thing for republicans
fem$rep &lt;- fem$party == 2
mean(fem$women[fem$rep], na.rm = T)</code></pre>
<pre><code>## [1] 0.06140351</code></pre>
<p>There are 435 members of Congress in the dataset. 11.03% of the representations are women. 52.41% of the representatives are republicans. 16.50% of democrats are women. 6.14% of republicans are women.</p>
<p><strong>Question 1b</strong></p>
<pre class="r"><code>#you can use the summary function to find the summary statistics of the AAUW variable
summary(fem$aauw)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.00    0.00   38.00   47.31  100.00  100.00</code></pre>
<pre class="r"><code>#you can use the table function to see how many legislators have each score
table(fem$aauw)</code></pre>
<pre><code>## 
##   0  13  14  25  38  40  50  60  63  75  88 100 
## 135  42   1  25  16   1  13   1  31  17  41 112</code></pre>
<pre class="r"><code>#you can create logical variables for democrats and people who have a score of 100 in order to 
#find the proportions
fem$dem.100 &lt;- fem$aauw == 100
sum(fem$women[fem$dem.100], na.rm = T)</code></pre>
<pre><code>## [1] 30</code></pre>
<pre class="r"><code>#you can create logical variables for democrats and people who have a score of 0 in order to 
#find the proportions
fem$dem.0 &lt;- fem$aauw == 0
sum(fem$women[fem$dem.0], na.rm = T)</code></pre>
<pre><code>## [1] 5</code></pre>
<p>The mean of the AAUW’s legislative score variable is 47.31, the median is 38, the minimum is 0, and the maximum is 100. 135 legislators have a score of 0, while 112 legislators have a score of 100. 30 democrats have a score of 100, while 5 have a score of 0.</p>
<p><strong>Question 1c</strong></p>
<pre class="r"><code>#you can use the linear model function to run a regression, and the summary function to display the output
reg1 &lt;- lm(aauw ~ female, data=fem)
summary(reg1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = aauw ~ female, data = fem)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -75.875 -43.765  -5.765  44.235  56.235 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   43.765      2.076  21.080  &lt; 2e-16 ***
## female        32.110      6.250   5.138 4.22e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 40.84 on 433 degrees of freedom
## Multiple R-squared:  0.05745,    Adjusted R-squared:  0.05528 
## F-statistic: 26.39 on 1 and 433 DF,  p-value: 4.218e-07</code></pre>
<p>The coefficient for the intercept (legislative support for feminist issues) is 43.765 and the coefficient for the legislator’s gender (whether or not they are female) is 32.11. Women are more likely to support feminist issues than men by 32.11 points, on average. When the legislator is male (when the female variable equals zero), the average legislative support for feminist issues is 43.765 points.</p>
<p><strong>Question 1d</strong></p>
<pre class="r"><code>#you can create a republican variable by subsetting for the people who responded 2 to the party question
fem$republican &lt;- NA
fem$republican[fem$party == 1] &lt;- 0
fem$republican[fem$party == 2] &lt;- 1
fem$republican[fem$party == 3] &lt;- 0

#you can use the linear model function to run a regression, and the summary function to display the output
reg2 &lt;- lm(aauw ~ female + republican, data=fem)
summary(reg2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = aauw ~ female + republican, data = fem)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -84.07 -11.18   1.82  15.93  76.82 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   84.072      1.431  58.766  &lt; 2e-16 ***
## female        13.063      2.998   4.357 1.65e-05 ***
## republican   -72.891      1.881 -38.756  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 19.33 on 432 degrees of freedom
## Multiple R-squared:  0.7895, Adjusted R-squared:  0.7885 
## F-statistic:   810 on 2 and 432 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The coefficient for the intercept (legislative support for feminist issues) is 84.072, the coefficient for the legislator’s gender (whether or not they are female) is 13.063, and the coefficient for the legislator being republican is -72.891. When the legislator is male and a democrat (or independent), the average legislative support for feminist issues is 84.072. Women are more likely to support feminist issues than men by 13.063 points, on average, when holding republicans fixed. Republicans are less likely to support feminist issues than Democrats or Independents by 72.891 points, on average, when holding women fixed.</p>
<p><strong>Question 1e</strong></p>
<pre class="r"><code>#you can use the linear model function to run a regression, and the summary function to display the output
reg3 &lt;- lm(aauw ~ female + demvote, data=fem)
summary(reg3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = aauw ~ female + demvote, data = fem)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -80.561 -25.249  -3.584  23.222  72.585 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -63.565      5.994 -10.605  &lt; 2e-16 ***
## female        16.870      4.742   3.558 0.000415 ***
## demvote      216.619     11.685  18.538  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 30.52 on 432 degrees of freedom
## Multiple R-squared:  0.475,  Adjusted R-squared:  0.4726 
## F-statistic: 195.5 on 2 and 432 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>#this tells you the change when you increase democratic vote share by 10 percentage points
216.619/10</code></pre>
<pre><code>## [1] 21.6619</code></pre>
<p>Holding gender fixed, the AAUW score increases by 21.662 points when you increase the Democratic vote share by 10 percentage points.</p>
<p><strong>Question 1f</strong></p>
<pre class="r"><code>#you can create separte variables for the different religions
#none coded
fem$rel.none &lt;- NA
fem$rel.none[fem$rgroup == 0] &lt;- 1
fem$rel.none[fem$rgroup == 1] &lt;- 0
fem$rel.none[fem$rgroup == 2] &lt;- 0
fem$rel.none[fem$rgroup == 3] &lt;- 0
fem$rel.none[fem$rgroup == 4] &lt;- 0

#protestant
fem$rel.prot &lt;- NA
fem$rel.prot[fem$rgroup == 0] &lt;- 0
fem$rel.prot[fem$rgroup == 1] &lt;- 1
fem$rel.prot[fem$rgroup == 2] &lt;- 0
fem$rel.prot[fem$rgroup == 3] &lt;- 0
fem$rel.prot[fem$rgroup == 4] &lt;- 0

#catholic/orthodox
fem$rel.cath &lt;- NA
fem$rel.cath[fem$rgroup == 0] &lt;- 0
fem$rel.cath[fem$rgroup == 1] &lt;- 0
fem$rel.cath[fem$rgroup == 2] &lt;- 1
fem$rel.cath[fem$rgroup == 3] &lt;- 0
fem$rel.cath[fem$rgroup == 4] &lt;- 0

#christian
fem$rel.chris &lt;- NA
fem$rel.chris[fem$rgroup == 0] &lt;- 0
fem$rel.chris[fem$rgroup == 1] &lt;- 0
fem$rel.chris[fem$rgroup == 2] &lt;- 0
fem$rel.chris[fem$rgroup == 3] &lt;- 1
fem$rel.chris[fem$rgroup == 4] &lt;- 0

#jewish
fem$rel.jew &lt;- NA
fem$rel.jew[fem$rgroup == 0] &lt;- 0
fem$rel.jew[fem$rgroup == 1] &lt;- 0
fem$rel.jew[fem$rgroup == 2] &lt;- 0
fem$rel.jew[fem$rgroup == 3] &lt;- 0
fem$rel.jew[fem$rgroup == 4] &lt;- 1

#you can use the linear model function to run a regression, and the summary function to display the output
reg4 &lt;- lm(aauw ~ female + demvote + rel.prot + rel.cath + rel.chris + rel.jew, data=fem)
summary(reg4)</code></pre>
<pre><code>## 
## Call:
## lm(formula = aauw ~ female + demvote + rel.prot + rel.cath + 
##     rel.chris + rel.jew, data = fem)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -72.186 -23.628  -3.245  21.369  74.360 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -23.749     14.077  -1.687 0.092311 .  
## female        17.924      4.640   3.863 0.000129 ***
## demvote      201.269     11.986  16.792  &lt; 2e-16 ***
## rel.prot     -35.144     12.370  -2.841 0.004712 ** 
## rel.cath     -31.692     12.462  -2.543 0.011340 *  
## rel.chris    -39.369     14.475  -2.720 0.006800 ** 
## rel.jew       -7.602     13.542  -0.561 0.574838    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 29.78 on 428 degrees of freedom
## Multiple R-squared:  0.5049, Adjusted R-squared:  0.4979 
## F-statistic: 72.73 on 6 and 428 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>#this tells you the change when you increase democratic vote share by 10 percentage points
201.269/10</code></pre>
<pre><code>## [1] 20.1269</code></pre>
<p>When the legislator is male, has no religion, and has a 0 for the democratic vote share, the average legislative support for feminist issues is 17.924. When holding religion and democratic vote share fixed, women are more likely to support feminist issues than men by 17.924 points. When holding religion and sex fixed, the democratic vote share increases by 20.1269 points. When holding sex, democratic vote share, Catholics, Christians, and Jews fixed, legislators who are Protestant are less likely than those who have no religion to support feminist issues by 35.144 points. When holding sex, democratic vote share, Protestants, Christians, and Jews fixed, legislators who are Catholic are less likely than those who have no religion to support feminist issues by 31.692 points. When holding sex, democratic vote share, Catholics, Protestants, and Jews fixed, legislators who are Christian are less likely than those who have no religion to support feminist issues by 39.369 points. When holding sex, democratic vote share, Catholics, Christians, and Protestants fixed, legislators who are Jewish are less likely than those who have no religion to support feminist issues by 7.602 points.</p>
</div>
<div id="problem-set-6" class="section level3">
<h3>Problem Set 6</h3>
<p>Because of the importance of the economy during an election and the different platforms on taxes and spending by both the Democratic and Republican parties, as well as the large difference between the average feeling thermometer score in regard to approval of the economy, I chose to model how a respondent feels about how the President (Barack Obama) is handling the economy to help explain why people like Donald Trump. <img src="Portfolio_files/figure-html/unnamed-chunk-49-1.png" width="672" /> After running a bivariate regression between these two variables, I found that people who approved of how Obama was handling the economy were less likely to like Trump by 33 points than someone who disapproved of how the economy was handled. I also found that the average support for Trump when someone does not approve of how the economy was handled by Obama was 58 points.</p>
<p>When controlling for party, sex, and age, people who approved of how Obama was handling the economy were still less likely to like Trump, but now by 21 points. Also similar to the bivariate regression, when someone was over 65 years old, male, republican, and disapproved of how Obama was handling the economy, the average support for Trump was 73 points. When holding sex and age fixed, democrats were less likely to like Trump than republicans by 23 points, while independents were less likely to like Trump than republicans by 15 points. When holding party and sex fixed, people who were aged 30 to 49 were less likely to like Trump than people aged 65 and older by 14 points. I did not see any significant difference in support for Trump between males and females, between people aged 18 to 29 and people aged 65 and older, and between people aged 50 to 64 and people aged 65 and older.</p>
<p>Both of the results for how the economy was being handled by Obama from the regressions are statistically significant, with p-values less than 0.01, meaning that we can reject the null hypothesis that approval of how Obama was handling the economy would not impact whether or not people like Trump.</p>
</div>
<div id="final---public-defender-representation-leads-to-longer-prison-sentences" class="section level3">
<h3>Final - Public Defender Representation Leads to Longer Prison Sentences</h3>
<p>Public defenders make up the backbone of our society. They guarantee the right to representation, a fair trial, and above all else, the ability to have someone with real, legal knowledge stand up for you in court.</p>
<p>The Sixth amendment guarantees everyone the right to counsel in all criminal prosecutions; however, public defenders are underpaid and overworked, as well as have little time to prepare for each case. In effect, the Sixth amendment was created to make sure that everybody has an adequate defense regardless of the severity of the crime, their ability to pay, or any other circumstance that could interfere with a fair trial. When public defenders do not have the time or resources to properly craft a defense, it uncovers a systemic crack within the Justice System: is the Sixth amendment actually more detrimental to a defendant than helpful, rendering this guarantee essentially useless?</p>
<p>This idea can be studied by looking at whether or not court appointed counsel has a larger impact on sentence length than other representation types. After analyzing data from official court records for defendants convicted and sentenced to serve time in jail and those convicted and sentenced to pay a fine for Driving While Intoxicated in Minnesota in 1982, the answer is yes, a defendant represented by a public defender is more likely to have a longer prison sentence than someone represented by a private attorney or someone that represented themselves.</p>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<p>On average, people represented by public defenders spend almost 10 days in prison. However, those represented by private attorneys or pro se (those that represented themselves), spend as little as 3.15 days in prison.</p>
<p>To model this further, I examined the relationship between representing yourself in court and being represented by a public defender or private attorney, as well as controlling for age and sex (since females are less likely than males to be convicted of Driving While Intoxicated and people over 65 are less likely to be driving).</p>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<p>When a defendant was male, older than 65 years old, and represented by a public defender, their average sentence length for Driving While Intoxicated was 9.67 days. However, when holding sex and age fixed, having a private attorney as representation is associated with a 7 day decrease in sentence length. Seven fewer days in prison for the same crime because someone was fortunate enough to be represented by a private attorney rather than a public defender clearly indicates that there are foundational problems that need to be investigated within public defender offices as a whole.</p>
<p>After looking at the data, I also thought that it was unusual that so many people were not only representing themselves, but also receiving lessor sentences than people represented by public defenders. Originally, I thought that people who represent themselves might have less serious cases, and therefore get shorter sentences regardless of representation. However, people who represent themselves were actually more likely to have a blood alcohol content over 0.10 (with 0.08 being the legal limit) than those represented by public defenders and private attorneys, as well as be convicted of a second charge of Driving While Intoxicated. This could possibly be caused by whether or not Minnesota public defender offices have less resources than other areas and public defenders are more accustomed to receiving longer sentences for their defendants, but I would need more research and data in order to determine the actual cause.</p>
<p>This leads to the question, why is the Sixth amendment failing so many defendants?</p>
<p>Public defender offices act more like factories, where cases come in the front door and go out the back within the same breath. Each case gets a formulaic defense that is applied within the couple of minutes that public defenders have to review it before acting as representation in court. There is a focus on brevity and completeness, rather than a fair and personalized defense for each individual.</p>
<p>This has, in turn, led to the increased implementation of plea deals, which help to alleviate the number of trials carried out by prosecutors but are used in place of putting in the time and resources to thoroughly review a case. As a result, prisoners are carrying out sentences that are almost always too long for the crime committed but act as a result of an insufficient plea deal.</p>
<p>The crutch of using a plea deal could be a factor in the additional seven days spent in prison by someone represented by a public defender. This highlights the disparity between being able to afford better representation and invoking one’s right to counsel, which should hold up to the “adequate” standard set by the Constitution.</p>
<p>This is clearly a flaw within the American Justice System, but what can be done to fix this?</p>
<p>First, and perhaps most importantly, public defender offices need to have an increase in funding so that they can complete their jobs more effectively. Second, public defender offices should create subdivisions within each office based upon certain infractions and categories within the Justice System. When an office assigns their attorneys to a case based upon their preferential and desired division, the lawyer is more inclined to become passionate and have greater knowledge in the case.</p>
<p>Additional research needs to be completed within other areas of the law to determine if this pattern carries over to crimes other than Driving While Intoxicated. If so, the entire premise of public defender offices needs to be evaluated in order to ensure that the Sixth amendment is effective and can be used as a support to defendants, rather than a detriment to them.</p>
<p>Anyone put on trial deserves, and is required by law, to have an attentive and interested lawyer to defend them. When lawyers take on too many cases, they ultimately contribute to the decline in effectiveness of the Sixth amendment and legal system as a whole. If a legal system cannot act effectively, there is no way to guarantee that justice will guide the decisions of prosecutors and defenders alike.</p>
</div>
</div>
<div id="statistical-methods-for-political-science" class="section level2">
<h2>Statistical Methods for Political Science</h2>
<h2>
Statistical Methods for Political Science
</h2>
<div id="midterm" class="section level3">
<h3>Midterm</h3>
<p><strong>Theory</strong></p>
<p>As COVID-19 started to worsen throughout the country, there was a stark difference in the way that Democratic versus Republican government officials handled the outbreak. States with Republican governors quickly ended stay at home orders and reopened nonessential businesses, while states with Democratic governors saw extensive statewide lockdowns and stricter rules about going out in public. These two ideals were met with both support and criticism from higher ranking government officials, such as praise by the President for reopening states and criticism from Democratic congressmen who supported a lockdown. These differing views on how to handle the virus were also seen at the local level, with everyday citizens sharing the same ideas about social distancing practices depending on their ideology. Similarly, the support from higher ranking government officials for one practice over the other influenced how people followed protocol.</p>
<p>I theorize that people who identify as liberal are more likely to socially distance than those who identify as conservative. States with a majority of people who identify as liberal have Democratic governors and state governments, so these areas already had lengthy stay at home orders and strict rules in place to limit the virus. People in these states mostly agreed with their government’s policies and were already used to social distancing and quarantining when the states reopened. States with a majority of people who identify as conservative, however, have Republican governments that emphasized keeping businesses and schools open. Florida and Texas, for example, saw crowded beaches and malls all summer while New York, and especially New York City, had quiet streets and closed restaurants. The reasons why people with different ideologies followed different social distancing practices also has to do with the core beliefs of those ideologies. Conservatives heavily value the economy, so it was expected for conservative governments to not order mandatory lockdowns in order for nonessential businesses to stay open. Liberals, however, generally believe in government action to solve problems and protect its citizens, so following lockdown orders and strictly social distancing was expected. I hypothesize that ideological views caused liberals to socially distance more than conservatives.</p>
<p><strong>Empirical Tests</strong></p>
<p>Based on my theory, I hypothesize that people who identify as liberal were less likely to partake in everyday activities (such as shopping for food, medicine, or essential household items, visiting close friends or family, going to work, using public transportation, and dining at a restaurant) than people who identify as conservative. A maintained assumption for this theory is that all of these activities have the same level of importance to each person. Shopping for food, medicine, or essential household items is necessary for people to survive, while dining at a restaurant depends on the person’s comfort level. This would cause me to expect that the percentage of people who were liberal and went shopping for those items is similar to the percentage of people who were conservative and went shopping for those items, while the other activities would vary depending on how greatly that person needed to complete them.</p>
<p>Additionally, I hypothesize that people who approve of Trump were more likely to partake in these activities than people who don’t approve of Trump. A maintained assumption of this is that people who approve of Trump therefore value his opinion and will follow his example (such as not socially distancing). However, some people may not take his opinion into consideration when deciding whether or not to participate in these activities. Also, people who approve of Trump mostly identify as conservative, while those who do not identify as liberal. Thus, I would expect the same as before that people who are liberal were less likely to partake in these activities then people who are conservative.</p>
<p><strong>Variable Coding</strong></p>
<p>In order to test my theory and hypothesis, I use data from a study completed by the Kaiser Family Foundation. The study was completed from June 8, 2020 to June 14, 2020, and focuses on the perception of health, race, and COVID-19 in the United States.</p>
<p>My dependent variable is constructed using a combination of four everyday activities that people abstained from. In question 18 of the survey, the respondents were asked how often they (18A) shop for food, medicine, or essential items, (18B) visit close friends or family, (18C) go to work, (18D) use public transit, and (18E) dine at a restaurant. The respondents had four response options: 1) “4 times or more”, 2)” 2-3 times”, 3) “1 time”, or 4) “Not at all.” Because shopping for food, medicine, or essential items was necessary for everybody, no matter their ideology, to continue during the pandemic, my dependent variable is a combination between questions 18B, 18C, 18D, and 18E. Also, since I am looking at whether or not people abstained from these activities, I only focus on whether or not the respondent said 4, or not at all, to any of the four options. My dependent variable is coded as a dummy variable that equals 1 if the respondent abstained from any of the four activities (visiting close family or friends, going to work, using public transit, or dining in a restaurant) and 0 if they participated in any of these activities at least once. A measurement assumption for this variable is that I am measuring social distancing as a whole by only four activities, when in reality people could be going out in public for a variety of reasons that do not match up with the variable.</p>
<p>My key independent variables are two dummy variables coded for whether or not someone is conservative or moderate. The “Conservative” dummy variable is coded as 1 if the respondent answered the ideology question as conservative and 0 if they responded as liberal, moderate, don’t know, or refused. The “Moderate” dummy variable is coded as 1 if the respondent the ideology question as moderate and 0 if they responded as liberal, conservative, don’t know, or refused. Both variables are coded as missing if someone did not answer the question. A measurement assumption for this variable is that people only conform to these specific ideologies, when in reality ideology is measured on more of a spectrum.</p>
<p><strong>Control Variables</strong></p>
<p>One control variable when analyzing the relationship between ideology and social distancing is whether or not someone approves of Trump. Because Trump is in such a large position of power and has control over the narrative of the virus, people look up to him to determine how they should act. When Trump does not socially distance or wear a mask, normal people see this as a sign that they should not either. I coded this variable (trump.approve) as a dummy variable that equals one if someone strongly or somewhat approves of Trump and 0 if they somewhat disapprove, strongly disapprove, don’t know, ore refused. I would expect someone who is liberal to somewhat or strongly disapprove of Trump, and therefore made it my excluded group.</p>
<p>Another control variable when analyzing the relationship between ideology and social distancing is a person’s age. I expect people over the age of 65 to social distance more, as the elderly are at a higher risk for catching the virus than those that are younger. As a result, I made people 65 years old to 97 years old my excluded group. I created dummy variables for each age group (18-29, 30-49, 50-64), with that specific group being coded as 1 and the other responses being coded as 0.</p>
<p>A third control variable is a person’s race. On average, people who are White are more likely to be liberal than people of another race. The biggest block of the Democratic vote, for example, has shifted to white liberals from African Americans. Because of this, I excluded people who identify as White from my regression. I created a dummy variable to represent minority groups, with 1 being for people who responded that they were Black or African American, Asian, other or mixed race, don’t know, or refused and 0 for people who responded that they were White.</p>
<p><strong>Descriptive Statistics</strong></p>
<pre><code>## 
## Table 1: Descriptive Statistics on Ideology and Social Distancing
## ==========================================================
## Statistic                   N   Mean  St. Dev.  Min   Max 
## ----------------------------------------------------------
## Abstained from Activities 1,296 0.235  0.424     0     1  
## Conservatie Ideology      1,296 0.350  0.477     0     1  
## Moderate Ideology         1,296 0.374  0.484     0     1  
## Trump Approval            1,269 0.406  0.491   0.000 1.000
## Age 18-29                 1,273 0.135  0.342   0.000 1.000
## Age 35-49                 1,273 0.270  0.444   0.000 1.000
## Age 50-64                 1,273 0.288  0.453   0.000 1.000
## Minority                  1,268 0.259  0.439   0.000 1.000
## ----------------------------------------------------------</code></pre>
<p>Table 1 presents descriptive statistics for my dependent variable, independent variables, and control variables. The table depicts the number of observations, mean, standard deviation, minimum, and maximum values for the following variables: Abstained From Activities, Conservative Ideology, Moderate Ideology, Trump Approval, Age 18-29, Age 35-49, Age 64, and Minorities.</p>
<p><strong>Results</strong> <img src="Portfolio_files/figure-html/unnamed-chunk-63-1.png" width="672" /> Figure 1 shows, as I predicted, that conservatives were less likely to socially distance than liberals or moderates. The “0” block represents people who responded to the survey as liberal, moderate, don’t know, or other, and is about 0.8 units higher than the conservative, or “1”, block.</p>
<pre><code>## 
## Table 2: Correlation of Abstained From Activities During COVID-19
## =============================================
##                      Dependent variable:     
##                 -----------------------------
##                     Abstained.activities     
##                 Ideology     All    Liberals 
##                    (1)       (2)       (3)   
## ---------------------------------------------
## Conservative    -0.081*** -0.073**           
##                  (0.030)   (0.034)           
##                                              
## Moderate         -0.006    -0.019            
##                  (0.030)   (0.031)           
##                                              
## Trump Approval             -0.049*    0.091  
##                            (0.028)   (0.069) 
##                                              
## Age 18-29                 -0.258*** -0.209***
##                            (0.039)   (0.069) 
##                                              
## Age 30-49                 -0.234*** -0.252***
##                            (0.031)   (0.063) 
##                                              
## Age 50-64                 -0.188***  -0.130* 
##                            (0.031)   (0.070) 
##                                              
## Minority Groups             0.022     0.031  
##                            (0.028)   (0.056) 
##                                              
## Constant        0.266***  0.437***  0.398*** 
##                  (0.022)   (0.030)   (0.046) 
##                                              
## ---------------------------------------------
## Observations      1,296     1,219      330   
## R2                0.008     0.071     0.055  
## =============================================
## Note:             *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>Table 2 presents a regression analysis for three subsets, all regressed on the dependent variable of abstained activities. The first column represents just the independent variable of ideology, for both conservatives and moderates. The average number of activities liberals abstained from was 0.26 units, while conservatives were 0.08 units less likely to abstain from any activities. This provides evidence for my hypothesis, as liberals were more likely to socially distance than those who identify as conservative. Moderates were 0.006 units less likely to abstain from any activities than liberals, which was also expected as moderates fall between liberals and conservatives on the ideology spectrum and subsequently should have fallen in the middle of the regression.</p>
<p>The second column in the regression includes all of the control variables that I used to test my theory. The average number of activities liberals abstained from activities was 0.437 units, while conservatives were 0.073 unites less likely to abstain from activities, holding fixed Trump approval, minority status, and age. This coefficient most likely increased because it now included my excluded groups of liberals, people who are white, people who are 65 years old and older, and people who do not approve of Trump. Theoretically, all of these categories would lead to increased social distancing, which is seen when the coefficient changes. The coefficient on Trump Approval indicates that people who support Trump are an additional 0.049 units less likely to abstain from any activities than people who do not support Trump. This was expected, since people who approve of Trump are likely to follow his behavior of not social distancing. Additionally, the coefficients on all three age groups (18-29, 20-49, 50-64) are negative, indicating that they are less likely to social distance than people who are aged 65 years old and older. This result was expected, as people who are younger are both at less of a risk of the virus and do not take the virus as seriously as those who are older. Finally, the coefficient on the minority groups variable indicates that people who identify as a minority are 0.022 units more likely to social distance than those that identify as White. Originally, I had hypothesized that people who identify as White would socially distance more since they make up the largest demographic of liberals. However, this result indicates that people who make up a minority group are more likely to socially distance than those who are White.</p>
<p>Similarly, to the second column, column three shows the same general patterns for the coefficients of the age and minority group variables. However, among liberals, people who approve of Trump are 0.091 units more likely to socially distance than liberals who do not approve of Trump. This result was not expected, as I would think that people who approve of Trump (who promotes not social distancing) would be less likely than people who do not approve of Trump (liberals especially) to socially distance. When looking at the data though, only 14.45% of liberals approve of Trump, while 85.55% disapprove (out of 357 observations). This makes it clear that the result is just unusual and there was little data to work with.</p>
<p><strong>Conclusion</strong> I was able to find evidence that people who identify as liberal are more likely to socially distance than those who identify as conservative. Because social distancing is such a broad topic, I would have wanted to have more information on the extent to which people social distanced (for example visiting family or friends while staying six feet apart and wearing a mask versus going inside of someone else’s house). This most likely would have changed the respondent’s attitude towards social distancing, as going inside of another person’s house does not adhere to the “definition” of social distancing set forth by the CDC. As a control variable, I would have wanted to look at the type of job a respondent has. If a respondent has a job where they are deemed an essential worker, then they would have had to respond yes to the “Did you go to work” (18C) question even if it was involuntary. I would have also wanted to look at a respondent’s religion. If a respondent goes to church twice a week, for example, then they might have a more relaxed attitude on social distancing if they were still going throughout the pandemic.</p>
<p>If I was able to look at these variables as well, I would expect the regression coefficient to increase. The majority of people in the country were not deemed essential workers, so I would expect more people to socially distance than the proportion of those who had to go to work. I would also expect the coefficient to increase in regard to religion because when the survey was taken in June, most places of worship were closed due to the pandemic. This would have caused more people to social distance if they were not able to practice their religion in person.</p>
<p>In regard to measurement error, it worries me that ideology is measured by only three choices. I would have wanted to have a more continuous variable to measure ideology, rather than just three options. In today’s political climate, there are numerous divisions within each political party and ideology separating one person’s view of being liberal or conservative from another. Right now, my study only analyzes ideology as if it is cut and dry, when in reality there are many different options to the question. This might have made respondents more open to answering the question if they had more options, especially if they responded as they don’t know.</p>
</div>
<div id="final" class="section level3">
<h3>Final</h3>
<p><strong>Introduction</strong></p>
<p>The process of scientific jury selection was first applied to a major case in the Harrisburg Seven trial in 1972. During the trial, social scientists used demographical information of the jurors to identify whether or not they would be biased towards a conviction and subsequently to serve on the jury. Ever since this case, prosecutors and defense attorneys alike have become accustomed to considering demographics – such as race, age, sex, economic background, marital status, religion, and relationship with the law – when picking a jury. During the voir dire process, lawyers have the opportunity to use peremptory and causal challenges in order to dismiss jurors that would not maintain fairness when listening to the case. A challenge for cause occurs when a lawyer removes someone for a legal reason (these are unlimited during a trial). A peremptory challenge occurs when a lawyer removes someone without a reason or explanation, but cannot be on the basis of race, ethnicity, or sex (the number is limited by the statute being tried). Increasingly, lawyers have been using these challenges in a subtle (in order to remain legal) way to achieve a jury that is more likely to either side in favor or against the defendant, depending on whether it is the prosecution or defense. According to the Equal Justice Initiative’s 2010 report on illegal racial discrimination in jury selection, 8 out of 10 African Americans in Houston County, Alabama that qualified for jury service have been struck by prosecutors from death penalty cases. It was also reported that in Jefferson Parish, Louisiana, there is no effective African American representation on the jury in 80 percent of criminal trials. This idea of deciding a case before presenting evidence just based upon racial biases and stereotypes seems like it should be a significant problem in the evaluation of trials and within the criminal justice system as a whole. This led me to the question “Are jurors more sympathetic to defendants of the same race?” and to my hypothesis that “juries in which at least 50% of the jurors are the same race as the defendant are more likely to side in favor of the defendant.”</p>
<p>In order to test my hypothesis, I analyzed data on four courthouses (Bronx County Supreme Court in New York, Los Angeles County Superior Court in California, Maricopa County Superior Court in Arizona, and District of Columbia Superior Court in Washington, DC). I ran a baseline regression between whether or not the defendant had at least one conviction and the proportion of the jury whose members are less than 50% racially similar to the defendant, as well as a controlled regression including victim believability, defendant sympathy, if the crime was a misdemeanor, and if the victim was not white. For both regressions, I found that the defendant was more likely to be convicted when the jury was less than 50% racially similar to the defendant than when the jury was at least 50% racially similar to the defendant, providing evidence for my hypothesis.</p>
<p><strong>Theory and Previous Literature</strong></p>
<p>While completing preliminary research, two sources emerged as predominantly important for this paper. In Professor of Law at Washington University School of Law Peter A. Joy’s 2015 paper for the Northwestern University Law Review entitled “Race Matters in Jury Selection,” he analyzed a study conducted by Samuel Sommers and Phoebe Ellsworth on implicit bias during the voir dire process. During the study, one group of mock jurors received a questionnaire about their racial attitudes and racial biases in the legal system while the other group was asked questions that disregarded race as a whole. According to Joy, the purpose of the questions was “not to identify racial bias in particular jurors, but rather to cause prospective jurors to think about their attitudes toward race to help jurors consciously guard against implicit bias.” The study found that both White and African American jurors were less likely to convict African American defendants after being asked the race-related questions during the voir dire, as well as that all-White juries were more likely to convict African American defendants than White defendants (Joy 2015). Ideally, all jurors would be asked these race-based questions in the real world during their voir dire in order to spark introspection and make themselves aware of any biases they may hold. However, race-relevant questions are only allowed during capital punishment cases, and the inclusion of a race-relevant voir dire is up to the discretion of the trial judge in all other cases. Originally, I planned on studying multiple demographics and how they impacted a juror’s decision on the defendant’s sentence as a whole. However, after this explanatory research, this paper became the foundation of my theory: implicit racial bias impacts jury decision making.</p>
<p>Similarly, in Justin D. Levinson (Associate Professor of Law at the University of Hawaii), Huajian Cai (Professor at the Key Laboratory of Mental Health and Institute of Psychology for the Chinese Academy of Sciences), and Danielle Young’s (member of the Department of Psychology at the University of Hawaii) research paper for the Ohio State Journal of Criminal Law entitled “Guilty By Implicit Racial Bias: The Guilty/Not Guilty Implicit Association Test,” they completed a study on how the Implicit Association Test (IAT) can be used in a legal setting. The study had participants (67 jury eligible students from the University of Hawaii) complete multiple tasks in order to measure racial beliefs and preferences. The first test was a Guilty/Not Guilty IAT test developed as a race IAT with “the attribute concepts of Guilty and Not Guilty and target concepts of Black and White” (Levinson 2010). The second test was a Pleasant/Unpleasant IAT test also developed as a race IAT to evaluate the concepts of Pleasant and Unpleasant with the target concepts of Black and White. The participants were also given the Modern Racism Scale, which has a series of questions in regard to racial beliefs, a feeling thermometer, which evaluates explicit racial preferences, and finally a robbery evidence evaluation task, which asked participants to decide whether a defendant was guilty or not guilty in a mock trial. The study found that participants in the Guilty/Not Guilty IAT displayed a “significant association between Black and Guilty compared to White and Guilty” and that participants in the Pleasant/Unpleasant IAT displayed a “significant association between Black and Unpleasant compared to White and Unpleasant” (Levinson 2010). Ultimately, Levinson, Cai, and Young were able to conclude that people are more likely to perceive black people as guilty, and that the implicit associations led to “predicted judgments of the probative value of evidence” (Levinson 2010). This study was crucial to the refinement of my theory and hypothesis, as it is such a significant problem that some trials are already decided before hearing any evidence, completely violating the “fair trial” requirement set forth by the Constitution.</p>
<p>After reviewing this previous literature, I have decided to focus on the proportion of jurors whose race matches the race of the defendant and whether or not that impacts their trial outcome. I have also decided to look at jury sympathy for the victim and defendant, as the second study proves the prominence of implicit biases when making decisions in regard to trial outcome. My research will add a unique perspective to this literature as I will be analyzing data from four different courthouses, all of which are located in demographically different areas (Bronx County, New York, Los Angeles County, California, Maricopa County, Arizona, and Washington, DC). I will also be analyzing the outcomes of the trials, rather than just the process of jury selection. This will allow me to connect the issue of racial biases in jury decision making to these different cities in order to highlight this significant fault within the justice system and the fact that it is present within all juries and not one particular area.</p>
<p><strong>Data</strong></p>
<p>I will use the data from the “Evaluation of Hung Juries in Bronx County, New York, Los Angeles County, California, Maricopa County, Arizona, and Washington, DC, 2000-2001 (ICPSR 3689)” by Paula Hannaford-Agor, Valerie Hans, Nicole Mott, and G. Thomas Munsterman. The data includes information on four courts (Bronx County Supreme Court in New York, Los Angeles County Superior Court in California, Maricopa County Superior Court in Arizona, and District of Columbia Superior Court in Washington, DC). Each court was sent a case data form and count sheet data form, as well as three questionnaires for the judges, attorneys, and jurors. The case data form includes the demographic information for the defendant and the victim(s) and the type of representation for the defendant. The count sheet data includes the case type, the jury’s decision for each count the defendant was charged with, the total number of convictions, and the sentence length. The judge questionnaire includes the evaluation of the evidence, case complexity, likelihood that the jury understood the case, importance of the victim’s testimony, and importance of the defendant’s testimony. The attorney questionnaire includes information assessing the case complexity, type of defense, whether or not the jury would become hung, and their own demographical information. The juror questionnaire includes responses regarding case complexity, whether or not the evidence was convincing, whether or not they had sympathy towards the defendant or victim(s), whether or not the defense had a strong case, which side they favored before, during, and after deliberations, and demographical information on themselves.</p>
<p>In order to analyze this data as a whole, I reshaped the attorney and jury datasets so that each row corresponded to a singular court site and case number. Once the data was widened, I was able to merge all five datasets together to create one larger dataset, entitled “law.” To construct my key independent variable (the proportion of the jury whose members are less than 50% racially similar to the defendant), I first created three dummy variables for the race of each jury member (White, not White, and unknown). Once all 36 variables were created (since trials could have up to 12 jurors), I added them together so that I would have a total count of the jury members of each race per case number (becoming the variables jury.white, jury.notwhite, and jury.unknown). Next, I created the same three dummy variables for the defendant’s race (def.white, def.notwhite, and def.unknown). Once these six variables were created, I constructed the continuous proportion variable (becoming the variable prop.race) by matching the race of the defendant with the number of people who matched that race on the jury and dividing that number by the total number of jurors for that case. I constructed my actual independent variable (becoming prop.race.less50) as a dummy variable that equals 1 if the proportion of the jury whose race matches the defendant was less than half of the members and 0 if the proportion of the jury is equal to or more than half of the members. Because my hypothesis is that juries in which at least 50% of the jurors are the same race as the defendant are more likely to side in favor of the defendant, I made the group in which the proportion of the jury whose race is equal to or more than half the same as the defendant the excluded group.</p>
<p>Afterwards, I constructed my dependent variable by combining the results for whether or not a defendant had a least one conviction for the six possible counts they could have been charged with in that particular case (becoming the variable yes.conv). The variable is coded as 1 if they had at least one conviction and 0 if they had no convictions. A measurement assumption of this variable, however, is that I am assuming that these juries had negative racial biases, when it could have been the opposite and positive, resulting in a lesser sentence or charge rather than a conviction for the defendant. Additionally, I chose to control for four different variables that all could influence both the proportion of jury members with a similar race to the defendant and whether or not the defendant was convicted on at least one count. First, I chose to control for whether or not the jury felt “a great deal of sympathy” for the defendant. It seems like having a large amount of sympathy for the defendant would cause the jury to rule in favor of the defendant, regardless of their race. I coded this variable (def.sympt) as a dummy variable that equals 1 if at least one of the jury members felt a great deal of sympathy for the defendant and 0 if none of the jury members felt a great deal of sympathy for the defendant. I would expect that someone who has no sympathy towards the defendant would rule more harshly, so I made that my excluded group. Similarly, I chose to control for whether or not the jury felt that the victim(s) was very believable. It seems like thinking that the victim(s) was very believable would cause the jury to rule against the defendant, regardless of race. I coded this variable (believe.vict) as a dummy variable that equals 1 if at least one of the jury members felt that the victim(s) was very believable and 0 if none of the jury members felt that the victims(s) was very believable. I would expect that someone who does not think that the victim was believable would rule less harshly against the defendant, so I made that my excluded group. My third control variable is the crime the defendant was charged with. I created two variables: the first (felony) equals 1 if the crime is classified as a felony and 0 if the crime is classified as a misdemeanor and the second (misdemeanor) equals 1 if the crime is classified as a misdemeanor and 0 if the crime is classified as a felony. I would expect that if someone was charged with a crime as serious as murder or rape, a juror would weigh that more heavily than whether or not they were the same race. Because of this, I made the felony variable my excluded group. Finally, my fourth control variable is the victim’s race. Because of the preconceived biases made by the jury in the research I previously discussed in my Literature Review, I decided to look at whether or not the victim was White, as most people in the study associated White with “Not Guilty” and “Pleasant.” I created two variables: the first (vict.white) equals 1 if the victim was White and 0 if victim was any other race and the second (vict.notwhite) equals 1 if the victim was any race other than White and 0 if the victim was White. Because I would expect the jury to favor White victims, I made that my excluded group.</p>
<pre><code>## 
## Table 1: Descriptive Statistics for Convictions and Proportion of Similar Race
## ==========================================================================================================================
## Statistic                                                                                  N    Mean  St. Dev.  Min   Max 
## --------------------------------------------------------------------------------------------------------------------------
## At least one conviction                                                                  17,456 0.543  0.498     0     1  
## Proportion of the jury whose members are less than 50% racially similar to the defendant 17,357 0.467  0.499   0.000 1.000
## Victim Believability                                                                     17,456 0.516  0.500     0     1  
## Defendant Sympathy                                                                       17,456 0.306  0.461     0     1  
## Not White Victim                                                                         17,456 0.982  0.133     0     1  
## Misdemeanor                                                                              17,456 0.135  0.342     0     1  
## --------------------------------------------------------------------------------------------------------------------------</code></pre>
<p>Table 1 presents descriptive statistics for my dependent variable, independent variable, and control variables. The table depicts the number of observations, mean, standard deviation, minimum, and maximum values for the following variables: the defendant having at least one conviction, the proportion of the jury whose members are less than 50% racially similar to the defendant, at least one jury member thinking the victim was very believable, at least one jury member having sympathy for the defendant, if the case had a victim that was not White, and if the crime was a misdemeanor.</p>
<p><strong>Hypothesis and Empirical Tests</strong></p>
<p>Like I previously mentioned, my hypothesis is that “juries in which at least 50% of the jurors are the same race as the defendant are more likely to side in favor of the defendant.” I plan on testing this hypothesis through a quantitative analysis by running a regression between whether or not the defendant had at least one conviction and the proportion of the jury whose members are less than 50% racially similar to the defendant. By doing this, I will be able to see how much more likely juries with a greater proportion of members who are the same race as the defendant are to rule in favor of the defendant than juries with a smaller proportion.</p>
<p>In this case, my null hypothesis is that juries in which at least 50% of the jurors are the same race as the defendant would make the same decision on the outcome of a case as juries that were not mostly the same race as the defendant. In order to reject my null hypothesis, I would need to see the coefficient for the proportion of the jury whose members are less than 50% racially similar to the defendant be zero after running the regression, meaning that there was no difference in the likelihood of having at least one conviction between juries that were more or less racially similar to the defendant. I would also need to see the p-value for the proportion of the jury whose members are less than 50% racially similar to the defendant be 0.05 or less, meaning that there was such a small chance of committing a type I error, or rejecting the null hypothesis that juries in which at least 50% of the jurors are the same race as the defendant would make the same decision on the outcome of a case as juries that were not mostly the same race as the defendant, when it is true.</p>
<p><strong>Results</strong> <img src="Portfolio_files/figure-html/unnamed-chunk-101-1.png" width="672" /></p>
<p>Figure 1 displays the average proportion of times that someone is convicted with a jury similar in race to the defendant. The first panel displays the average proportions when the jury members were both at least 50% racially similar to the defendant and less than 50% racially similar to the defendant for when the defendant was convicted on at least one count and was not white. The second panel also displays the average proportions when the jury members were both at least 50% racially similar to the defendant and less than 50% racially similar to the defendant, but now for when the defendant was convicted on at least one count and was white.</p>
<p>Both panels highlight, as I hypothesized, the difference in jury decision making when the jury is made up of a different percent of members that match the race of the defendant. When the jury is made up of at least 50% of members of the same race as the defendant, they are more likely to not convict the defendant than juries made up of members that are less than 50% of the same race. If the defendant was not white and the majority of the jury was not white, the defendant was less likely to be convicted by a larger margin than if the defendant was white and the majority of the jury was also white.</p>
<p>To model this further, I ran two regressions between having at least one conviction and the proportion of the jury whose members were less than 50% racially similar to the defendant, controlling for victim believability, defendant sympathy, if the crime was a misdemeanor, and if the victim was not white in the second regression.</p>
<pre><code>## 
## Table 2: Proportion of the Jury whose Race Matches the Defendant and Total Convictions
## =========================================================================================
##                                                                  Dependent variable:     
##                                                              ----------------------------
##                                                                        yes.conv          
##                                                                 Baseline      Controls   
##                                                                   (1)            (2)     
## -----------------------------------------------------------------------------------------
## Prop of the jury less than 50% racially similar to defendant    0.023***      0.024***   
##                                                                 (0.008)        (0.008)   
##                                                                                          
## Victim Believability                                                           -0.013    
##                                                                                (0.008)   
##                                                                                          
## Sympathy for the Defendant                                                     -0.011    
##                                                                                (0.009)   
##                                                                                          
## Not White Victim                                                              -0.137***  
##                                                                                (0.029)   
##                                                                                          
## Misdemeanor                                                                     0.004    
##                                                                                (0.011)   
##                                                                                          
## Constant                                                        0.532***      0.676***   
##                                                                 (0.005)        (0.029)   
##                                                                                          
## -----------------------------------------------------------------------------------------
## Observations                                                     17,357        17,357    
## R2                                                               0.001          0.002    
## =========================================================================================
## Note:                                                         *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>Table 2 presents both regression analyses, regressed on the dependent variable of having at least one conviction. The average chance of the defendant having at least one conviction in their case when the jury was at least 50% racially similar to the defendant was 54.3 percentage points. When the proportion of the jury whose members were less than 50% racially similar to the defendant, the defendant was more likely to be convicted by 12.4 percentage points. This baseline provides evidence for my hypothesis, as juries with more members who matched the race of the defendant were less likely to convict the defendant. Additionally, the p-value for the average chance of the defendant having at least one conviction in their case when the jury was at least 50% racially similar to the defendant is 2 X 10 ^ -16. Since this is an extremely small value, I can reject the null hypothesis that juries in which at least 50% of the jurors are the same race as the defendant would make the same decision on the outcome of a case as juries that were not mostly the same race as the defendant.</p>
<p>Furthermore, when no jury members thought that the victim was believable or had sympathy for the defendant, the crime was a felony, and the victim was white, the average chance of a defendant having at least one conviction in their case when the jury was at least 50% racially similar to the defendant was 80.1 percentage points. When the proportion of the jury whose members were less than 50% racially similar to the defendant, the defendant was more likely to be convicted by 16.5 percentage points than when the jury was at least 50% racially similar to the defendant, holding fixed victim believability, defendant sympathy, if the crime was a misdemeanor, and if the victim was not white. I expected there to be an increase in the coefficient, as juries without a majority of the same race should be more likely to convict when there is no overarching demographical connection. When at least one jury member thought that the victim was very believable, the defendant was more likely to be convicted than if none of the jury members thought that the victim was very believable by only 0.4 percentage points. I expected there to be an increase in the likelihood of a conviction since I assumed the jury would feel bad for the victim, but the small increase could be accounted for by the fact that maybe only one out of the 12 jurors believed the victim, or they believed the victim but not to a “very believable” extent. When the victim was not white, the defendant was less likely to be convicted than if the victim was white by 36.6 percentage points. This seemed strange at first, as I expected juries to be harsher on defendants when the victim was white. However, the majority of defendants were not white (136 out of 185) and 58% of the time, juries were made up of a majority of people who identified as not white. This indicates that juries had more sympathy towards defendants that were not white, rather than white defendants. This is also proven in Figure 1, as not white defendants with a jury that was mostly not white were less likely to be convicted than white defendants with a jury was mostly white.</p>
<p>Additionally, there were two results from the second regression that surprised me. When at least one jury member had a great deal of sympathy for the defendant, the defendant was more likely to be convicted than if none of the jury had sympathy for the defendant by 19.7 percentage points. I expected this coefficient to be negative, since feeling bad for the defendant would theoretically lead the jury to not convict them. Similarly to the victim believability variable, I was only measuring if a jury member had “a great deal of sympathy” for the defendant, and they could have felt some sympathy but did not reach the “great deal” threshold. Criminal cases also require a unanimous decision by the jury, so it could be possible that at least one juror felt a “great deal of sympathy” but was overpowered by the rest of the jury, leading to a conviction. Likewise, when the crime was a misdemeanor, the defendant was more likely to be convicted than if it was a felony by 14.5 percentage points. I expected this result to be negative as well, since I thought that people charged with felonies would be more likely to be convicted than someone charged with a less serious crime. A juror could possibly have more sympathy for a defendant if the defendant was going to jail versus if they were sentenced to a fine or probation (usually the outcome of misdemeanor cases), and maybe most of the cases in the data did not result in jail or prison sentences, accounting for both of these discrepancies.</p>
<p><strong>Discussion and Conclusion</strong></p>
<p>I was able to find evidence that juries in which at least 50% of the jurors are the same race as the defendant are more likely to side in favor of the defendant. For both regressions, the defendant was more likely to be convicted when the jury was less than 50% racially similar to the defendant than when the jury was at least 50% racially similar to the defendant, including when I held fixed victim believability, defendant sympathy, if the crime was a misdemeanor, and if the victim was not white. I also found that when at least one jury member thought that the victim was very believable, the defendant was more likely to be convicted than if none of the jury members thought that the victim was very believable and when the victim was not white, the defendant was less likely to be convicted than if the victim was white. Surprisingly, I found that when at least one jury member had a great deal of sympathy for the defendant, the defendant was more likely to be convicted than if none of the jury had sympathy for the defendant and when the crime was a misdemeanor, the defendant was more likely to be convicted than if it was a felony. More research needs to be conducted on these two variables (for example, splitting the cases up by type of crime rather than overall category) in order to account for these discrepancies.</p>
<p>My results are consistent with previous research completed on the topic. Racially similar juries are more likely to side the same way as each other, as well as in favor of the defendant if they matched the race. In a 2003 study completed by law professor Phoebe C. Ellsworth for the University of Michigan Law School Scholarship Repository researching jury decision making in a race salient and non-race salient mock trial, Ellsworth found that “White mock jurors were more likely to vote to convict the Black defendant (90% of jurors voted to convict) than the White defendant (70% conviction rate).” This is consistent with my results, as juries that were mostly White with a non-White defendant and juries that were mostly not-White with a White defendant were more likely to convict than if the races matched. My results also matched the studies discussed in the Literature Review, as the first paper written by Joy (actually analyzing another study completed by Ellsworth) found that that all-White juries were more likely to convict African American defendants than White defendants. The second paper, written by Levinson, Cai, and Young, also found that people are more likely to perceive black people as guilty and therefore convict them. While I did not look at race in depth and rather as White or not-White, my results are consistent with all three studies and the idea that implicit racial bias impacts jury decision making.</p>
<p>Some potential confounds for interpreting my results and stopping me from finding a causal effect between my variables are other demographical information that I did not have available in the data. Controlling for a juror’s religion could have been useful since different religions value different punishments and sentences. For example, Catholics are less likely to support the death penalty than Protestants and would be more lenient towards a defendant in a death penalty case (Miller 2007). Similarly, controlling for whether or not a juror was married or had kids would be helpful to better explain the relationship between my independent and dependent variables. Picking parents and grandparents for a trial of a teenager, for example, would make the jury more sympathetic towards the defendant when the defendant could have been their own kid. Additionally, a potential confound for interpreting my results is whether or not a juror has any experience or strong opinions on the criminal justice system. If a juror was a lawyer or police officer, or had a family member arrested or in jail, they would have a different opinion and perspective on the case than if they had no or very limited knowledge on the topic. Ideally, I would have been able to control for all of these demographical characteristics, as well as harder to measure variables (empathy, feelings on the legal system, feelings the day of the trial), in order to be more confident that the proportion of the jury being a certain race impacts how the jury decides in the case.</p>
<p>Based on my results, it is clear that jury selection needs to be better regulated so that juries do not have preconceived biases before making life changing decisions. A case should not be decided before evidence is even presented, and attorneys should have a responsibly to create a diverse, fair jury rather than one crafted just to win. Further research needs to be completed on other demographical information in order to create a more comprehensive way to reshape the voir dire process and ensure that the legal system works more efficiently.</p>
<p><strong>Works Cited</strong> Hannaford-Agor, Paula L., Hans, Valerie P., Mott, Nicole L., and Munsterman, G. Thomas. Evaluation of Hung Juries in Bronx County, New York, Los Angeles County, California, Maricopa County, Arizona, and Washington, DC, 2000-2001. [distributor], 2006-03-30. Illegal Racial Discrimination in Jury Selection: A Continuing Legacy. (2020, August 21). Retrieved December 17, 2020, from <a href="https://eji.org/reports/illegal-racial-discrimination-in-jury-selection/" class="uri">https://eji.org/reports/illegal-racial-discrimination-in-jury-selection/</a> Joy, P. (109). RACE MATTERS IN JURY SELECTION. Northwestern University Law Review. Levinson, J., Cai, H., &amp; Young, D. (2010). Guilty By Implicit Racial Bias: The Guilty/Not Guilty Implicit Association Test. OHIO STATE JOURNAL OF CRIMINAL LAW, 8, 187-208. Miller, M.K., Hayward, R.D. Religious Characteristics and the Death Penalty. Law Hum Behav 32, 113–123 (2008). Sommers, S., &amp; Ellsworth, P. (2003). How Much Do We Really Know about Race and Juries? A Review of Social Science Theory and Research. University of Michigan Law School Scholarship Repository, 3, 997-1031.</p>
</div>
</div>
<div id="statistics-for-the-social-sciences" class="section level2">
<h2>Statistics for the Social Sciences</h2>
<h2>
Statistics for the Social Sciences
</h2>
<div id="assignment-1" class="section level3">
<h3>Assignment 1</h3>
<p><strong>Problem 1</strong></p>
<p>Install the datasets package on the console below using <code>install.packages("datasets")</code>. Now load the library.</p>
<pre class="r"><code>#Install the dataset package  
#install.packages(&quot;datasets&quot;)

#load in the library 
library(datasets)</code></pre>
<p>Load the USArrests dataset and rename it <code>dat</code>. Note that this dataset comes with R, in the package datasets, so there’s no need to load data from your computer. Why is it useful to rename the dataset?</p>
<pre class="r"><code>#Load in the dataset
USArrests</code></pre>
<pre><code>##                Murder Assault UrbanPop Rape
## Alabama          13.2     236       58 21.2
## Alaska           10.0     263       48 44.5
## Arizona           8.1     294       80 31.0
## Arkansas          8.8     190       50 19.5
## California        9.0     276       91 40.6
## Colorado          7.9     204       78 38.7
## Connecticut       3.3     110       77 11.1
## Delaware          5.9     238       72 15.8
## Florida          15.4     335       80 31.9
## Georgia          17.4     211       60 25.8
## Hawaii            5.3      46       83 20.2
## Idaho             2.6     120       54 14.2
## Illinois         10.4     249       83 24.0
## Indiana           7.2     113       65 21.0
## Iowa              2.2      56       57 11.3
## Kansas            6.0     115       66 18.0
## Kentucky          9.7     109       52 16.3
## Louisiana        15.4     249       66 22.2
## Maine             2.1      83       51  7.8
## Maryland         11.3     300       67 27.8
## Massachusetts     4.4     149       85 16.3
## Michigan         12.1     255       74 35.1
## Minnesota         2.7      72       66 14.9
## Mississippi      16.1     259       44 17.1
## Missouri          9.0     178       70 28.2
## Montana           6.0     109       53 16.4
## Nebraska          4.3     102       62 16.5
## Nevada           12.2     252       81 46.0
## New Hampshire     2.1      57       56  9.5
## New Jersey        7.4     159       89 18.8
## New Mexico       11.4     285       70 32.1
## New York         11.1     254       86 26.1
## North Carolina   13.0     337       45 16.1
## North Dakota      0.8      45       44  7.3
## Ohio              7.3     120       75 21.4
## Oklahoma          6.6     151       68 20.0
## Oregon            4.9     159       67 29.3
## Pennsylvania      6.3     106       72 14.9
## Rhode Island      3.4     174       87  8.3
## South Carolina   14.4     279       48 22.5
## South Dakota      3.8      86       45 12.8
## Tennessee        13.2     188       59 26.9
## Texas            12.7     201       80 25.5
## Utah              3.2     120       80 22.9
## Vermont           2.2      48       32 11.2
## Virginia          8.5     156       63 20.7
## Washington        4.0     145       73 26.2
## West Virginia     5.7      81       39  9.3
## Wisconsin         2.6      53       66 10.8
## Wyoming           6.8     161       60 15.6</code></pre>
<pre class="r"><code>#rename the USArrests dataset
dat &lt;- USArrests</code></pre>
<p><strong>It is useful to rename the dataset because it is easier to work with. If the data set had a longer or more complicated name, it would be difficult and time consuming to type out the dataset name every time when you want to perform a function on the data. It also lets you keep track of your work if you make different versions of it so that it is not contaminated by changes that were meant to be on one version and not the other. This will let you replicate the work if changes were made to different versions.</strong></p>
<p><strong>Problem 2</strong> Use this command to make the state names into a new variable called State.</p>
<pre class="r"><code>dat$state &lt;- tolower(rownames(USArrests))</code></pre>
<p>This dataset has the state names as row names, so we just want to make them into a new variable. We also make them all lower case, because that will help us draw a map later - the map function requires the states to be lower case.</p>
<p>List the variables contained in the dataset <code>USArrests</code>.</p>
<pre class="r"><code>#find the variables in the dataset
names(dat)</code></pre>
<pre><code>## [1] &quot;Murder&quot;   &quot;Assault&quot;  &quot;UrbanPop&quot; &quot;Rape&quot;     &quot;state&quot;</code></pre>
<p><strong>The variables in the USArrests dataset are Murder, Assault, UrbanPop, Rape, and State.</strong></p>
<p><strong>Problem 3</strong></p>
<p>What type of variable (from the DVB chapter) is <code>Murder</code>?</p>
<p><strong>Murder is a quantitative variable from the DVB chapter since it is a count of numbers.</strong></p>
<p>What R Type of variable is it?</p>
<p><strong>Murder is a numeric variable from the R type since it is also a count of numbers and functions such as mean or median can be applied.</strong></p>
<p><strong>Problem 4</strong></p>
<p>What information is contained in this dataset, in general? What do the numbers mean?</p>
<p><strong>The dataset contains arrest numbers for 4 types of crimes within all 50 states. Each state has a corresponding arrest rate for murder, assault, and rape, as well as the percent of urban population within the state. The numbers mean the number of arrests per 100,000 people in that state.</strong></p>
<p><strong>Problem 5</strong></p>
<p>Draw a histogram of <code>Murder</code> with proper labels and title.</p>
<pre class="r"><code>hist(dat$Murder, main = &quot;Histogram of Murder Arrests&quot;, xlab = &quot;Number of Murder Arrests&quot;, ylab = &quot;Frequency&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-109-1.png" width="672" /></p>
<p><strong>Problem 6</strong></p>
<p>Please summarize <code>Murder</code> quantitatively. What are its mean and median? What is the difference between mean and median? What is a quartile, and why do you think R gives you the 1st Qu. and 3rd Qu.?</p>
<pre class="r"><code>#finding the summary statistics of the murder variable
summary(dat$Murder)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.800   4.075   7.250   7.788  11.250  17.400</code></pre>
<p><strong>The mean of the murder variable is 7.788 murders, while the median is 7.250 murders. The mean is the sum of all data values divided by the number of values, or the average of the values within the dataset. The median, however, is the middle value after all of the values are put in numerical order. When distributions are skewed or there are outliers in the data, the median is better to use than the mean because the mean changes with skew or outliers and the median is more robust. A quartile is each of three values in which the data can be distributed into even fourths. I think that R gives us the 1st quartile and 3rd quartile because they can be used to calculate the interquartile range, which tells us the interval where half of the values within the data set lie. The median is also the 2nd quartile, so the three groups are given that split the data in even fourths.</strong></p>
<p><strong>Problem 7</strong></p>
<p>Repeat the same steps you followed for <code>Murder</code>, for the variables <code>Assault</code> and <code>Rape</code>. Now plot all three histograms together. You can do this by using the command <code>par(mfrow=c(3,1))</code> and then plotting each of the three.</p>
<pre class="r"><code>#make a histogram of assault
hist(dat$Assault, main = &quot;Histogram of Assault Arrests&quot;, xlab = &quot;Number of Assault Arrests&quot;, ylab = &quot;Frequency&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-111-1.png" width="480" /></p>
<pre class="r"><code>#make a histogram of rape
hist(dat$Rape, main = &quot;Histogram of Rape Arrests&quot;, xlab = &quot;Number of Rape Arrests&quot;, ylab = &quot;Frequency&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-111-2.png" width="480" /></p>
<pre class="r"><code>#plot all three histograms together
par(mfrow=c(3,1))
hist(dat$Murder, main = &quot;Histogram of Murder Arrests&quot;, xlab = &quot;Number of Murder Arrests&quot;, ylab = &quot;Frequency&quot;)
hist(dat$Assault, main = &quot;Histogram of Assault Arrests&quot;, xlab = &quot;Number of Assault Arrests&quot;, ylab = &quot;Frequency&quot;)
hist(dat$Rape, main = &quot;Histogram of Rape Arrests&quot;, xlab = &quot;Number of Rape Arrests&quot;, ylab = &quot;Frequency&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-111-3.png" width="480" /></p>
<p>What does the command par do, in your own words (you can look this up by asking R <code>?par</code>)?</p>
<p><strong>Par can be used to set the parameters on a graph, making it easier to combine multiple graphs into one.</strong></p>
<p>What can you learn from plotting the histograms together?</p>
<p><strong>By plotting these histograms together, you can compare the arrest rates for each crime. Murders happen the least frequently and assaults happen the most frequently. You can also see that the histogram for murder is unimodal and skewed, the histogram for assaults is bimodal, and the histogram for rapes is also unimodal and skewed.</strong></p>
<p><strong>Problem 8</strong></p>
<p>In the console below (not in text), type <code>install.packages("maps")</code> and press Enter, and then type <code>install.packages("ggplot2")</code> and press Enter. This will install the packages so you can load the libraries.</p>
<pre class="r"><code>#install and load in the maps and ggplot2 packages
library(&#39;maps&#39;) 
library(&#39;ggplot2&#39;) </code></pre>
<p>Run this code:</p>
<pre class="r"><code>ggplot(dat, aes(map_id=state, fill=Murder)) + 
  geom_map(map=map_data(&quot;state&quot;)) + 
  expand_limits(x=map_data(&quot;state&quot;)$long, y=map_data(&quot;state&quot;)$lat)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-113-1.png" width="720" /> What does this code do? Explain what each line is doing.</p>
<p><strong>The first line above the map code loads the library for both packages. The first line of the map code is creating a plot of murder. The ggplot function is telling R to create a plot, the “dat” is telling R which dataset to use in the plot, and the “aes(map_id=state, fill=Murder))” is telling R to plot each state and fill it with the corresponding murder arrest rate. The geom_map function creates shapes for a reference map, and the map=map_data(“state”) function is telling R to map the data that corresponds with the state variable that we created earlier. The expand_limits function makes sure that the entire map fits within the plot of the graph and the (x=map_data(“state”)<span class="math inline">\(long, y=map_data(&quot;state&quot;)\)</span>lat) tells R which data to base the plot limits off of, which in this case is the state data.</strong></p>
</div>
<div id="assignment-2" class="section level3">
<h3>Assignment 2</h3>
<p><strong>Problem 1</strong></p>
<p>Load in the data.</p>
<pre class="r"><code>dat &lt;- read.csv(file = &quot;data/Assignment2datacopy.csv&quot;)</code></pre>
<p>What are the dimensions of the dataset?</p>
<pre class="r"><code>dim(dat)</code></pre>
<pre><code>## [1] 171   7</code></pre>
<p><strong>There are 171 columns and 7 rows (or 171 respondents and 7 questions)</strong></p>
<p><strong>Problem 2</strong></p>
<p>Describe the variables in the dataset.</p>
<pre class="r"><code>names(dat)</code></pre>
<pre><code>## [1] &quot;mjage&quot;     &quot;cigage&quot;    &quot;iralcage&quot;  &quot;age2&quot;      &quot;sexatract&quot; &quot;speakengl&quot;
## [7] &quot;irsex&quot;</code></pre>
<p><strong>There are 7 variables in the dataset:</strong> <strong>1. “mjage” (How old were you the first time you used marijuana or hashish?)</strong> <strong>2. “cigage” (How old were you when you first started smoking cigarettes everyday?)</strong> <strong>3. “iralcage” (How old were you when you first tried alcohol?)</strong> <strong>4. “age2” (Recoded final edited age (since respondents had multiple chances to change their age throughout the survey))</strong> <strong>5. “sexatract” (Sexual attraction)</strong> <strong>6. “speakengl” (How well do you speak English)</strong> <strong>7. “irsex” (Imputation revised gender)</strong></p>
<p><strong>Mjage, cigage, and iralcage are numeric variables (or quantitative) because the respondent gave an exact number for their answers for each variable and functions like mean can be applied to find the average age the respondents tried marijuana, cigarettes, and alcohol. Age2, sexatract, speakengl, and irsex are categorical variables because the respondents’ answers were split into categories when coded. Because respondents had the opportunity to change their age throughout the interview, the age variable was calculated from the raw birth date and the final edited interview date, the age entered in the questionnaire roster (if it exists), and the pre-interview screener age because interviewees had the opportunity to change their age throughout the interview.</strong></p>
<p>What is this dataset about? Who collected the data, what kind of sample is it, and what was the purpose of generating the data? <strong>The dataset is a small sample from the National Survey of Drug Use and Health, which was conducted by the Center for Behavioral Health Statistics and Quality (CBHSQ, formerly the Office of Applied Studies) within the Substance Abuse and Mental Health Services Administration (SAMHSA) and is conducted by RTI International, Research Triangle Park, North Carolina. The survey was conducted through a computer assisted administration, and was changed from a strictly national design to a state-based sampling plan in 1999. The primary purpose of generating the data was to measure the prevalence and correlation of substance use and mental health issues in the United States, according to the NSDUH 2019 Codebook.</strong></p>
<p><strong>Problem 3: Age and gender</strong></p>
<p>What is the age distribution of the sample like? Make sure you read the codebook to know what the variable values mean.</p>
<pre class="r"><code>hist(dat$age2, main = &quot;Histogram of Recoded Final Edited Age Categories&quot;, xlab = &quot;Recoded Final Edited Age Categories&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-117-1.png" width="672" /> <strong>The age variable is skewed to the left, with the majority of the values in the 14-16 bin (which means that the majority of the people in the dataset are 30 to 64 years old, according to the codebook). There are fewer people who were very young when filling out the survey, with only a few in the 4-6 and 6-8 bins (meaning that the respondents in these bins were 15-17 and 18-19, respectively).</strong></p>
<p>Do you think this age distribution representative of the US population? Why or why not?</p>
<pre class="r"><code>min(dat$age2) #the youngest respondent was 15 (category 4)</code></pre>
<pre><code>## [1] 4</code></pre>
<pre class="r"><code>max(dat$age2) #the oldest respondent was 65 years old or older (category 17)</code></pre>
<pre><code>## [1] 17</code></pre>
<p><strong>Yes, I think that this age distribution is representative of the US population. The survey was conducted in order to evaluate drug use, and I would think that people would not try drugs before turning 15 (the youngest respondent) and very few people would continue using drugs when they are 65 years old or older (the oldest respondent). Also, according to the codebook, the participants were randomly selected to complete the survey, which fulfills one requirement of a representative sample. It makes sense that the number of respondents increased when the respondents were 19 to 23 (8 to 12 categories) as those are the ages when most people would be exposed to marijuana, drugs, and alcohol for the first time, and that the number of respondents increased again when the respondents were 24 to 64 (categories 12 to 16), as those are the ages that people would continue to use marijuana, drugs, and alcohol as adults (especially after potentially being exposed in college). I also think that it makes sense that the majority of respondents were 34 to 64 years old, because older generations are more likely to have smoked cigarettes everyday before perception and knowledge changed.</strong></p>
<p>Is the sample balanced in terms of gender? If not, are there more females or males?</p>
<pre class="r"><code>table(dat$irsex, dat$age2)</code></pre>
<pre><code>##    
##      4  6  7  8  9 10 11 12 13 14 15 16 17
##   1  1  1  1  0  1  1  3  3 14 10 33 14  9
##   2  1  0  0  2  6  2  3  4 13  6 29 10  4</code></pre>
<p><strong>The sample is not balanced in terms of gender. There were 91 males included in the dataset and 80 females included.</strong></p>
<p>Use this code to draw a stacked bar plot to view the relationship between sex and age. What can you conclude from this plot?</p>
<pre class="r"><code>tab.agesex &lt;- table(dat$irsex, dat$age2)
barplot(tab.agesex,
        main = &quot;Stacked barchart&quot;,
        xlab = &quot;Age category&quot;, ylab = &quot;Frequency&quot;,
        legend.text = rownames(tab.agesex),
        beside = FALSE) # Stacked bars (default)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-120-1.png" width="672" /> <strong>From this plot, I can conclude that more males answered the survey than females, as most age categories have a higher frequency of males than females (especially in categories 6 and 7). I can also conclude than more older males answered the survey than older females. As indicated in both the plot and the table of sex and age categories, the highest five categories (13, 14, 15, 16, and 17) had more male responses than females (1 more, 4 more, 4 more, 4 more, and 5 more, respectively).</strong></p>
<p><strong>Problem 4: Substance use</strong></p>
<p>For which of the three substances included in the dataset (marijuana, alcohol, and cigarettes) do individuals tend to use the substance earlier?</p>
<pre class="r"><code>boxplot(dat$mjage, dat$cigage, dat$iralcage, main = &quot;Substance Usage and Age&quot;,
        ylab=&quot;Age&quot;, xlab=&quot;Types of Substances&quot;, 
        names=c(&quot;Marijuana or Hashish&quot;,&quot;Cigarettes&quot;,&quot;Alcohol&quot;))</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-121-1.png" width="672" /> <strong>Based on the boxplot of substance usage and age, it looks like individuals tend to use alcohol earlier than marijuana / hashish or earlier than they started smoking cigarettes every day.</strong></p>
<pre class="r"><code>min(dat$mjage) </code></pre>
<pre><code>## [1] 7</code></pre>
<pre class="r"><code>min(dat$cigage) </code></pre>
<pre><code>## [1] 10</code></pre>
<pre class="r"><code>min(dat$iralcage) </code></pre>
<pre><code>## [1] 5</code></pre>
<p><strong>Looking at the minimum values for each type of substance, the minimum recorded age for when an individual first tried marijuana / hashish was 7, the minimum recorded age for when an individual first started smoking cigarettes everyday was 10, and the minimum recorded age for when an individual first tried alcohol was 5. This also confirms that like individuals tend to use alcohol earlier than marijuana / hashish or earlier than they started smoking cigarettes every day.</strong></p>
<p><strong>Problem 5: Sexual attraction</strong></p>
<p>What does the distribution of sexual attraction look like? Is this what you expected?</p>
<pre class="r"><code>dat.sexat &lt;- subset(dat$sexatract, subset = dat$sexatract&lt;7)

hist(dat.sexat, main = &quot;Distribution of Sexual Attraction&quot;, xlab = &quot;Sexual Attraction&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-123-1.png" width="672" /> <strong>The distribution of sexual attraction is skewed to the right, with the majority of respondents answering that they are only attracted to the opposite sex (category one). The next greatest frequency of response is that the respondent is mostly attracted to the opposite sex (category 2), and then followed by the respondent is equally attracted to males and females (category 3). This is exactly what I expected because I would think that an overwhelming majority of people would be straight as that is most common in the United States.</strong></p>
<p>What is the distribution of sexual attraction by gender?</p>
<pre class="r"><code>table(dat$irsex, dat$sexatract)</code></pre>
<pre><code>##    
##      1  2  3  4  5  6 99
##   1 82  3  0  1  2  1  2
##   2 54 13  9  2  1  0  1</code></pre>
<pre class="r"><code>library(dplyr) 

dat$sexatract &lt;- dat$sexatract %&gt;% na_if(., &quot;85&quot;)
dat$sexatract &lt;- dat$sexatract %&gt;% na_if(., &quot;94&quot;)
dat$sexatract &lt;- dat$sexatract %&gt;% na_if(., &quot;97&quot;)
dat$sexatract &lt;- dat$sexatract %&gt;% na_if(., &quot;98&quot;)
dat$sexatract &lt;- dat$sexatract %&gt;% na_if(., &quot;99&quot;)

dat.five &lt;- data.frame(dat$sexatract, dat$irsex)

barplot &lt;- barplot(table(dat.five$dat.irsex, dat.five$dat.sexatract),
        main = &quot;Sexual Attraction and Gender&quot;,
        xlab = &quot;Sexaul Attraction Category&quot;,
        ylab = &quot;Count&quot;,
        border = &quot;black&quot;,
        col = c(&quot;hotpink1&quot;, &quot;blue&quot;),
        ylim = c(0,171), legend.text = c(&quot;Males&quot;,&quot;Females&quot;),
        )</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-124-1.png" width="672" /></p>
<pre class="r"><code>barplot</code></pre>
<pre><code>## [1] 0.7 1.9 3.1 4.3 5.5 6.7</code></pre>
<p><strong>Looking at the stacked bar plot of sexual attraction and gender, it confirms what was seen in the table that more males than females responded that they are only attracted to the opposite sex (category one). It also confirms that more females than males answered that they are mostly attracted to the opposite sex (category 2) and are equally attracted to males and females (category 3).</strong></p>
<p><strong>Problem 6: English speaking</strong></p>
<p>What does the distribution of English speaking look like in the sample?</p>
<pre class="r"><code>hist(dat$speakengl, main = &quot;Histogram of English Speakers&quot;, xlab = &quot;How Well You Speak English (1 - lowest, 4 - highest)&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-125-1.png" width="672" /> <strong>The distribution of English speaking looks extremely skewed to the right, with the majority of respondents answering that they speak English very well (category one).</strong></p>
<pre class="r"><code>table(dat$speakengl)</code></pre>
<pre><code>## 
##   1   2   3 
## 161   8   2</code></pre>
<p><strong>Looking at the table of respondents who answered the “Speak English” question,161 people answered that the speak English very well, eight answered that they speak English well, and only two answered that they speak English not well. This confirms the skew seen in the histogram.</strong></p>
<p>Is this what you might expect for a random sample of the US population? <strong>This is what I would expect a random sample of the US population to look like, as the majority of people in the US speak English and speak it very well. Even if there were people included in the random sample from places that do not speak as much English, like Texas or Miami, for example, the large amount of people who do speak English well would account for what is seen in the histogram.</strong></p>
<p>Are there more English speaker females or males?</p>
<pre class="r"><code>table(dat$irsex, dat$speakengl)</code></pre>
<pre><code>##    
##      1  2  3
##   1 84  7  0
##   2 77  1  2</code></pre>
<pre class="r"><code>barplot(table(dat$irsex, dat$speakengl),
        main=&quot;English Speaking Level and Gender&quot;,
        xlab=&quot;English Speaking Level Category&quot;,
        ylab=&quot;Count&quot;,
        col=c(&quot;hotpink1&quot;, &quot;blue&quot;),
        ylim = c(0,200),  legend.text = c(&quot;Males&quot;,&quot;Females&quot;)
)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-127-1.png" width="672" /> <strong>Looking at the stacked bar plot of English speaking level and gender, it looks like the number of males and females that answered that they spoke English very well was pretty close, will a few more males (exactly 11, according to the table). The plot also confirms that more men responded that they spoke English well than women, and that no men responded that they spoke English not well.</strong></p>
</div>
<div id="exam-1" class="section level3">
<h3>Exam 1</h3>
<p>Load the data into an R data frame.</p>
<pre class="r"><code>dat &lt;- read.csv(file = &quot;data/fatal-police-shootings-datacopy.csv&quot;)</code></pre>
<p><strong>Problem 1 (10 points)</strong></p>
<ol style="list-style-type: lower-alpha">
<li>Describe the dataset. This is the source: <a href="https://github.com/washingtonpost/data-police-shootings" class="uri">https://github.com/washingtonpost/data-police-shootings</a> . Write two sentences (max.) about this.</li>
</ol>
<p><strong>The dataset is made up of every fatal police shooting in the US since January 1, 2015, as collected by the Washington Post. The dataset includes 6594 people and 17 variables, which contain important information about each shooting, such as the name of the person shot, their mental health status, and their race.</strong></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>How many observations are there in the data frame?</li>
</ol>
<pre class="r"><code>dim(dat)</code></pre>
<pre><code>## [1] 6594   17</code></pre>
<p><strong>There are 6594 observations in the data frame.</strong></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Look at the names of the variables in the data frame. Describe what “body_camera”, “flee”, and “armed” represent, according to the codebook. Again, only write one sentence (max) per variable.</li>
</ol>
<pre class="r"><code>names(dat)</code></pre>
<pre><code>##  [1] &quot;id&quot;                      &quot;name&quot;                   
##  [3] &quot;date&quot;                    &quot;manner_of_death&quot;        
##  [5] &quot;armed&quot;                   &quot;age&quot;                    
##  [7] &quot;gender&quot;                  &quot;race&quot;                   
##  [9] &quot;city&quot;                    &quot;state&quot;                  
## [11] &quot;signs_of_mental_illness&quot; &quot;threat_level&quot;           
## [13] &quot;flee&quot;                    &quot;body_camera&quot;            
## [15] &quot;longitude&quot;               &quot;latitude&quot;               
## [17] &quot;is_geocoding_exact&quot;</code></pre>
<p><strong>The “body_camera” variable indicates whether or not news reports stated that a police officer that was at the scene of the incident was wearing a body camera and may have recorded parts of the incident. The “flee” variable indicates whether or not news reports have started that the victim was moving away from the officer during the incident. The “armed” variable indicates whether or not the victim was armed with any type of weapon that could have been thought to as possible to cause harm by an officer.</strong></p>
<ol start="4" style="list-style-type: lower-alpha">
<li>What are three weapons that you are surprised to find in the “armed” variable? Make a table of the values in “armed” to see the options.</li>
</ol>
<pre class="r"><code>head(table(dat$armed))</code></pre>
<pre><code>## 
##                 air conditioner      air pistol  Airsoft pistol              ax 
##             207               1               1               3              24 
##        barstool 
##               1</code></pre>
<p><strong>Three weapons that I was surprised to find in the dataset were an air conditioner, microphone, and pen.I feel like an air conditioner and microphone are definitely unusual weapons, and the pen does not seem like it would do anything against an officer’s gun.</strong></p>
<p><strong>Problem 2 (10 points)</strong></p>
<ol style="list-style-type: lower-alpha">
<li>Describe the age distribution of the sample. Is this what you would expect to see?</li>
</ol>
<pre class="r"><code>hist(dat$age, main = &quot;Age Distribution of the Sample&quot;, xlab = &quot;Age&quot;, xlim = c(0, 100))</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-132-1.png" width="672" /></p>
<p><strong>The age distribution of the sample is skewed to the right, with a large number of values between ages 20 and 40. This indicates that most people killed by the police in fatal shootings are young, and that older people between ages 60 and 100 are being killed in fatal police shootings at a much lower rate. This is exactly what I expected to see in the age distribution because younger people, especially in the 20 to 40 year old range, are more likely to have encounters with the police, as well as be healthy and be able to try to talk to or provoke the police and try to run away.</strong></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>To understand the center of the age distribution, would you use a mean or a median, and why? Find the one you picked.</li>
</ol>
<pre class="r"><code>median(dat$age, na.rm = TRUE)</code></pre>
<pre><code>## [1] 35</code></pre>
<p><strong>To understand the center of the age distribution, I would use the median because the data is skewed to the right and the median is a more robust measure of skewed data as the mean changes with skew or outliers. The median of the age distribution for this dataset is 35 years old, and I removed the missing values since there are only 308 of them out of 6594 observations and they indicate that the person’s age was unknown or missing.</strong></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Describe the gender distribution of the sample. Do you find this surprising?</li>
</ol>
<pre class="r"><code>table(dat$gender)</code></pre>
<pre><code>## 
##         F    M 
##    3  293 6298</code></pre>
<pre class="r"><code>counts &lt;- table(dat$gender, useNA = &quot;ifany&quot;)
barplot(counts, main = &quot;Gender Distrubution&quot;, xlab = &quot;Gender&quot;, ylab = &quot;Counts&quot;, names=c(&quot;None&quot;, &quot;Females&quot;, &quot;Males&quot;))</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-134-1.png" width="672" /></p>
<p><strong>The gender distribution is definitely skewed, with 6005 more males being fatally shot by police officers than females. This can also be seen in the barplot, with the majority of respondents being males. There were also 3 unknown values in the data, indicating no gender according to the codebook. I do not find this surprising, however, because the majority of police shootings that we hear about in the news are of males, with the rare occurrence of a female. While females are definitely still being killed by the police, it is happening disproportionately to males which is indicated by both the news and the data.</strong></p>
<p><strong>Problem 3 (10 points)</strong></p>
<ol style="list-style-type: lower-alpha">
<li>How many police officers had a body camera, according to news reports? What proportion is this of all the incidents in the data? Are you surprised that it is so high or low?</li>
</ol>
<pre class="r"><code>table(dat$body_camera)</code></pre>
<pre><code>## 
## False  True 
##  5684   910</code></pre>
<pre class="r"><code>910/6594</code></pre>
<pre><code>## [1] 0.1380042</code></pre>
<p><strong>According to the news reports, 910 police officers had a body camera, which is about 13.80% of all incidents in the data. I am surprised that this is so low because I would think that after the 2014 killing of Michael Brown and subsequent police shootings, police offices and the public would call for an increased use of body cameras.</strong></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>In how many of the incidents was the victim fleeing? What proportion is this of the total number of incidents in the data? Is this what you would expect?</li>
</ol>
<pre class="r"><code>table(dat$flee)</code></pre>
<pre><code>## 
##                     Car        Foot Not fleeing       Other 
##         491        1058         845        3952         248</code></pre>
<pre class="r"><code>1058+845+248 #fleeing</code></pre>
<pre><code>## [1] 2151</code></pre>
<pre class="r"><code>2151/6594</code></pre>
<pre><code>## [1] 0.3262056</code></pre>
<p><strong>There are 2151 incidents of the victim fleeing, which is about 32.62% of all incidents in the data. This was calculated by including the 491 unknown observations in the “not fleeing” group, as what this category of observations represents was not included in the codebook and cannot be assumed to be fleeing, especially since there is already an “other” group. This is pretty much what I would expect because I would think that the majority of people when interacting with the police would not flee, since it is known that the consequences would probably be worse because of it.</strong></p>
<p><strong>Problem 4 (10 points)</strong></p>
<ol style="list-style-type: lower-alpha">
<li>Describe the relationship between the variables “body camera” and “flee” using a stacked barplot. What can you conclude from this relationship?</li>
</ol>
<p><em>Hint 1: The categories along the x-axis are the options for “flee”, each bar contains information about whether the police officer had a body camera (vertically), and the height along the y-axis shows the frequency of that category).</em></p>
<p><em>Hint 2: Also, if you are unsure about the syntax for barplot, run ?barplot in R and see some examples at the bottom of the documentation. This is usually a good way to look up the syntax of R code. You can also Google it.</em></p>
<pre class="r"><code>library(ggplot2) 
ggplot(dat, aes(fill=body_camera, y=frequency(body_camera), x=flee)) + 
  geom_bar(position=&quot;stack&quot;, stat=&quot;identity&quot;) +
  ggtitle(&quot;Stacked Barplot of Body Camera Usage and Victim Fleeing&quot;) +
  labs(y=&quot;Frequency of Body Camera Usage&quot;, x = &quot;How the Victim Fled&quot;) +
  labs(fill = &quot;Body Camera Usage&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-137-1.png" width="672" /></p>
<p><strong>From the plot, it can be seen that everybody that fled from the police did so more often when the police were not wearing a body camera. It looks like not fleeing and the police wearing a body camera are related, as when more police wore a body camera less people fled (since the not fleeing bar had the most police wearing a body camera). You can also see that the most people fled by using a car, which during those indicidents police were less likely to wear a body camera than to actually have it on. While it is unknown what the first bar represents as it is left out of the codebook, it follows the same pattern of the other bars that the police did not wear body cameras more often than they did.</strong></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Describe the relationship between age and race by using a boxplot. What can you conclude from this relationship?</li>
</ol>
<p><em>Hint 1: The categories along the x-axis are the race categories and the height along the y-axis is age.</em></p>
<p><em>Hint 2: Also, if you are unsure about the syntax for boxplot, run ?boxplot in R and see some examples at the bottom of the documentation. This is usually a good way to look up the syntax of R code. You can also Google it.</em></p>
<pre class="r"><code>boxplot &lt;- ggplot(dat, aes(x=race, y=age)) + 
  geom_boxplot() +
  ggtitle(&quot;Boxplot of Race and Age&quot;)
boxplot</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-138-1.png" width="672" /></p>
<p><strong>The relationship between race and age looks pretty even, with the median values of age for each race category between 25 and 37. The range for all ages except White is between 12 and 62, while the White range extends from about 5 to 80. There are a significant number of outliers, however, for the Black, Hispanic, and White categories. The NAs for age were removed, and the unknown race category was plotted first on the graph but cannot be interpreted as it is not in the codebook. From this relationship, I can conclude that on average, all races of a similar age between 25 and 37 are killed at similar rates. While there are some outliers to this, especially for the Black and Hispanic categories, all of the medians of age for each race is almost identical.</strong></p>
<p><strong>Extra credit (10 points)</strong></p>
<ol style="list-style-type: lower-alpha">
<li>What does this code tell us?</li>
</ol>
<pre class="r"><code>mydates &lt;- as.Date(dat$date)
head(mydates)
(mydates[length(mydates)] - mydates[1])</code></pre>
<p><strong>The as.Date function in R converts the string of date values into actual dates that functions can be applied to. The head function makes sure that this conversion was done correctly by showing us the first 6 values of the data. The “(mydates[length(mydates)] - mydates[1])” function tells us the time difference between the last observation (which is the entire length of mydates) and the first obeservation (which is "mydates[1]), which ends up being 2458 days.</strong></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>On Friday, a new report was published that was described as follows by The Guardian: “More than half of US police killings are mislabelled or not reported, study finds.” Without reading this article now (due to limited time), why do you think police killings might be mislabelled or underreported?</li>
</ol>
<p><strong>I think that police killings may be mislabeled or underreported because police offices do not want to admit to have killing anyone or engaging in illegal or overly forceful activities. I think that there may be disparities between the police reporting shooting and witnesses observing shootings, as police offices may be less likely to do so. I also think that police offices may mislabel the shooting as something that was not their fault so that they do not get attacked by the public for the incident.</strong></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Regarding missing values in problem 4, do you see any? If so, do you think that’s all that’s missing from the data?</li>
</ol>
<p><strong>Yes, I saw missing values in problem 4 in the age category. The race and fleeing variable also had unknown values, as it was not indicated by the codebook. I do not think that this is all that is missing from the data, as other variables, such as gender, also had missing or unknown values. I think that going along with part b, some of the data reported was mislabeled or incorrect, and this may be represented by the missing or unknown values in the data.</strong></p>
</div>
<div id="assignment-3" class="section level3">
<h3>Assignment 3</h3>
<p>Load the data.</p>
<pre class="r"><code>library(readr)
library(knitr)
dat.crime &lt;- read_delim(&quot;/Users/briannafisher/Dropbox/CRIM250/Data/crime_simple.txt&quot;, delim = &quot;\t&quot;)</code></pre>
<p>This is a dataset from a textbook by Brian S. Everitt about crime in the US in 1960. The data originate from the Uniform Crime Report of the FBI and other government sources. The data for 47 states of the USA are given.</p>
<p>Here is the codebook:</p>
<p>R: Crime rate: # of offenses reported to police per million population</p>
<p>Age: The number of males of age 14-24 per 1000 population</p>
<p>S: Indicator variable for Southern states (0 = No, 1 = Yes)</p>
<p>Ed: Mean of years of schooling x 10 for persons of age 25 or older</p>
<p>Ex0: 1960 per capita expenditure on police by state and local government</p>
<p>Ex1: 1959 per capita expenditure on police by state and local government</p>
<p>LF: Labor force participation rate per 1000 civilian urban males age 14-24</p>
<p>M: The number of males per 1000 females</p>
<p>N: State population size in hundred thousands</p>
<p>NW: The number of non-whites per 1000 population</p>
<p>U1: Unemployment rate of urban males per 1000 of age 14-24</p>
<p>U2: Unemployment rate of urban males per 1000 of age 35-39</p>
<p>W: Median value of transferable goods and assets or family income in tens of $</p>
<p>X: The number of families per 1000 earning below 1/2 the median income</p>
<p>We are interested in checking whether the reported crime rate (# of offenses reported to police per million population) and the average education (mean number of years of schooling for persons of age 25 or older) are related.</p>
<p><strong>Problem 1</strong></p>
<p>How many observations are there in the dataset? To what does each observation correspond?</p>
<pre class="r"><code>dim(dat.crime)</code></pre>
<pre><code>## [1] 47 14</code></pre>
<p><strong>There are 47 observations in the dataset, with 14 different variables. Each observation corresponds to a particular state.</strong></p>
<p><strong>Problem 2</strong></p>
<p>Draw a scatterplot of the two variables. Calculate the correlation between the two variables. Can you come up with an explanation for this relationship?</p>
<pre class="r"><code>plot(dat.crime$Ed, dat.crime$R, main = &quot;Scatterplot of Average Education and Reported Crime Rate&quot;, xlab = &quot;mean number of years of schooling for persons of age 25 or older times 10&quot;, ylab = &quot;# of offenses reported to police per million population&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-142-1.png" width="576" /></p>
<pre class="r"><code>cor(dat.crime$R, dat.crime$Ed)</code></pre>
<pre><code>## [1] 0.3228349</code></pre>
<p><strong>The correlation of the reported crime rate (measured in the number of offenses reported to police per million population) and average education (measured in the mean number of years of schooling for persons of age 25 or older times 10) is 0.32, which is pretty weak. The scatterplot of these variables proves this, as it is positive but fairly spread out. One explanation for this relationship may be that states with bigger cities are populated by people with more years of education than states with rural areas, and states with bigger cities have higher reported crime rates. While these two variables specificaly are not highly correlated (since the value was 0.32), high average education and high reported crime rates are both characteristic of states with big cities.</strong></p>
<p><strong>Problem 3</strong></p>
<p>Regress reported crime rate (y) on average education (x) and call this linear model <code>crime.lm</code> and write the summary of the regression by using this code, which makes it look a little nicer <code>{r, eval=FALSE} kable(summary(crime.lm)$coef, digits = 2)</code>.</p>
<pre class="r"><code>crime.lm &lt;- lm(formula = R ~ Ed, data = dat.crime)

summary(crime.lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = R ~ Ed, data = dat.crime)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -60.061 -27.125  -4.654  17.133  91.646 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) -27.3967    51.8104  -0.529   0.5996  
## Ed            1.1161     0.4878   2.288   0.0269 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 37.01 on 45 degrees of freedom
## Multiple R-squared:  0.1042, Adjusted R-squared:  0.08432 
## F-statistic: 5.236 on 1 and 45 DF,  p-value: 0.02688</code></pre>
<p><strong>Problem 4</strong> 4. Are the four assumptions of linear regression satisfied? To answer this, draw the relevant plots. (Write a maximum of one sentence per assumption.)</p>
<pre class="r"><code>#linearity and independence assumptions - residuals vs. x plot
plot(dat.crime$Ed, crime.lm$residuals, ylim=c(-15,15), main=&quot;Residuals vs. Average Education&quot;, xlab=&quot;mean number of years of schooling for persons of age 25 or older times 10&quot;, ylab=&quot;Residuals&quot;)
abline(h = 0, lty=&quot;dashed&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-145-1.png" width="672" /></p>
<pre class="r"><code>#linearity assumption - residuals vs. fitted
plot(crime.lm, which=1)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-146-1.png" width="672" /></p>
<p><strong>Both the residuals vs. x plot and residuals vs. fitted plot show no clear linear patterns and the residuals appear evenly spread out above and below both lines, so it satisfies the linearity assumption (because there are also only 47 observations, with the number that are given we can assume this is good enough for linear regression).</strong></p>
<p><strong>Looking again at the residuals vs. x plot, we see that there are no patterns in the plot, so the independence assumption is fulfilled.</strong></p>
<pre class="r"><code>#equal variance / homoscedasticity assumption - resuiduals vs. predicted values
plot(crime.lm, which=3)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-147-1.png" width="672" /></p>
<p><strong>Looking at the residuals plotted against the predicted values, the red line looks pretty flat and there are no significant trends in the red line, so the equal variance / homoscedasticity assumption looks satisfied, but the scatterplot we made earlier shows no strong linear association.</strong></p>
<pre class="r"><code>#Normal population assumption - residuals vs. leverage plot
plot(crime.lm, which=5)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-148-1.png" width="672" /></p>
<pre class="r"><code>#Normal population assumption - Normal qq plot
plot(crime.lm, which=2)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-149-1.png" width="672" /></p>
<p><strong>The residuals vs. leverage plot fulfills the normal population assumption as the values are within Cook’s distance, but the normal qq plot looks like there is an issue with some of the values curving, especially since we know these are not outliers.</strong></p>
<p><strong>Problem 5</strong></p>
<p>Is the relationship between reported crime and average education statistically significant? Report the estimated coefficient of the slope, the standard error, and the p-value. What does it mean for the relationship to be statistically significant?</p>
<p><strong>The relationship between reported crime rates and average education is statistically significant. The coefficient of the slope is 1.1161, which means that on average, for every additional year of average education, the reported crime rate increases by 1.1161 offenses reported to police per million population. The standard error is 0.4878, which means that the number of offenses reported to the police per million population can vary by 0.4878 offenses. The p-value is 0.02688, which means that we can reject the null hypothesis and conclude that there is a relationship between reported crime rates and average education. For the relationship to be statistically significant, it would mean that the p-value is less than 0.05, telling us that it is unlikely that the relationship between reported crime rates and average education is due to chance.</strong></p>
<p><strong>Problem 6</strong></p>
<p>How are reported crime and average education related? In other words, for every unit increase in average education, how does reported crime rate change (per million) per state?</p>
<p><strong>On average, for every additional year of average education, the reported crime rate increases by 1.1161 offenses reported to police per million population per state. </strong></p>
<p><strong>Problem 7</strong></p>
<p>Can you conclude that if individuals were to receive more education, then crime will be reported more often? Why or why not?</p>
<p><strong>You cannot conclude that if individuals were to receive more education, then crime will be reported more often. While the p-value indicates that there is some relationship between the two variables, the correlation between them is only 0.32 and the r-squared value is 0.1042. Neither of these values indicate a strong relationship between average education and reported crime rate. This statement also implies causation, that if people received more education then crime will decrease. Causation cannot be assumed, as there may be other causes or hidden variables that influence the decrease in crime rate with increased education. Similarly, this dataset is in relation to states, not individuals, so this statement could not be proven by this linear model.</strong></p>
</div>
<div id="exam-2" class="section level3">
<h3>Exam 2</h3>
<p><strong>Instructions</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>Create a folder in your computer (a good place would be under Crim 250, Exams).</p></li>
<li><p>Download the dataset from the Canvas website (sim.data.csv) onto that folder, and save your Exam 2.Rmd file in the same folder.</p></li>
<li><p>Data description: This dataset provides (simulated) data about 200 police departments in one year. It contains information about the funding received by the department as well as incidents of police brutality. Suppose this dataset (sim.data.csv) was collected by researchers to answer this question: <strong>“Does having more funding in a police department lead to fewer incidents of police brutality?”</strong></p></li>
<li><p>Codebook:</p></li>
</ol>
<ul>
<li>funds: How much funding the police department received in that year in millions of dollars.</li>
<li>po.brut: How many incidents of police brutality were reported by the department that year.</li>
<li>po.dept.code: Police department code</li>
</ul>
<p><strong>Problem 1: EDA (10 points)</strong></p>
<p>Describe the dataset and variables. Perform exploratory data analysis for the two variables of interest: funds and po.brut.</p>
<pre class="r"><code>dat &lt;- read.csv(file = &#39;/Users/briannafisher/Dropbox/Github/BriannaFisher/data/sim.data.2.csv&#39;)

dim(dat)</code></pre>
<pre><code>## [1] 200   3</code></pre>
<pre class="r"><code>names(dat)</code></pre>
<pre><code>## [1] &quot;po.dept.code&quot; &quot;funds&quot;        &quot;po.brut&quot;</code></pre>
<p><strong>The dataset has 200 observations and 3 variables, with each observation corresponding to a particular police department. The variables are “po.dept.code”, which is the police department code, “funds”, which is the amount of funding the department received that year in millions of dollars, and “po.brut”, which is the number of incidents of police brutality reported by the department that year.</strong></p>
<pre class="r"><code>plot(dat$funds, dat$po.brut, main = &quot;Scatterplot of Department Funding and Police Brutality&quot;, xlab = &quot;Department Funding in Millions of Dollars that Year&quot;, ylab = &quot;Number of Incidents of Police Brutality that Year&quot;, xlim = c(0,100))</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-151-1.png" width="672" /></p>
<pre class="r"><code>cor(dat$funds, dat$po.brut)</code></pre>
<pre><code>## [1] -0.9854706</code></pre>
<p><strong>Looking at the scatterplot of department funding and police brutality, it looks like the two have a strong, negative association. As department funding increases, it looks like reported police brutality incidents decreases. The two variables also have a correlation of -0.985, indicating that the two are strongly, negatively correlated.</strong></p>
<p><strong>Problem 2: Linear regression (30 points)</strong></p>
<ol style="list-style-type: lower-alpha">
<li>Perform a simple linear regression to answer the question of interest. To do this, name your linear model “reg.output” and write the summary of the regression by using “summary(reg.output)”.</li>
</ol>
<pre class="r"><code>reg.output &lt;- lm(formula = po.brut ~ funds, data = dat)
summary(reg.output)</code></pre>
<pre><code>## 
## Call:
## lm(formula = po.brut ~ funds, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.9433 -0.2233  0.2544  0.5952  1.1803 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 40.543069   0.282503  143.51   &lt;2e-16 ***
## funds       -0.367099   0.004496  -81.64   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9464 on 198 degrees of freedom
## Multiple R-squared:  0.9712, Adjusted R-squared:  0.971 
## F-statistic:  6666 on 1 and 198 DF,  p-value: &lt; 2.2e-16</code></pre>
<ol start="2" style="list-style-type: lower-alpha">
<li>Report the estimated coefficient, standard error, and p-value of the slope. Is the relationship between funds and incidents statistically significant? Explain.</li>
</ol>
<p><strong>The relationship between reported incidents of police brutality and department funding is statistically significant. The estimated coefficient is -0.367, which means that one unit higher of funding is associated with 0.367 less reported incidents of police brutality. The standard error is 0.0045, which means that the number of reported incidents of police brutality can vary by 0.0045 incidents. The p-value is less than 2.2e-16, which means that we can reject the null hypothesis and conclude that there is a relationship between reported incidents of police brutality and department funding. For the relationship to be statistically significant, it would mean that the p-value is less than 0.05 (which 2.2e-16 is), telling us that it is unlikely that the relationship between reported incidents of police brutality and department funding is due to chance.</strong></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Draw a scatterplot of po.brut (y-axis) and funds (x-axis). Right below your plot command, use abline to draw the fitted regression line, like this:</li>
</ol>
<pre class="r"><code>plot(dat$funds, dat$po.brut, main = &quot;Scatterplot of Department Funding and Police Brutality&quot;, xlab = &quot;Department Funding in Millions of Dollars that Year&quot;, ylab = &quot;Number of Incidents of Police Brutality that Year&quot;, xlim = c(0,100))
abline(reg.output, col = &quot;red&quot;, lwd=2)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-154-1.png" width="576" /> Does the line look like a good fit? Why or why not?</p>
<p><strong>Looking at the scatterplot, the line does not look like a good fit. The points on the scatterplot look curved, with the values on both ends of the line curving away from the line. If the line was a good fit, it would have no pattern and the data would also form a straight line. However, looking at the regression output, the R squared is 0.9712, indicating that the model fits the data incredibly well. Because my eyes can be deceiving and I changed the x-axis of the plot to include 0 squishing the data more towards the end of the plot, I would go with the R squared value and say that the line is a good fit.</strong></p>
<ol start="4" style="list-style-type: lower-alpha">
<li>Are the four assumptions of linear regression satisfied? To answer this, draw the relevant plots. (Write a maximum of one sentence per assumption.) If not, what might you try to do to improve this (if you had more time)?</li>
</ol>
<p>Assumption 1</p>
<pre class="r"><code>#linearity and independence assumptions - residuals vs. x plot
plot(dat$funds, reg.output$residuals, ylim=c(-15,15), main=&quot;Residuals vs. Department Funding&quot;, xlab=&quot;Department Funding in Millions of Dollars that Year&quot;, ylab=&quot;Residuals&quot;)
abline(h = 0, lty=&quot;dashed&quot;)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-155-1.png" width="672" /></p>
<pre class="r"><code>#linearity assumption - residuals vs. fitted
plot(reg.output, which=1)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-156-1.png" width="672" /> <strong>Because both the residuals vs. x plot and residuals vs. fitted plot show clear curved patterns in the plots instead of no pattern at all, I would say that the linearity assumption is not satisfied.</strong></p>
<p>Assumption 2 <strong>Looking again at the residuals vs. x plot, we see that there is a curved pattern in the plot, so the independence assumption is not satisfied.</strong></p>
<p>Assumption 3</p>
<pre class="r"><code>#equal variance / homoscedasticity assumption - resuiduals vs. predicted values
plot(reg.output, which=3)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-157-1.png" width="672" /> <strong>Looking at both the scatterplot from question 1 and the scale-location plot, there is a clear pattern in the plot, so the equal variance / homoscedasticity assumption is not satisfied.</strong></p>
<p>Assumption 4</p>
<pre class="r"><code>#Normal population assumption - residuals vs. leverage plot
plot(reg.output, which=5)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-158-1.png" width="672" /></p>
<pre class="r"><code>#Normal population assumption - Normal qq plot
plot(reg.output, which=2)</code></pre>
<p><img src="Portfolio_files/figure-html/unnamed-chunk-159-1.png" width="672" /> <strong>Looking at the residuals vs. leverage plot (it looks like many values are clustered at the top of the line) and the normal qq plot (the values on the ends of the line are curving away from the line), the normal population assumption is not satisfied.</strong></p>
<p><strong>Because none of the assumptions are satisfied, if I had more time I would perform some sort of transformation to the data. I would probably start with logging funds, because I know that wages are usually logged when performing linear regression and funds are a pretty similar type of variable.</strong></p>
<ol start="5" style="list-style-type: lower-alpha">
<li>Answer the question of interest based on your analysis: “Does having more funding in a police department lead to fewer incidents of police brutality?”</li>
</ol>
<p><strong>Based on my analysis, it cannot be concluded that having more funding in a police department leads to fewer incidents of police brutality, as this implies causation and there could be hidden variables influencing the relationship. However,it does seem like having more funding in a police department is associated with fewer incidents of police brutality. The scatterplot and correlation show a strong negative relationship between the variables, and the p-value is statistically significant indicating that this relationship is not due to chance. Also, I do not think that the linear model is the best way to represent the data without performing any transformations. All of the assumptions were not satisfied, and even the scatterplot indicates a slight curving pattern. Transforming the data may give more insight into the association between department funding and incidents of police brutality.</strong></p>
<p><strong>Problem 3: Data ethics (10 points)</strong></p>
<p>Describe the dataset. Considering our lecture on data ethics, what concerns do you have about the dataset? Once you perform your analysis to answer the question of interest using this dataset, what concerns might you have about the results?</p>
<p><strong>Considering our lecture on data ethics, I am concerned that the dataset is biased because the police brutality variable is based off of the reported incidents of police brutality by the departments themselves. There is no way to know if departments are under reporting brutality, so the numbers in the dataset may not be correct or may not actually be reporting the ground truth of police brutality. Once I performed my analysis, I am concerned that the results may interpreted that more funding leads to less brutality (as even stated in the question of interest). This, however, implies causation, and while there is a relationship between the two variables there may be another hidden variable that is truly causing the association. It may be that having more funding leads to better service programs being provided by the police which leads to decreased police brutality, but that is something that cannot be proven with this dataset and analysis. In terms of implications, a policy maker may look at this analysis and think that increasing funding is the best solution to decreasing police brutality, which other research has proven that may not be the case. This analysis could, essentially, cause more harm if all police departments had an increase in funding but the funding was misused or put towards other things than decreasing police brutality.</strong></p>
</div>
<div id="final-project" class="section level3">
<h3>Final Project</h3>
<p><strong>Marijuana Possession Charges</strong> <strong>An exploratory look at the relationship between race and sentencing outcomes</strong> <strong>By Brianna Fisher, Sophie Faircloth, Anna Sophia Lotman, Hannah Wassermann</strong></p>
<p><strong>INTRODUCTION</strong> Following Nixon’s call for the War on Drugs in June 1971, there was a push for mandatory sentencing in drug-related crimes, effectively expanding the role of the federal government in drug-related arrests and sentencing (Drug Policy Alliance, 2021). Recently, there has been an uptick of discussion in the political world and in pop culture of the United States on the War on Drugs, namely regarding the adjustment of the severity of sentencing and its disproportionate effect on minority communities. These racial disparities are particularly discussed in regards to marijuana-related charges, as Black Americans have been found to be four times more likely to be arrested for marijuana charges than White Americans and six times more likely to be incarcerated for drug-related charges (Rahamatulla, 2017). Many agree that a solution to the disproportionate effects of drug-related charges needs to be created, and through the work discussed in this paper, we hope to focus our attention on sentencing outcomes and race for marijuana charges to further understand where disparities lie. Due to the increasing prevalence and history of drug-related sentencing, we wanted to examine the relationships in drug-related sentencing between race and sentencing likelihood, especially focusing on marijuana. We expect, from past studies examined in and outside of this course, that the rate of prison sentencing for Black people would be higher than the rate of prison sentencing of white people. These motivations lead to the research question: Are Black people sentenced federally to prison for marijuana at a higher rate than white people?</p>
<p><strong>DESCRIPTION OF THE DATA USED</strong> The data set we utilized for this review is a record of federal criminal sentences as provided by the US Sentencing Committee (United States Sentencing Commission, 2007). This data set includes information on federal cases sentenced under the guidelines of the Sentencing Reform Act of 1984. Because of the research question prompting this paper, we chose to look at only cases where the defendant was charged with possession of marijuana. We used the “Drug Type 1” variable in the dataset to create a subset with these cases, as this variable indicated that marijuana was the highest penalty incurring drug the defendant was found with. Because we wanted to look at the relationship between race and being sentenced to prison, our independent variable was the defendant’s race (either Black or white cases) and our dependent variable was the type of sentence (either prison or no prison). We extrapolated specifically Black and white instantiations in the race variable so as to control for the number of independent variables being observed. We also decided to control for the defendant’s age, the defendant’s gender, and whether or not the defendant has a criminal record. We thought that these three variables would be the most influential in determining sentence type, both as legal (criminal record) and extralegal (age and gender) variables.</p>
<p><strong>EXPLORATORY DATA ANALYSIS</strong> <img src="Portfolio_files/figure-html/unnamed-chunk-163-1.png" width="672" /> To most successfully represent the variables being examined in this paper, we chose to create a bar plot representing the proportion of people sentenced and not sentenced to prison for both races included in the dataset.</p>
<p><strong>REPRESENTATIVE MODEL AND DIAGNOSTIC INFORMATION</strong> For the sake of our data set, we used a logistic regression. A logistic regression model is very similar to a linear regression; however, it is used to evaluate relationships between binary variables. Because our variables - race and sentencing outcome - both have only two options for the purpose of our study, we can examine this relationship between the binaries. Rather than a standard line, the graph of fit is a logit. The application of this model is to categorical variables, and the dependent variable matches the variable type of the independent.</p>
<p><strong>LOGISTIC REGRESSION ASSUMPTIONS</strong> <img src="Portfolio_files/figure-html/unnamed-chunk-165-1.png" width="672" /></p>
<p>For the sake of our data set, we used a logistic regression. A logistic regression model is very similar to a linear regression; however, it is used to evaluate relationships between binary variables. Because our variables - race and sentencing outcome - both have only two options for the purpose of our study, we can examine this relationship between the binaries. Rather than a standard line, the graph of fit is a logit. The application of this model is to categorical variables, and the dependent variable matches the variable type of the independent.</p>
<p>The Residual vs. Fitted Plots in this figure look to see if there are any curvilinear trends in the plots that were originally missed. Because logistic regression is already curvilinear - as demonstrated by the logit that was previously displayed - these plots do not tell us any definitive information on the validity of this regression.</p>
<p>The QQ plot in the figure determines if the residuals are normally distributed. This plot is not indicative of anything definitive either because residuals do not have to be normally distributed in a logistic regression.</p>
<p>The Residuals vs. Leverage plots help identify outliers, but this plot too is not particularly useful because the results are not definitive either. From the models demonstrated in the assumptions, we do not gain information that definitively determines the strength of the logistic regression model for our data set, but the assumption display is crucial to data analysis so as to examine any particularly significant data that strays from the norm.</p>
<p><strong>Regression Model</strong></p>
<pre><code>## 
## Logistic Regression Outputs
## ================================================
##                         Dependent variable:     
##                     ----------------------------
##                                prison           
##                        Baseline      Controls   
##                          (1)            (2)     
## ------------------------------------------------
## Black                 -0.684***      -0.845***  
##                        (0.132)        (0.140)   
##                                                 
## Ages 70-97                             0.260    
##                                       (1.036)   
##                                                 
## Ages 50-69                             0.100    
##                                       (0.207)   
##                                                 
## Ages 30-49                           0.441***   
##                                       (0.116)   
##                                                 
## Female                               -1.194***  
##                                       (0.120)   
##                                                 
## No Criminal History                    0.071    
##                                       (0.117)   
##                                                 
## Constant               2.829***      2.881***   
##                        (0.060)        (0.102)   
##                                                 
## ------------------------------------------------
## Observations            6,123          6,123    
## ================================================
## Note:                *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>Looking at the baseline regression, we can see that Black people in the dataset were less likely to be sentenced to prison at a significant rate. Similarly, looking at the controlled regression, we can see that Black people were more likely to not be sentenced when compared to white people in the dataset. We split the age variable into three groups (of 16-29, 30-49, 50-69, and 70-97), leaving the 16-29 group out of the regression as our control because we thought that this group would be most likely to be in possession of marijuana. To our surprise, all three age groups were more likely to be sentenced to prison for marijuana charges than people that were 16-29. This may be due to the fact that there could have been more people in these other groups, or there could have been a large number of minors in the 16-29 group that were not sentenced to prison. We split the gender variable into male or female, leaving out the male group since we thought that this group would be more likely to be sentenced to prison. Looking at the model, we can see that this is true, and females were significantly less likely to be sentenced to prison than males. Finally, we split the criminal history variable into yes or no groups, leaving out the yes group since we thought that this group would be more likely to be sentenced to prison. To our surprise, having no criminal history made the defendant more likely to be sentenced to prison, but by an insignificant amount.</p>
<p><strong>CAUSAL ANALYSIS</strong> Because we used the “Drug Type 1” variable, we assumed that the defendant was either only found with marijuana or the other drug(s) did not incur any penalty. This did not take into account whether or not the defendant was charged with something incurring a felony charge, such as possession of drug paraphernalia or intent to sell. In an ideal world, our data would have been clear about what the defendant was actually charged with on all fronts, rather than just what drugs they were in possession of at the time of their arrest. We also only looked at about 6,000 observations, since we dropped everyone who was not Black or white. However, there were more white people in the dataset and more white people in prison in the dataset, so the groups may not have been proportional.</p>
<p><strong>DAG:</strong></p>
<pre><code>BEING ARRESTED 
^           |
|           v
RACE -------&gt; BEING SENTENCED TO PRISON</code></pre>
<p>Additionally, the DAG portrays the situation where the defendant’s race determines whether or not they will be arrested, and therefore the arrest determines whether they will be sentenced. Not everyone who is caught with marijuana is arrested, and not everyone who is arrested for marijuana charges is convicted. Similarly, the biases of the police and prosecutors could be at play, influencing both the arrest and sentence outcome. Because of these confounders, we would not be able to conclude that there was a causal analysis.</p>
<p><strong>LIMITATIONS AND FUTURE DIRECTIONS</strong></p>
<p>While setting out to examine sentencing outcomes and their relationship to race for marijuana-related charges, we observed a series of confounds:</p>
<ol style="list-style-type: decimal">
<li>Sentencing charges are not equated to arrests. It is worth looking in the future into the relationship between arrests and race for marijuana-related charges.</li>
<li>Our analysis did not include sentencing duration. Sentencing severity is a large component of the discussion for racial disparities in sentencing, not just the binary of whether someone was sentenced or not.</li>
<li>Though we controlled for different factors such as past arrests because they are crucial in determining sentencing outcomes they should be analyzed further in conjunction with the information we observed for future studies.</li>
</ol>
<p>This dataset is also a really interesting example of how a failure through data analysis can lead to false projections and misleading statistics. This is also a useful showcase of how easily data can also be manipulated depending on the neglect of specific outliers or parameters for the research. As previously stated, this data set is very limited. We don’t feel it can answer our research question in an accurate way, but it still provides a good lesson on the sensitivity of datasets with outlying variables and unobserved confounders.</p>
<p>For future research, it could be extremely valuable to look at the current different sentencing rates across states with different laws regarding the legality and decriminalization of marijuana. All marijuana usage (whether medical or recreational) is a federal crime, so theoretically everyone in the dataset should have been arrested regardless of race. However, this is not the case, and it is important to look at how there are disparities between federal and state sentences. Additionally, state police and prosecutors have much more discretion in deciding who to arrest, and what crimes to charge. We also were only able to look at sentencing as a binary factor without the important information of arrests records in general or whether or not the defendant has been arrested for marijuana in the past. In the future, we want to do a more well-rounded in-depth data analysis including everyone who was arrested (regardless of their conviction status), as well as the state’s current laws regarding marijuana.</p>
<p><strong>References</strong> Commission, U. S. S. (2014, June 25). Monitoring of federal criminal sentences, [united states], 2007. Monitoring of Federal Criminal Sentences, [United States], 2007. Retrieved December 12, 2021, from <a href="https://www.icpsr.umich.edu/web/NACJD/studies/22623" class="uri">https://www.icpsr.umich.edu/web/NACJD/studies/22623</a>. Cusick Director, J., Cusick, J., Director, Director, C. M. A., Montecinos, C., Director, A., Director, S. H. A., Hananel, S., Oduyeru Manager, L., Oduyeru, L., Manager, Gordon Director, P., Gordon, P., Director, J. P. D., Parshall, J., Director, D., Pearl, B., Perez, M., Chung, E., … Simpson, E. (2021, October 28). Ending the war on drugs: By the numbers. Center for American Progress. Retrieved December 12, 2021, from <a href="https://www.americanprogress.org/article/ending-war-drugs-numbers/" class="uri">https://www.americanprogress.org/article/ending-war-drugs-numbers/</a>. Rahamatulla, A. (2017, March 23). The War on Drugs has failed. what’s next? Ford Foundation. Retrieved December 12, 2021, from <a href="https://www.fordfoundation.org/just-matters/just-matters/posts/the-war-on-drugs-has-failed-what-s-next/" class="uri">https://www.fordfoundation.org/just-matters/just-matters/posts/the-war-on-drugs-has-failed-what-s-next/</a>. We Are The Drug Policy Alliance. (n.d.). A history of the Drug War. Drug Policy Alliance. Retrieved December 12, 2021, from <a href="https://drugpolicy.org/issues/brief-history-drug-war" class="uri">https://drugpolicy.org/issues/brief-history-drug-war</a>.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
